# LunarLander-v2-Solve (MTS contest) BEST SCORE: 231

> Награда за переход от верхней части экрана к посадочной площадке и нулевой скорости составляет около 100..140 баллов. Если посадочный аппарат отходит от посадочной площадки, он теряет награду. Эпизод заканчивается, если посадочный модуль падает или останавливается, получая дополнительные -100 или +100 очков. Каждый контакт с землей оценивается в +10 очков. Использование главного двигателя стоит -0,3 балла за каждый кадр. Выполнение условия посадки дает дополнительные 200 баллов. Возможна посадка вне посадочной площадки. Топливо бесконечно, поэтому агент может научиться летать, а затем научиться приземляться. Доступны четыре отдельных действия: ничего не делать, запустить двигатель с левой ориентацией, главный двигатель, двигатель с правой ориентацией.
## Метрика
> LunarLander-v2 определяет «решение» как получение среднего вознаграждения 200 за 100 последовательных испытаний.

## Использованные нами библиотеки:
* pytorch
* numpy
* gym
* pickle (Для сохранения агентов)

## Что мы делали
* За бейзлайн взяли модель и реализацию DQN отсюда https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/DeepQLearning (По факту от нее уже ничего не осталось), и адаптировали к нашей задаче. Получили ответ более чем за 1000 итераций. Также посмотрели решение второго места лидерборда https://github.com/plopd/deep-reinforcement-learning/blob/master/dqn/Deep_Q_Network.ipynb 
* Далее изменили алгоритм обновления весов фиксированной модели. Вместо простово перекидывания, стали взвешивать. Это дало лучший скор в 800 итераций
```python
if self.replace_target_cnt is not None and \
    self.learn_step_counter % self.replace_target_cnt == 0:

    self.Q_next.load_state_dict(self.Q_eval.state_dict())
```

```python
def update_weight(self, model_from, model_to, tau):
        for from_p, to_p in zip(model_from.parameters(), model_to.parameters()):
            to_p.data.copy_(tau*from_p.data + (1.0-tau)*to_p.data)
```
* Переписали память на numpy массивы - это дало серьезный прирост к скорости обучения. И инициализировали ее размером бача(Делая случайные действия в среде).
* Далее начался тюнинг, самым значимым параметром был EPSILON (он контролировал начальное изучение среды), определяющий вероятность совершения случайного действия, также мы минимизировали его разными методами, т.е. линейно, экспоненциально и т.д. После оптимизации задача решалась за +- 600 итераций.
* В нашей задаче максимальный reward равен 100 и даётся он при успешной посадке.Наш алгоритм пытается получить максимальный Ревард за итерацию и начинает летать, пытаясь получить ревард за успешную посадку ещё раз.Таким образом, чтобы избежать потерь reward’ов 
за использования основного двигателя, мы заканчиваем эпизод, если наш «Луноход» приземлился на землю. Это улучшило скор до 550 эпизодов и сильно сократило время обучения.
```python
def kostil(reward):
    return (
        reward == 100 or 
        reward == -100 or 
        reward == 10 or 
        reward == 200
        )
```
* Чтобы смотреть, как обучается модель мы написали класс для временного включения визуализации из gymа, 
```python
def try_block(env, scores, pbar, visualize):
    try:
        main_loop(env, scores, pbar, visualize)
    except KeyboardInterrupt:
        inp = input("""
            o - Остановка обучения,
            +v - Включить визуализацию
            -v - Выключить визуализацию
        """)
        if inp == 'o':
            print("Остановка обучения.")
        elif inp == '+v':
            try_block(env, scores, pbar, True)
        elif inp == '-v':
            try_block(env, scores, pbar, False)
        else:
            print("Продолжаем :)")
            try_block(env, scores, pbar, visualize)
```
Например в данной ситуации модель переобучилась
![https://github.com/furfa/LunarLander-v2-Solve/blob/master/img/605.png](https://github.com/furfa/LunarLander-v2-Solve/blob/master/img/605.png)