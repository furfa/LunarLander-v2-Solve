# LunarLander-v2-Solve (MTS contest) BEST SCORE: 231

> Награда за переход от верхней части экрана к посадочной площадке и нулевой скорости составляет около 100..140 баллов. Если посадочный аппарат отходит от посадочной площадки, он теряет награду. Эпизод заканчивается, если посадочный модуль падает или останавливается, получая дополнительные -100 или +100 очков. Каждый контакт с землей оценивается в +10 очков. Использование главного двигателя стоит -0,3 балла за каждый кадр. Выполнение условия посадки дает дополнительные 200 баллов. Возможна посадка вне посадочной площадки. Топливо бесконечно, поэтому агент может научиться летать, а затем научиться приземляться. Доступны четыре отдельных действия: ничего не делать, запустить двигатель с левой ориентацией, главный двигатель, двигатель с правой ориентацией.
## Метрика
> LunarLander-v2 определяет «решение» как получение среднего вознаграждения 200 за 100 последовательных испытаний.

## Использованные нами библиотеки:
* pytorch
* numpy
* gym
* pickle (Для сохранения агентов)

## Что мы делали
* За бейзлайн взяли модель и реализацию DQN отсюда https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/DeepQLearning (По факту от нее уже ничего не осталось), и адаптировали к нашей задаче. Получили ответ более чем за 1000 итераций. Также посмотрели решение второго места лидерборда https://github.com/plopd/deep-reinforcement-learning/blob/master/dqn/Deep_Q_Network.ipynb 
* Далее изменили алгоритм обновления весов фиксированной модели. Вместо простово перекидывания, стали взвешивать. Это дало лучший скор в 800 итераций
```python
if self.replace_target_cnt is not None and \
    self.learn_step_counter % self.replace_target_cnt == 0:

    self.Q_next.load_state_dict(self.Q_eval.state_dict())
```

```python
def update_weight(self, model_from, model_to, tau):
        for from_p, to_p in zip(model_from.parameters(), model_to.parameters()):
            to_p.data.copy_(tau*from_p.data + (1.0-tau)*to_p.data)
```
* Переписали память на numpy массивы - это дало серьезный прирост к скорости обучения. И инициализировали ее размером бача(Делая случайные действия в среде).
* Далее начался тюнинг, самым значимым параметром был EPSILON (он контролировал начальное изучение среды), определяющий вероятность совершения случайного действия, также мы минимизировали его разными методами, т.е. линейно, экспоненциально и т.д. Теперь задача решалась за +- 600 итераций.


  

