{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class GymRunner():\n",
    "\n",
    "    env = None\n",
    "\n",
    "    def __init__(env_name = \"LunarLander-v2\"):\n",
    "        self.env = gym.make(env_name)\n",
    "        #self.env._max_episode_steps = 1200\n",
    "\n",
    "    def get_shapes():\n",
    "        return {\n",
    "            \"action_space\" : self.env.action_space.n,\n",
    "            \"observation_space\" : self.env.observation_space.shape,\n",
    "        }\n",
    "\n",
    "    def test_agent(agent, n_iters):\n",
    "        done = False\n",
    "        observation = self.env.reset()\n",
    "        reward = 0\n",
    "        info = {}\n",
    "\n",
    "        for _ in range(n_iters):\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "\n",
    "                action = agent.act(observation, reward, done)\n",
    "\n",
    "                observation, reward, done, info = self.env.step(action)\n",
    "\n",
    "                print(f\"REWARD = {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [env_id]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "from gym import wrappers, logger\n",
    "\n",
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "# New comment\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('env_id', nargs='?', default='LunarLander-v2', help='Select the environment to run')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # You can set the level to logger.DEBUG or logger.WARN if you\n",
    "    # want to change the amount of output.\n",
    "    logger.set_level(logger.INFO)\n",
    "    \n",
    "    env = gym.make(args.env_id)\n",
    "\n",
    "    # You provide the directory to write to (can be an existing\n",
    "    # directory, including one with existing data -- all monitor files\n",
    "    # will be namespaced). You can also dump to a tempdir if you'd\n",
    "    # like: tempfile.mkdtemp().\n",
    "    outdir = '/tmp/random-agent-results'\n",
    "    env = wrappers.Monitor(env, directory=outdir, force=True)\n",
    "    env.seed(0)\n",
    "    agent = RandomAgent(env.action_space)\n",
    "\n",
    "    episode_count = 100\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        ob = env.reset()\n",
    "        while True:\n",
    "            action = agent.act(ob, reward, done)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            # Note there's no env.render() here. But the environment still can open window and\n",
    "            # render if asked by env.monitor: it calls env.render('rgb_array') to record video.\n",
    "            # Video is not recorded every episode, see capped_cubic_video_schedule for details.\n",
    "\n",
    "    # Close the env and write monitor result info to disk\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.render()\n",
    "env.reset()\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "    13/100000: episode: 1, duration: 0.039s, episode steps: 13, steps per second: 334, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.107 [-1.319, 0.772], mean_best_reward: --\n",
      "    24/100000: episode: 2, duration: 0.005s, episode steps: 11, steps per second: 2243, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.151 [-0.941, 1.832], mean_best_reward: --\n",
      "    37/100000: episode: 3, duration: 0.005s, episode steps: 13, steps per second: 2441, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.134 [-1.324, 2.292], mean_best_reward: --\n",
      "    51/100000: episode: 4, duration: 0.005s, episode steps: 14, steps per second: 2646, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.098 [-1.204, 2.050], mean_best_reward: --\n",
      "    68/100000: episode: 5, duration: 0.006s, episode steps: 17, steps per second: 2851, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.087 [-1.945, 1.031], mean_best_reward: --\n",
      "    82/100000: episode: 6, duration: 0.005s, episode steps: 14, steps per second: 2802, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.116 [-0.788, 1.541], mean_best_reward: --\n",
      "   102/100000: episode: 7, duration: 0.007s, episode steps: 20, steps per second: 2905, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.086 [-0.747, 1.392], mean_best_reward: --\n",
      "   116/100000: episode: 8, duration: 0.005s, episode steps: 14, steps per second: 2786, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.084 [-1.549, 0.975], mean_best_reward: --\n",
      "   127/100000: episode: 9, duration: 0.004s, episode steps: 11, steps per second: 2648, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.091 [-1.886, 1.222], mean_best_reward: --\n",
      "   156/100000: episode: 10, duration: 0.010s, episode steps: 29, steps per second: 2977, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.655 [0.000, 1.000], mean observation: -0.011 [-2.438, 1.730], mean_best_reward: --\n",
      "   168/100000: episode: 11, duration: 0.005s, episode steps: 12, steps per second: 2567, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.932, 1.150], mean_best_reward: --\n",
      "   183/100000: episode: 12, duration: 0.006s, episode steps: 15, steps per second: 2699, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.100 [-1.333, 2.274], mean_best_reward: --\n",
      "   196/100000: episode: 13, duration: 0.005s, episode steps: 13, steps per second: 2752, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.113 [-1.718, 2.721], mean_best_reward: --\n",
      "   208/100000: episode: 14, duration: 0.004s, episode steps: 12, steps per second: 2746, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.354, 2.185], mean_best_reward: --\n",
      "   217/100000: episode: 15, duration: 0.003s, episode steps: 9, steps per second: 2585, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.121 [-1.379, 2.248], mean_best_reward: --\n",
      "   227/100000: episode: 16, duration: 0.004s, episode steps: 10, steps per second: 2640, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.436, 1.572], mean_best_reward: --\n",
      "   242/100000: episode: 17, duration: 0.005s, episode steps: 15, steps per second: 2901, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.113 [-1.360, 0.622], mean_best_reward: --\n",
      "   256/100000: episode: 18, duration: 0.005s, episode steps: 14, steps per second: 2849, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.100 [-1.190, 1.996], mean_best_reward: --\n",
      "   270/100000: episode: 19, duration: 0.005s, episode steps: 14, steps per second: 2831, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.081 [-1.519, 0.819], mean_best_reward: --\n",
      "   283/100000: episode: 20, duration: 0.005s, episode steps: 13, steps per second: 2795, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.747, 0.992], mean_best_reward: --\n",
      "   299/100000: episode: 21, duration: 0.005s, episode steps: 16, steps per second: 2937, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.117 [-0.576, 1.184], mean_best_reward: --\n",
      "   314/100000: episode: 22, duration: 0.006s, episode steps: 15, steps per second: 2599, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.992, 1.590], mean_best_reward: --\n",
      "   354/100000: episode: 23, duration: 0.017s, episode steps: 40, steps per second: 2335, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.110 [-0.888, 1.608], mean_best_reward: --\n",
      "   364/100000: episode: 24, duration: 0.004s, episode steps: 10, steps per second: 2453, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.099, 1.970], mean_best_reward: --\n",
      "   378/100000: episode: 25, duration: 0.005s, episode steps: 14, steps per second: 2743, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.066 [-1.383, 2.121], mean_best_reward: --\n",
      "   397/100000: episode: 26, duration: 0.010s, episode steps: 19, steps per second: 1963, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.045 [-2.581, 1.769], mean_best_reward: --\n",
      "   428/100000: episode: 27, duration: 0.010s, episode steps: 31, steps per second: 3093, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.117 [-2.221, 0.996], mean_best_reward: --\n",
      "   451/100000: episode: 28, duration: 0.007s, episode steps: 23, steps per second: 3091, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.053 [-0.844, 1.576], mean_best_reward: --\n",
      "   465/100000: episode: 29, duration: 0.005s, episode steps: 14, steps per second: 2765, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.071 [-1.293, 0.831], mean_best_reward: --\n",
      "   482/100000: episode: 30, duration: 0.006s, episode steps: 17, steps per second: 2943, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.072 [-1.722, 1.030], mean_best_reward: --\n",
      "   504/100000: episode: 31, duration: 0.007s, episode steps: 22, steps per second: 3128, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.103 [-1.745, 0.773], mean_best_reward: --\n",
      "   529/100000: episode: 32, duration: 0.008s, episode steps: 25, steps per second: 3172, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.062 [-2.220, 1.329], mean_best_reward: --\n",
      "   549/100000: episode: 33, duration: 0.007s, episode steps: 20, steps per second: 3012, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.091 [-1.232, 0.804], mean_best_reward: --\n",
      "   565/100000: episode: 34, duration: 0.006s, episode steps: 16, steps per second: 2861, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.098 [-1.133, 2.087], mean_best_reward: --\n",
      "   652/100000: episode: 35, duration: 0.026s, episode steps: 87, steps per second: 3387, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.164 [-1.532, 1.254], mean_best_reward: --\n",
      "   672/100000: episode: 36, duration: 0.007s, episode steps: 20, steps per second: 3070, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.053 [-1.201, 1.977], mean_best_reward: --\n",
      "   716/100000: episode: 37, duration: 0.013s, episode steps: 44, steps per second: 3282, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.568 [0.000, 1.000], mean observation: -0.055 [-2.395, 1.395], mean_best_reward: --\n",
      "   728/100000: episode: 38, duration: 0.004s, episode steps: 12, steps per second: 2755, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.107 [-0.774, 1.524], mean_best_reward: --\n",
      "   742/100000: episode: 39, duration: 0.005s, episode steps: 14, steps per second: 2864, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.106 [-2.619, 1.567], mean_best_reward: --\n",
      "   795/100000: episode: 40, duration: 0.016s, episode steps: 53, steps per second: 3358, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.020 [-1.309, 0.945], mean_best_reward: --\n",
      "   816/100000: episode: 41, duration: 0.007s, episode steps: 21, steps per second: 2977, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.069 [-1.439, 0.979], mean_best_reward: --\n",
      "   833/100000: episode: 42, duration: 0.006s, episode steps: 17, steps per second: 2891, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.106 [-1.847, 0.944], mean_best_reward: --\n",
      "   883/100000: episode: 43, duration: 0.015s, episode steps: 50, steps per second: 3358, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.254 [-0.785, 1.712], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   893/100000: episode: 44, duration: 0.004s, episode steps: 10, steps per second: 2493, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.145 [-1.523, 2.391], mean_best_reward: --\n",
      "   924/100000: episode: 45, duration: 0.010s, episode steps: 31, steps per second: 3144, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: -0.034 [-2.329, 1.418], mean_best_reward: --\n",
      "   939/100000: episode: 46, duration: 0.009s, episode steps: 15, steps per second: 1640, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.066 [-2.251, 1.410], mean_best_reward: --\n",
      "   955/100000: episode: 47, duration: 0.006s, episode steps: 16, steps per second: 2595, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.060 [-2.520, 1.606], mean_best_reward: --\n",
      "   998/100000: episode: 48, duration: 0.023s, episode steps: 43, steps per second: 1862, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.003 [-1.143, 1.426], mean_best_reward: --\n",
      "  1019/100000: episode: 49, duration: 0.009s, episode steps: 21, steps per second: 2246, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.076 [-1.190, 2.051], mean_best_reward: --\n",
      "  1030/100000: episode: 50, duration: 0.005s, episode steps: 11, steps per second: 2077, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.101 [-1.197, 1.872], mean_best_reward: --\n",
      "  1045/100000: episode: 51, duration: 0.007s, episode steps: 15, steps per second: 2185, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.093 [-0.954, 1.767], mean_best_reward: --\n",
      "  1059/100000: episode: 52, duration: 0.006s, episode steps: 14, steps per second: 2254, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.119 [-2.666, 1.539], mean_best_reward: --\n",
      "  1075/100000: episode: 53, duration: 0.007s, episode steps: 16, steps per second: 2384, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.075 [-1.607, 2.480], mean_best_reward: --\n",
      "  1091/100000: episode: 54, duration: 0.007s, episode steps: 16, steps per second: 2453, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.081 [-2.094, 1.362], mean_best_reward: --\n",
      "  1102/100000: episode: 55, duration: 0.004s, episode steps: 11, steps per second: 2632, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.113 [-1.155, 1.851], mean_best_reward: --\n",
      "  1115/100000: episode: 56, duration: 0.005s, episode steps: 13, steps per second: 2728, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.080 [-1.397, 2.108], mean_best_reward: --\n",
      "  1126/100000: episode: 57, duration: 0.004s, episode steps: 11, steps per second: 2615, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-2.142, 1.356], mean_best_reward: --\n",
      "  1149/100000: episode: 58, duration: 0.008s, episode steps: 23, steps per second: 3041, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.064 [-0.566, 1.110], mean_best_reward: --\n",
      "  1202/100000: episode: 59, duration: 0.016s, episode steps: 53, steps per second: 3356, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.002 [-1.379, 0.753], mean_best_reward: --\n",
      "  1216/100000: episode: 60, duration: 0.005s, episode steps: 14, steps per second: 2874, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.093 [-1.609, 2.527], mean_best_reward: --\n",
      "  1254/100000: episode: 61, duration: 0.011s, episode steps: 38, steps per second: 3317, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.061 [-1.214, 0.587], mean_best_reward: --\n",
      "  1271/100000: episode: 62, duration: 0.006s, episode steps: 17, steps per second: 2954, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.096 [-1.814, 0.995], mean_best_reward: --\n",
      "  1370/100000: episode: 63, duration: 0.028s, episode steps: 99, steps per second: 3504, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.085 [-1.246, 1.047], mean_best_reward: --\n",
      "  1389/100000: episode: 64, duration: 0.006s, episode steps: 19, steps per second: 3052, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.080 [-0.740, 1.199], mean_best_reward: --\n",
      "  1459/100000: episode: 65, duration: 0.021s, episode steps: 70, steps per second: 3257, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.107 [-2.246, 1.724], mean_best_reward: --\n",
      "  1475/100000: episode: 66, duration: 0.009s, episode steps: 16, steps per second: 1846, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.095 [-1.595, 0.815], mean_best_reward: --\n",
      "  1489/100000: episode: 67, duration: 0.006s, episode steps: 14, steps per second: 2450, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.078 [-1.181, 1.906], mean_best_reward: --\n",
      "  1513/100000: episode: 68, duration: 0.014s, episode steps: 24, steps per second: 1758, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.076 [-0.776, 1.601], mean_best_reward: --\n",
      "  1532/100000: episode: 69, duration: 0.007s, episode steps: 19, steps per second: 2616, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.069 [-0.780, 1.464], mean_best_reward: --\n",
      "  1541/100000: episode: 70, duration: 0.009s, episode steps: 9, steps per second: 1045, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.119 [-2.130, 1.384], mean_best_reward: --\n",
      "  1568/100000: episode: 71, duration: 0.020s, episode steps: 27, steps per second: 1334, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.008 [-1.488, 0.931], mean_best_reward: --\n",
      "  1577/100000: episode: 72, duration: 0.012s, episode steps: 9, steps per second: 775, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-1.148, 1.991], mean_best_reward: --\n",
      "  1591/100000: episode: 73, duration: 0.011s, episode steps: 14, steps per second: 1287, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.086 [-1.613, 2.588], mean_best_reward: --\n",
      "  1622/100000: episode: 74, duration: 0.029s, episode steps: 31, steps per second: 1070, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.160 [-0.540, 1.168], mean_best_reward: --\n",
      "  1638/100000: episode: 75, duration: 0.014s, episode steps: 16, steps per second: 1142, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.093 [-0.963, 1.647], mean_best_reward: --\n",
      "  1651/100000: episode: 76, duration: 0.011s, episode steps: 13, steps per second: 1138, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.125 [-0.935, 1.534], mean_best_reward: --\n",
      "  1686/100000: episode: 77, duration: 0.028s, episode steps: 35, steps per second: 1271, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.053 [-1.702, 1.962], mean_best_reward: --\n",
      "  1707/100000: episode: 78, duration: 0.016s, episode steps: 21, steps per second: 1275, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.041 [-1.700, 1.027], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1720/100000: episode: 79, duration: 0.011s, episode steps: 13, steps per second: 1168, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.094 [-1.725, 1.021], mean_best_reward: --\n",
      "  1729/100000: episode: 80, duration: 0.007s, episode steps: 9, steps per second: 1353, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.446, 1.607], mean_best_reward: --\n",
      "  1741/100000: episode: 81, duration: 0.006s, episode steps: 12, steps per second: 1886, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.117 [-1.355, 2.126], mean_best_reward: --\n",
      "  1752/100000: episode: 82, duration: 0.008s, episode steps: 11, steps per second: 1456, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.125 [-2.329, 1.418], mean_best_reward: --\n",
      "  1771/100000: episode: 83, duration: 0.011s, episode steps: 19, steps per second: 1761, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.084 [-1.328, 0.762], mean_best_reward: --\n",
      "  1795/100000: episode: 84, duration: 0.011s, episode steps: 24, steps per second: 2089, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.541, 1.014], mean_best_reward: --\n",
      "  1814/100000: episode: 85, duration: 0.009s, episode steps: 19, steps per second: 2205, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.075 [-1.479, 0.966], mean_best_reward: --\n",
      "  1847/100000: episode: 86, duration: 0.013s, episode steps: 33, steps per second: 2562, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.060 [-0.404, 1.106], mean_best_reward: --\n",
      "  1859/100000: episode: 87, duration: 0.005s, episode steps: 12, steps per second: 2191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.135 [-1.521, 2.544], mean_best_reward: --\n",
      "  1874/100000: episode: 88, duration: 0.006s, episode steps: 15, steps per second: 2614, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.100 [-2.323, 1.341], mean_best_reward: --\n",
      "  1889/100000: episode: 89, duration: 0.005s, episode steps: 15, steps per second: 2839, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.206, 2.017], mean_best_reward: --\n",
      "  1899/100000: episode: 90, duration: 0.004s, episode steps: 10, steps per second: 2670, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.123 [-1.928, 1.217], mean_best_reward: --\n",
      "  1918/100000: episode: 91, duration: 0.006s, episode steps: 19, steps per second: 3051, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.088 [-2.458, 1.429], mean_best_reward: --\n",
      "  1938/100000: episode: 92, duration: 0.007s, episode steps: 20, steps per second: 3052, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.070 [-1.205, 2.036], mean_best_reward: --\n",
      "  1974/100000: episode: 93, duration: 0.011s, episode steps: 36, steps per second: 3312, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.064 [-1.197, 2.305], mean_best_reward: --\n",
      "  1991/100000: episode: 94, duration: 0.006s, episode steps: 17, steps per second: 3010, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.089 [-1.089, 0.573], mean_best_reward: --\n",
      "  1999/100000: episode: 95, duration: 0.003s, episode steps: 8, steps per second: 2533, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.160 [-1.142, 2.028], mean_best_reward: --\n",
      "  2009/100000: episode: 96, duration: 0.004s, episode steps: 10, steps per second: 2599, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.136 [-1.520, 2.502], mean_best_reward: --\n",
      "  2029/100000: episode: 97, duration: 0.007s, episode steps: 20, steps per second: 3072, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-0.930, 0.391], mean_best_reward: --\n",
      "  2043/100000: episode: 98, duration: 0.005s, episode steps: 14, steps per second: 2827, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.106 [-1.156, 2.137], mean_best_reward: --\n",
      "  2075/100000: episode: 99, duration: 0.010s, episode steps: 32, steps per second: 3207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.053 [-1.194, 0.590], mean_best_reward: --\n",
      "  2092/100000: episode: 100, duration: 0.006s, episode steps: 17, steps per second: 3023, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.044 [-0.827, 1.272], mean_best_reward: --\n",
      "  2114/100000: episode: 101, duration: 0.008s, episode steps: 22, steps per second: 2922, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.050 [-1.562, 2.549], mean_best_reward: 93.000000\n",
      "  2157/100000: episode: 102, duration: 0.013s, episode steps: 43, steps per second: 3286, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.111 [-1.152, 0.722], mean_best_reward: --\n",
      "  2171/100000: episode: 103, duration: 0.005s, episode steps: 14, steps per second: 2831, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-1.363, 0.796], mean_best_reward: --\n",
      "  2185/100000: episode: 104, duration: 0.005s, episode steps: 14, steps per second: 2782, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.101 [-1.455, 0.977], mean_best_reward: --\n",
      "  2207/100000: episode: 105, duration: 0.007s, episode steps: 22, steps per second: 2940, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.072 [-0.622, 1.316], mean_best_reward: --\n",
      "  2227/100000: episode: 106, duration: 0.008s, episode steps: 20, steps per second: 2462, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-0.980, 0.582], mean_best_reward: --\n",
      "  2258/100000: episode: 107, duration: 0.015s, episode steps: 31, steps per second: 2072, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.022 [-1.348, 2.048], mean_best_reward: --\n",
      "  2280/100000: episode: 108, duration: 0.007s, episode steps: 22, steps per second: 3089, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.104 [-0.752, 1.535], mean_best_reward: --\n",
      "  2294/100000: episode: 109, duration: 0.008s, episode steps: 14, steps per second: 1732, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.947, 1.640], mean_best_reward: --\n",
      "  2308/100000: episode: 110, duration: 0.005s, episode steps: 14, steps per second: 2642, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.085 [-0.984, 1.715], mean_best_reward: --\n",
      "  2348/100000: episode: 111, duration: 0.013s, episode steps: 40, steps per second: 3071, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.038 [-1.006, 1.910], mean_best_reward: --\n",
      "  2412/100000: episode: 112, duration: 0.019s, episode steps: 64, steps per second: 3349, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.005 [-0.774, 1.449], mean_best_reward: --\n",
      "  2513/100000: episode: 113, duration: 0.030s, episode steps: 101, steps per second: 3409, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.504 [-1.088, 2.439], mean_best_reward: --\n",
      "  2552/100000: episode: 114, duration: 0.012s, episode steps: 39, steps per second: 3375, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.075 [-0.457, 1.338], mean_best_reward: --\n",
      "  2578/100000: episode: 115, duration: 0.009s, episode steps: 26, steps per second: 2971, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.046 [-1.216, 2.009], mean_best_reward: --\n",
      "  2596/100000: episode: 116, duration: 0.006s, episode steps: 18, steps per second: 2964, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.617, 0.961], mean_best_reward: --\n",
      "  2605/100000: episode: 117, duration: 0.004s, episode steps: 9, steps per second: 2311, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.133 [-1.664, 0.991], mean_best_reward: --\n",
      "  2632/100000: episode: 118, duration: 0.009s, episode steps: 27, steps per second: 3098, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.002 [-1.188, 1.588], mean_best_reward: --\n",
      "  2671/100000: episode: 119, duration: 0.014s, episode steps: 39, steps per second: 2864, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.007 [-0.955, 1.384], mean_best_reward: --\n",
      "  2741/100000: episode: 120, duration: 0.021s, episode steps: 70, steps per second: 3267, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.748, 1.411], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2829/100000: episode: 121, duration: 0.027s, episode steps: 88, steps per second: 3234, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.497, 1.342], mean_best_reward: --\n",
      "  2973/100000: episode: 122, duration: 0.053s, episode steps: 144, steps per second: 2728, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.116 [-1.161, 1.565], mean_best_reward: --\n",
      "  2990/100000: episode: 123, duration: 0.007s, episode steps: 17, steps per second: 2428, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.119 [-1.588, 0.759], mean_best_reward: --\n",
      "  3019/100000: episode: 124, duration: 0.010s, episode steps: 29, steps per second: 2870, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.126 [-0.499, 1.035], mean_best_reward: --\n",
      "  3046/100000: episode: 125, duration: 0.009s, episode steps: 27, steps per second: 2879, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.088 [-1.620, 0.613], mean_best_reward: --\n",
      "  3078/100000: episode: 126, duration: 0.010s, episode steps: 32, steps per second: 3121, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.018 [-0.762, 1.369], mean_best_reward: --\n",
      "  3097/100000: episode: 127, duration: 0.006s, episode steps: 19, steps per second: 2955, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.068 [-1.673, 1.018], mean_best_reward: --\n",
      "  3128/100000: episode: 128, duration: 0.010s, episode steps: 31, steps per second: 3197, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.006 [-1.396, 1.019], mean_best_reward: --\n",
      "  3192/100000: episode: 129, duration: 0.019s, episode steps: 64, steps per second: 3386, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.029 [-0.782, 1.436], mean_best_reward: --\n",
      "  3265/100000: episode: 130, duration: 0.023s, episode steps: 73, steps per second: 3223, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.131 [-0.743, 1.137], mean_best_reward: --\n",
      "  3307/100000: episode: 131, duration: 0.013s, episode steps: 42, steps per second: 3217, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.006 [-1.533, 1.141], mean_best_reward: --\n",
      "  3318/100000: episode: 132, duration: 0.004s, episode steps: 11, steps per second: 2743, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.146 [-0.973, 1.795], mean_best_reward: --\n",
      "  3341/100000: episode: 133, duration: 0.007s, episode steps: 23, steps per second: 3081, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.028 [-2.127, 1.398], mean_best_reward: --\n",
      "  3356/100000: episode: 134, duration: 0.006s, episode steps: 15, steps per second: 2424, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-1.157, 1.982], mean_best_reward: --\n",
      "  3387/100000: episode: 135, duration: 0.010s, episode steps: 31, steps per second: 3174, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.096 [-1.492, 0.596], mean_best_reward: --\n",
      "  3405/100000: episode: 136, duration: 0.006s, episode steps: 18, steps per second: 3055, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.098 [-1.467, 0.935], mean_best_reward: --\n",
      "  3424/100000: episode: 137, duration: 0.009s, episode steps: 19, steps per second: 2149, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.108 [-1.509, 0.786], mean_best_reward: --\n",
      "  3459/100000: episode: 138, duration: 0.016s, episode steps: 35, steps per second: 2210, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.000 [-1.132, 1.400], mean_best_reward: --\n",
      "  3507/100000: episode: 139, duration: 0.015s, episode steps: 48, steps per second: 3111, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.105 [-1.028, 1.053], mean_best_reward: --\n",
      "  3531/100000: episode: 140, duration: 0.009s, episode steps: 24, steps per second: 2723, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.780, 1.232], mean_best_reward: --\n",
      "  3585/100000: episode: 141, duration: 0.017s, episode steps: 54, steps per second: 3108, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.083 [-0.737, 0.894], mean_best_reward: --\n",
      "  3598/100000: episode: 142, duration: 0.007s, episode steps: 13, steps per second: 1891, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.117 [-1.882, 0.979], mean_best_reward: --\n",
      "  3616/100000: episode: 143, duration: 0.006s, episode steps: 18, steps per second: 2859, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.146 [-0.952, 2.014], mean_best_reward: --\n",
      "  3647/100000: episode: 144, duration: 0.010s, episode steps: 31, steps per second: 3068, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.053 [-0.610, 0.941], mean_best_reward: --\n",
      "  3662/100000: episode: 145, duration: 0.005s, episode steps: 15, steps per second: 2849, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.106 [-0.578, 1.202], mean_best_reward: --\n",
      "  3685/100000: episode: 146, duration: 0.008s, episode steps: 23, steps per second: 2969, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.124 [-1.271, 0.399], mean_best_reward: --\n",
      "  3695/100000: episode: 147, duration: 0.004s, episode steps: 10, steps per second: 2623, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.100 [-1.402, 2.075], mean_best_reward: --\n",
      "  3719/100000: episode: 148, duration: 0.008s, episode steps: 24, steps per second: 3167, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.043 [-1.502, 0.797], mean_best_reward: --\n",
      "  3742/100000: episode: 149, duration: 0.007s, episode steps: 23, steps per second: 3149, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.103 [-0.972, 1.462], mean_best_reward: --\n",
      "  3765/100000: episode: 150, duration: 0.007s, episode steps: 23, steps per second: 3068, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.029 [-1.380, 2.181], mean_best_reward: --\n",
      "  3795/100000: episode: 151, duration: 0.010s, episode steps: 30, steps per second: 3088, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.586, 0.930], mean_best_reward: 116.000000\n",
      "  3831/100000: episode: 152, duration: 0.011s, episode steps: 36, steps per second: 3258, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.115 [-1.206, 0.538], mean_best_reward: --\n",
      "  3855/100000: episode: 153, duration: 0.008s, episode steps: 24, steps per second: 3121, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.782, 1.388], mean_best_reward: --\n",
      "  3918/100000: episode: 154, duration: 0.018s, episode steps: 63, steps per second: 3416, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.207 [-1.296, 0.850], mean_best_reward: --\n",
      "  3957/100000: episode: 155, duration: 0.012s, episode steps: 39, steps per second: 3313, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.085 [-0.898, 1.321], mean_best_reward: --\n",
      "  4000/100000: episode: 156, duration: 0.013s, episode steps: 43, steps per second: 3362, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.059 [-0.603, 0.872], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4022/100000: episode: 157, duration: 0.008s, episode steps: 22, steps per second: 2638, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.415, 1.031], mean_best_reward: --\n",
      "  4064/100000: episode: 158, duration: 0.015s, episode steps: 42, steps per second: 2812, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.054 [-1.553, 1.533], mean_best_reward: --\n",
      "  4126/100000: episode: 159, duration: 0.022s, episode steps: 62, steps per second: 2834, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: 0.021 [-1.123, 1.374], mean_best_reward: --\n",
      "  4147/100000: episode: 160, duration: 0.007s, episode steps: 21, steps per second: 2847, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.052 [-1.119, 1.759], mean_best_reward: --\n",
      "  4199/100000: episode: 161, duration: 0.016s, episode steps: 52, steps per second: 3281, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.227 [-0.669, 1.554], mean_best_reward: --\n",
      "  4217/100000: episode: 162, duration: 0.006s, episode steps: 18, steps per second: 2996, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.122 [-0.575, 1.202], mean_best_reward: --\n",
      "  4233/100000: episode: 163, duration: 0.005s, episode steps: 16, steps per second: 2911, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.093 [-1.221, 2.048], mean_best_reward: --\n",
      "  4311/100000: episode: 164, duration: 0.022s, episode steps: 78, steps per second: 3469, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.189 [-1.376, 1.540], mean_best_reward: --\n",
      "  4329/100000: episode: 165, duration: 0.006s, episode steps: 18, steps per second: 2996, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.075 [-0.808, 1.265], mean_best_reward: --\n",
      "  4340/100000: episode: 166, duration: 0.004s, episode steps: 11, steps per second: 2685, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-1.032, 1.648], mean_best_reward: --\n",
      "  4509/100000: episode: 167, duration: 0.049s, episode steps: 169, steps per second: 3448, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.145 [-1.289, 1.343], mean_best_reward: --\n",
      "  4544/100000: episode: 168, duration: 0.011s, episode steps: 35, steps per second: 3063, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.034 [-1.366, 0.830], mean_best_reward: --\n",
      "  4625/100000: episode: 169, duration: 0.023s, episode steps: 81, steps per second: 3464, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.052 [-0.966, 1.424], mean_best_reward: --\n",
      "  4648/100000: episode: 170, duration: 0.007s, episode steps: 23, steps per second: 3123, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.042 [-1.735, 1.007], mean_best_reward: --\n",
      "  4668/100000: episode: 171, duration: 0.011s, episode steps: 20, steps per second: 1855, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.079 [-1.430, 0.754], mean_best_reward: --\n",
      "  4680/100000: episode: 172, duration: 0.006s, episode steps: 12, steps per second: 2025, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.140 [-0.940, 1.534], mean_best_reward: --\n",
      "  4732/100000: episode: 173, duration: 0.019s, episode steps: 52, steps per second: 2764, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.134 [-1.226, 0.826], mean_best_reward: --\n",
      "  4742/100000: episode: 174, duration: 0.004s, episode steps: 10, steps per second: 2381, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.102 [-1.423, 2.222], mean_best_reward: --\n",
      "  4769/100000: episode: 175, duration: 0.009s, episode steps: 27, steps per second: 3003, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.065 [-0.923, 0.609], mean_best_reward: --\n",
      "  4794/100000: episode: 176, duration: 0.008s, episode steps: 25, steps per second: 3037, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.048 [-0.799, 1.072], mean_best_reward: --\n",
      "  4847/100000: episode: 177, duration: 0.016s, episode steps: 53, steps per second: 3334, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.056 [-0.783, 1.233], mean_best_reward: --\n",
      "  4893/100000: episode: 178, duration: 0.014s, episode steps: 46, steps per second: 3260, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.042 [-1.336, 0.592], mean_best_reward: --\n",
      "  4919/100000: episode: 179, duration: 0.008s, episode steps: 26, steps per second: 3105, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.422, 1.237], mean_best_reward: --\n",
      "  4942/100000: episode: 180, duration: 0.007s, episode steps: 23, steps per second: 3075, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.056 [-0.776, 1.391], mean_best_reward: --\n",
      "  4970/100000: episode: 181, duration: 0.009s, episode steps: 28, steps per second: 3139, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.065 [-0.752, 1.303], mean_best_reward: --\n",
      "  4983/100000: episode: 182, duration: 0.005s, episode steps: 13, steps per second: 2789, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.118 [-1.547, 0.944], mean_best_reward: --\n",
      "  5011/100000: episode: 183, duration: 0.009s, episode steps: 28, steps per second: 3150, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.084 [-1.811, 0.791], mean_best_reward: --\n",
      "  5024/100000: episode: 184, duration: 0.005s, episode steps: 13, steps per second: 2746, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.130 [-1.516, 0.744], mean_best_reward: --\n",
      "  5044/100000: episode: 185, duration: 0.007s, episode steps: 20, steps per second: 2984, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.107 [-0.573, 1.182], mean_best_reward: --\n",
      "  5073/100000: episode: 186, duration: 0.009s, episode steps: 29, steps per second: 3164, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.103 [-1.357, 0.778], mean_best_reward: --\n",
      "  5126/100000: episode: 187, duration: 0.016s, episode steps: 53, steps per second: 3346, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.130 [-1.510, 1.362], mean_best_reward: --\n",
      "  5173/100000: episode: 188, duration: 0.014s, episode steps: 47, steps per second: 3332, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.026 [-1.197, 0.905], mean_best_reward: --\n",
      "  5196/100000: episode: 189, duration: 0.008s, episode steps: 23, steps per second: 3046, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.116 [-0.630, 1.493], mean_best_reward: --\n",
      "  5217/100000: episode: 190, duration: 0.007s, episode steps: 21, steps per second: 2999, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.041 [-1.337, 1.918], mean_best_reward: --\n",
      "  5263/100000: episode: 191, duration: 0.014s, episode steps: 46, steps per second: 3314, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.079 [-1.782, 1.536], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5368/100000: episode: 192, duration: 0.036s, episode steps: 105, steps per second: 2927, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.045 [-1.513, 1.524], mean_best_reward: --\n",
      "  5404/100000: episode: 193, duration: 0.012s, episode steps: 36, steps per second: 2979, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.059 [-0.604, 0.908], mean_best_reward: --\n",
      "  5471/100000: episode: 194, duration: 0.021s, episode steps: 67, steps per second: 3195, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.247 [-1.323, 0.777], mean_best_reward: --\n",
      "  5482/100000: episode: 195, duration: 0.004s, episode steps: 11, steps per second: 2635, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.113 [-1.484, 0.840], mean_best_reward: --\n",
      "  5515/100000: episode: 196, duration: 0.011s, episode steps: 33, steps per second: 3027, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.076 [-0.816, 0.971], mean_best_reward: --\n",
      "  5563/100000: episode: 197, duration: 0.015s, episode steps: 48, steps per second: 3257, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.006 [-1.179, 1.428], mean_best_reward: --\n",
      "  5583/100000: episode: 198, duration: 0.006s, episode steps: 20, steps per second: 3380, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.624, 1.135], mean_best_reward: --\n",
      "  5601/100000: episode: 199, duration: 0.006s, episode steps: 18, steps per second: 3022, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.104 [-0.765, 1.426], mean_best_reward: --\n",
      "  5633/100000: episode: 200, duration: 0.010s, episode steps: 32, steps per second: 3272, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.619, 0.973], mean_best_reward: --\n",
      "  5689/100000: episode: 201, duration: 0.017s, episode steps: 56, steps per second: 3279, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.199 [-1.042, 1.349], mean_best_reward: 124.500000\n",
      "  5715/100000: episode: 202, duration: 0.008s, episode steps: 26, steps per second: 3071, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-0.924, 0.539], mean_best_reward: --\n",
      "  5742/100000: episode: 203, duration: 0.009s, episode steps: 27, steps per second: 3139, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.113 [-0.597, 1.252], mean_best_reward: --\n",
      "  5776/100000: episode: 204, duration: 0.011s, episode steps: 34, steps per second: 3057, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.538, 0.971], mean_best_reward: --\n",
      "  5800/100000: episode: 205, duration: 0.008s, episode steps: 24, steps per second: 3094, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.623, 1.235], mean_best_reward: --\n",
      "  5831/100000: episode: 206, duration: 0.010s, episode steps: 31, steps per second: 3031, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.125 [-0.500, 0.877], mean_best_reward: --\n",
      "  5854/100000: episode: 207, duration: 0.007s, episode steps: 23, steps per second: 3073, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.063 [-1.871, 0.972], mean_best_reward: --\n",
      "  5907/100000: episode: 208, duration: 0.018s, episode steps: 53, steps per second: 2874, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.076 [-0.588, 1.072], mean_best_reward: --\n",
      "  5942/100000: episode: 209, duration: 0.016s, episode steps: 35, steps per second: 2124, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.084 [-0.541, 1.200], mean_best_reward: --\n",
      "  5991/100000: episode: 210, duration: 0.029s, episode steps: 49, steps per second: 1672, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.078 [-1.415, 0.547], mean_best_reward: --\n",
      "  6031/100000: episode: 211, duration: 0.023s, episode steps: 40, steps per second: 1728, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.623, 0.886], mean_best_reward: --\n",
      "  6053/100000: episode: 212, duration: 0.012s, episode steps: 22, steps per second: 1907, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.077 [-1.700, 0.944], mean_best_reward: --\n",
      "  6082/100000: episode: 213, duration: 0.015s, episode steps: 29, steps per second: 1959, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.069 [-1.160, 0.631], mean_best_reward: --\n",
      "  6105/100000: episode: 214, duration: 0.009s, episode steps: 23, steps per second: 2444, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.085 [-0.768, 1.208], mean_best_reward: --\n",
      "  6189/100000: episode: 215, duration: 0.032s, episode steps: 84, steps per second: 2634, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.190 [-1.458, 0.752], mean_best_reward: --\n",
      "  6247/100000: episode: 216, duration: 0.021s, episode steps: 58, steps per second: 2800, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.543, 1.307], mean_best_reward: --\n",
      "  6333/100000: episode: 217, duration: 0.027s, episode steps: 86, steps per second: 3215, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.176 [-1.942, 2.250], mean_best_reward: --\n",
      "  6384/100000: episode: 218, duration: 0.015s, episode steps: 51, steps per second: 3347, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.211 [-1.338, 0.546], mean_best_reward: --\n",
      "  6399/100000: episode: 219, duration: 0.006s, episode steps: 15, steps per second: 2723, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.964, 1.557], mean_best_reward: --\n",
      "  6448/100000: episode: 220, duration: 0.015s, episode steps: 49, steps per second: 3317, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.009 [-1.207, 1.328], mean_best_reward: --\n",
      "  6509/100000: episode: 221, duration: 0.018s, episode steps: 61, steps per second: 3395, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.047 [-1.170, 1.026], mean_best_reward: --\n",
      "  6529/100000: episode: 222, duration: 0.007s, episode steps: 20, steps per second: 2902, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.088 [-0.796, 1.230], mean_best_reward: --\n",
      "  6683/100000: episode: 223, duration: 0.050s, episode steps: 154, steps per second: 3107, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.293 [-1.166, 1.913], mean_best_reward: --\n",
      "  6718/100000: episode: 224, duration: 0.012s, episode steps: 35, steps per second: 3034, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.059 [-1.103, 0.597], mean_best_reward: --\n",
      "  6745/100000: episode: 225, duration: 0.009s, episode steps: 27, steps per second: 3089, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.110 [-1.069, 0.635], mean_best_reward: --\n",
      "  6756/100000: episode: 226, duration: 0.004s, episode steps: 11, steps per second: 2712, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.120 [-0.783, 1.247], mean_best_reward: --\n",
      "  6773/100000: episode: 227, duration: 0.006s, episode steps: 17, steps per second: 2970, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-1.427, 0.819], mean_best_reward: --\n",
      "  6790/100000: episode: 228, duration: 0.006s, episode steps: 17, steps per second: 2888, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.083 [-0.997, 1.634], mean_best_reward: --\n",
      "  6812/100000: episode: 229, duration: 0.007s, episode steps: 22, steps per second: 3072, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.077 [-1.174, 0.619], mean_best_reward: --\n",
      "  6831/100000: episode: 230, duration: 0.006s, episode steps: 19, steps per second: 3004, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.117 [-0.944, 0.367], mean_best_reward: --\n",
      "  6924/100000: episode: 231, duration: 0.027s, episode steps: 93, steps per second: 3455, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.042 [-0.984, 1.067], mean_best_reward: --\n",
      "  6943/100000: episode: 232, duration: 0.006s, episode steps: 19, steps per second: 3028, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.132 [-0.558, 1.033], mean_best_reward: --\n",
      "  6955/100000: episode: 233, duration: 0.004s, episode steps: 12, steps per second: 2783, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.129 [-1.781, 0.946], mean_best_reward: --\n",
      "  6969/100000: episode: 234, duration: 0.005s, episode steps: 14, steps per second: 2874, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.112 [-0.804, 1.589], mean_best_reward: --\n",
      "  6999/100000: episode: 235, duration: 0.010s, episode steps: 30, steps per second: 3062, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.043 [-1.275, 0.803], mean_best_reward: --\n",
      "  7018/100000: episode: 236, duration: 0.007s, episode steps: 19, steps per second: 2907, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.087 [-0.843, 1.498], mean_best_reward: --\n",
      "  7037/100000: episode: 237, duration: 0.007s, episode steps: 19, steps per second: 2910, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.110 [-0.387, 1.125], mean_best_reward: --\n",
      "  7079/100000: episode: 238, duration: 0.014s, episode steps: 42, steps per second: 3051, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.083 [-1.046, 0.732], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7153/100000: episode: 239, duration: 0.023s, episode steps: 74, steps per second: 3184, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.104 [-1.298, 0.744], mean_best_reward: --\n",
      "  7197/100000: episode: 240, duration: 0.015s, episode steps: 44, steps per second: 2891, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.087 [-0.950, 1.095], mean_best_reward: --\n",
      "  7227/100000: episode: 241, duration: 0.012s, episode steps: 30, steps per second: 2477, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.103 [-1.505, 0.582], mean_best_reward: --\n",
      "  7327/100000: episode: 242, duration: 0.029s, episode steps: 100, steps per second: 3414, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.209 [-1.149, 0.856], mean_best_reward: --\n",
      "  7366/100000: episode: 243, duration: 0.011s, episode steps: 39, steps per second: 3442, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.040 [-0.737, 1.248], mean_best_reward: --\n",
      "  7419/100000: episode: 244, duration: 0.017s, episode steps: 53, steps per second: 3192, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.024 [-0.626, 0.932], mean_best_reward: --\n",
      "  7435/100000: episode: 245, duration: 0.006s, episode steps: 16, steps per second: 2744, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.833, 1.209], mean_best_reward: --\n",
      "  7449/100000: episode: 246, duration: 0.005s, episode steps: 14, steps per second: 2715, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.100 [-1.027, 1.765], mean_best_reward: --\n",
      "  7559/100000: episode: 247, duration: 0.032s, episode steps: 110, steps per second: 3398, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.074 [-1.433, 1.178], mean_best_reward: --\n",
      "  7586/100000: episode: 248, duration: 0.009s, episode steps: 27, steps per second: 3136, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.085 [-0.645, 1.294], mean_best_reward: --\n",
      "  7635/100000: episode: 249, duration: 0.015s, episode steps: 49, steps per second: 3247, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.016 [-0.806, 0.998], mean_best_reward: --\n",
      "  7673/100000: episode: 250, duration: 0.012s, episode steps: 38, steps per second: 3170, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.130 [-1.361, 0.416], mean_best_reward: --\n",
      "  7689/100000: episode: 251, duration: 0.007s, episode steps: 16, steps per second: 2431, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.060 [-1.421, 0.986], mean_best_reward: 107.500000\n",
      "  7717/100000: episode: 252, duration: 0.009s, episode steps: 28, steps per second: 3221, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.090 [-1.403, 0.785], mean_best_reward: --\n",
      "  7748/100000: episode: 253, duration: 0.010s, episode steps: 31, steps per second: 3194, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.109 [-0.742, 1.388], mean_best_reward: --\n",
      "  7785/100000: episode: 254, duration: 0.012s, episode steps: 37, steps per second: 2968, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.007 [-1.501, 0.957], mean_best_reward: --\n",
      "  7818/100000: episode: 255, duration: 0.014s, episode steps: 33, steps per second: 2283, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.042 [-1.449, 0.844], mean_best_reward: --\n",
      "  7858/100000: episode: 256, duration: 0.013s, episode steps: 40, steps per second: 3095, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.081 [-0.893, 0.616], mean_best_reward: --\n",
      "  7870/100000: episode: 257, duration: 0.005s, episode steps: 12, steps per second: 2658, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.749, 1.319], mean_best_reward: --\n",
      "  7911/100000: episode: 258, duration: 0.011s, episode steps: 41, steps per second: 3573, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.063 [-0.571, 0.986], mean_best_reward: --\n",
      "  7958/100000: episode: 259, duration: 0.014s, episode steps: 47, steps per second: 3391, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.015 [-1.671, 1.157], mean_best_reward: --\n",
      "  8012/100000: episode: 260, duration: 0.016s, episode steps: 54, steps per second: 3405, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.234, 1.062], mean_best_reward: --\n",
      "  8034/100000: episode: 261, duration: 0.007s, episode steps: 22, steps per second: 3036, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.418, 1.000], mean_best_reward: --\n",
      "  8069/100000: episode: 262, duration: 0.011s, episode steps: 35, steps per second: 3207, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.025 [-1.564, 0.953], mean_best_reward: --\n",
      "  8117/100000: episode: 263, duration: 0.014s, episode steps: 48, steps per second: 3353, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.036 [-0.791, 1.226], mean_best_reward: --\n",
      "  8158/100000: episode: 264, duration: 0.012s, episode steps: 41, steps per second: 3297, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.122 [-1.230, 0.569], mean_best_reward: --\n",
      "  8204/100000: episode: 265, duration: 0.014s, episode steps: 46, steps per second: 3327, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.185 [-0.640, 1.282], mean_best_reward: --\n",
      "  8258/100000: episode: 266, duration: 0.016s, episode steps: 54, steps per second: 3361, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.111 [-0.776, 1.289], mean_best_reward: --\n",
      "  8277/100000: episode: 267, duration: 0.006s, episode steps: 19, steps per second: 3130, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.124 [-1.158, 0.398], mean_best_reward: --\n",
      "  8297/100000: episode: 268, duration: 0.006s, episode steps: 20, steps per second: 3089, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.076 [-0.603, 1.123], mean_best_reward: --\n",
      "  8333/100000: episode: 269, duration: 0.011s, episode steps: 36, steps per second: 3260, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.085 [-1.468, 0.772], mean_best_reward: --\n",
      "  8357/100000: episode: 270, duration: 0.008s, episode steps: 24, steps per second: 3159, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.076 [-0.799, 1.623], mean_best_reward: --\n",
      "  8390/100000: episode: 271, duration: 0.010s, episode steps: 33, steps per second: 3347, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.084 [-0.603, 1.155], mean_best_reward: --\n",
      "  8438/100000: episode: 272, duration: 0.015s, episode steps: 48, steps per second: 3218, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.073 [-0.965, 1.719], mean_best_reward: --\n",
      "  8460/100000: episode: 273, duration: 0.010s, episode steps: 22, steps per second: 2307, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.076 [-1.418, 0.936], mean_best_reward: --\n",
      "  8476/100000: episode: 274, duration: 0.006s, episode steps: 16, steps per second: 2759, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.110 [-1.627, 0.771], mean_best_reward: --\n",
      "  8541/100000: episode: 275, duration: 0.024s, episode steps: 65, steps per second: 2732, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.072 [-1.302, 1.298], mean_best_reward: --\n",
      "  8567/100000: episode: 276, duration: 0.008s, episode steps: 26, steps per second: 3185, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.087 [-0.457, 1.315], mean_best_reward: --\n",
      "  8599/100000: episode: 277, duration: 0.010s, episode steps: 32, steps per second: 3211, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-0.439, 1.205], mean_best_reward: --\n",
      "  8642/100000: episode: 278, duration: 0.014s, episode steps: 43, steps per second: 3143, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.153 [-0.385, 0.955], mean_best_reward: --\n",
      "  8664/100000: episode: 279, duration: 0.007s, episode steps: 22, steps per second: 3133, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.443, 0.886], mean_best_reward: --\n",
      "  8689/100000: episode: 280, duration: 0.008s, episode steps: 25, steps per second: 3164, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.118 [-0.765, 1.171], mean_best_reward: --\n",
      "  8709/100000: episode: 281, duration: 0.007s, episode steps: 20, steps per second: 2990, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.086 [-1.163, 0.644], mean_best_reward: --\n",
      "  8745/100000: episode: 282, duration: 0.011s, episode steps: 36, steps per second: 3315, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.108 [-0.605, 0.927], mean_best_reward: --\n",
      "  8767/100000: episode: 283, duration: 0.007s, episode steps: 22, steps per second: 3033, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.091 [-0.652, 1.319], mean_best_reward: --\n",
      "  8780/100000: episode: 284, duration: 0.005s, episode steps: 13, steps per second: 2696, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.105 [-1.871, 1.161], mean_best_reward: --\n",
      "  8832/100000: episode: 285, duration: 0.016s, episode steps: 52, steps per second: 3280, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.211, 0.505], mean_best_reward: --\n",
      "  8896/100000: episode: 286, duration: 0.021s, episode steps: 64, steps per second: 3119, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.147 [-0.974, 0.619], mean_best_reward: --\n",
      "  8930/100000: episode: 287, duration: 0.011s, episode steps: 34, steps per second: 3006, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.002 [-1.206, 1.743], mean_best_reward: --\n",
      "  8960/100000: episode: 288, duration: 0.010s, episode steps: 30, steps per second: 3108, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.106 [-0.961, 0.584], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9030/100000: episode: 289, duration: 0.021s, episode steps: 70, steps per second: 3275, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.175 [-1.148, 1.171], mean_best_reward: --\n",
      "  9068/100000: episode: 290, duration: 0.013s, episode steps: 38, steps per second: 2917, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.092 [-1.018, 2.156], mean_best_reward: --\n",
      "  9111/100000: episode: 291, duration: 0.018s, episode steps: 43, steps per second: 2346, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.104 [-0.850, 1.727], mean_best_reward: --\n",
      "  9139/100000: episode: 292, duration: 0.011s, episode steps: 28, steps per second: 2531, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.799, 1.235], mean_best_reward: --\n",
      "  9161/100000: episode: 293, duration: 0.011s, episode steps: 22, steps per second: 1933, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.064 [-1.334, 2.063], mean_best_reward: --\n",
      "  9185/100000: episode: 294, duration: 0.009s, episode steps: 24, steps per second: 2640, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.079, 0.576], mean_best_reward: --\n",
      "  9230/100000: episode: 295, duration: 0.016s, episode steps: 45, steps per second: 2878, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.049 [-1.759, 0.939], mean_best_reward: --\n",
      "  9278/100000: episode: 296, duration: 0.016s, episode steps: 48, steps per second: 2955, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.054 [-0.598, 1.038], mean_best_reward: --\n",
      "  9359/100000: episode: 297, duration: 0.024s, episode steps: 81, steps per second: 3410, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.066 [-1.175, 1.754], mean_best_reward: --\n",
      "  9381/100000: episode: 298, duration: 0.007s, episode steps: 22, steps per second: 3124, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.098 [-0.947, 1.774], mean_best_reward: --\n",
      "  9454/100000: episode: 299, duration: 0.022s, episode steps: 73, steps per second: 3360, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.015 [-0.948, 1.651], mean_best_reward: --\n",
      "  9472/100000: episode: 300, duration: 0.006s, episode steps: 18, steps per second: 3005, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.092 [-0.761, 1.626], mean_best_reward: --\n",
      "  9561/100000: episode: 301, duration: 0.026s, episode steps: 89, steps per second: 3391, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.205 [-1.813, 0.932], mean_best_reward: 102.000000\n",
      "  9585/100000: episode: 302, duration: 0.008s, episode steps: 24, steps per second: 3141, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-1.011, 0.577], mean_best_reward: --\n",
      "  9604/100000: episode: 303, duration: 0.006s, episode steps: 19, steps per second: 3028, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.046 [-0.641, 1.072], mean_best_reward: --\n",
      "  9697/100000: episode: 304, duration: 0.034s, episode steps: 93, steps per second: 2722, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.123 [-1.221, 0.798], mean_best_reward: --\n",
      "  9710/100000: episode: 305, duration: 0.006s, episode steps: 13, steps per second: 2126, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.108 [-1.907, 1.157], mean_best_reward: --\n",
      "  9735/100000: episode: 306, duration: 0.009s, episode steps: 25, steps per second: 2896, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.096 [-0.572, 1.408], mean_best_reward: --\n",
      "  9806/100000: episode: 307, duration: 0.040s, episode steps: 71, steps per second: 1771, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.174 [-1.280, 1.386], mean_best_reward: --\n",
      "  9824/100000: episode: 308, duration: 0.012s, episode steps: 18, steps per second: 1440, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.075 [-0.797, 1.172], mean_best_reward: --\n",
      "  9835/100000: episode: 309, duration: 0.005s, episode steps: 11, steps per second: 2057, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.140 [-0.954, 1.692], mean_best_reward: --\n",
      "  9884/100000: episode: 310, duration: 0.021s, episode steps: 49, steps per second: 2345, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.009 [-0.867, 1.318], mean_best_reward: --\n",
      "  9909/100000: episode: 311, duration: 0.015s, episode steps: 25, steps per second: 1682, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.051 [-1.180, 1.910], mean_best_reward: --\n",
      "  9923/100000: episode: 312, duration: 0.012s, episode steps: 14, steps per second: 1172, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-1.126, 1.741], mean_best_reward: --\n",
      "  9941/100000: episode: 313, duration: 0.008s, episode steps: 18, steps per second: 2255, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.074 [-0.789, 1.309], mean_best_reward: --\n",
      "  9962/100000: episode: 314, duration: 0.009s, episode steps: 21, steps per second: 2450, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.100 [-0.551, 1.202], mean_best_reward: --\n",
      "  9973/100000: episode: 315, duration: 0.005s, episode steps: 11, steps per second: 2409, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.132 [-0.958, 1.756], mean_best_reward: --\n",
      " 10008/100000: episode: 316, duration: 0.011s, episode steps: 35, steps per second: 3063, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.056 [-0.938, 1.593], mean_best_reward: --\n",
      " 10029/100000: episode: 317, duration: 0.007s, episode steps: 21, steps per second: 3027, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.044 [-1.022, 1.768], mean_best_reward: --\n",
      " 10061/100000: episode: 318, duration: 0.010s, episode steps: 32, steps per second: 3253, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.083 [-0.609, 1.519], mean_best_reward: --\n",
      " 10081/100000: episode: 319, duration: 0.007s, episode steps: 20, steps per second: 3067, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.046 [-1.951, 1.402], mean_best_reward: --\n",
      " 10121/100000: episode: 320, duration: 0.013s, episode steps: 40, steps per second: 3124, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: -0.006 [-1.167, 1.866], mean_best_reward: --\n",
      " 10159/100000: episode: 321, duration: 0.012s, episode steps: 38, steps per second: 3119, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.120 [-0.554, 0.993], mean_best_reward: --\n",
      " 10218/100000: episode: 322, duration: 0.022s, episode steps: 59, steps per second: 2719, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.137 [-1.312, 0.777], mean_best_reward: --\n",
      " 10241/100000: episode: 323, duration: 0.008s, episode steps: 23, steps per second: 2891, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.070 [-1.148, 0.785], mean_best_reward: --\n",
      " 10254/100000: episode: 324, duration: 0.005s, episode steps: 13, steps per second: 2618, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.117 [-1.197, 2.026], mean_best_reward: --\n",
      " 10271/100000: episode: 325, duration: 0.006s, episode steps: 17, steps per second: 2811, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.103 [-0.641, 1.416], mean_best_reward: --\n",
      " 10313/100000: episode: 326, duration: 0.013s, episode steps: 42, steps per second: 3281, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.015 [-1.108, 1.436], mean_best_reward: --\n",
      " 10337/100000: episode: 327, duration: 0.008s, episode steps: 24, steps per second: 3125, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.120 [-0.586, 1.500], mean_best_reward: --\n",
      " 10388/100000: episode: 328, duration: 0.015s, episode steps: 51, steps per second: 3381, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.056 [-0.820, 1.141], mean_best_reward: --\n",
      " 10401/100000: episode: 329, duration: 0.004s, episode steps: 13, steps per second: 3054, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.100 [-0.978, 1.758], mean_best_reward: --\n",
      " 10419/100000: episode: 330, duration: 0.006s, episode steps: 18, steps per second: 3051, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.085 [-0.652, 1.365], mean_best_reward: --\n",
      " 10461/100000: episode: 331, duration: 0.013s, episode steps: 42, steps per second: 3309, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.053 [-1.149, 0.557], mean_best_reward: --\n",
      " 10493/100000: episode: 332, duration: 0.010s, episode steps: 32, steps per second: 3114, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.048 [-0.622, 1.113], mean_best_reward: --\n",
      " 10535/100000: episode: 333, duration: 0.013s, episode steps: 42, steps per second: 3310, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-1.143, 0.771], mean_best_reward: --\n",
      " 10547/100000: episode: 334, duration: 0.004s, episode steps: 12, steps per second: 2742, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.594, 2.357], mean_best_reward: --\n",
      " 10596/100000: episode: 335, duration: 0.015s, episode steps: 49, steps per second: 3306, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.091 [-0.744, 1.182], mean_best_reward: --\n",
      " 10656/100000: episode: 336, duration: 0.018s, episode steps: 60, steps per second: 3262, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.168 [-1.150, 0.861], mean_best_reward: --\n",
      " 10667/100000: episode: 337, duration: 0.004s, episode steps: 11, steps per second: 2475, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.122 [-0.764, 1.269], mean_best_reward: --\n",
      " 10686/100000: episode: 338, duration: 0.007s, episode steps: 19, steps per second: 2772, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.129 [-0.616, 1.570], mean_best_reward: --\n",
      " 10712/100000: episode: 339, duration: 0.008s, episode steps: 26, steps per second: 3081, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.040 [-1.173, 1.881], mean_best_reward: --\n",
      " 10729/100000: episode: 340, duration: 0.006s, episode steps: 17, steps per second: 2996, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.122 [-0.559, 1.191], mean_best_reward: --\n",
      " 10753/100000: episode: 341, duration: 0.008s, episode steps: 24, steps per second: 3067, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.096 [-0.578, 0.940], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10838/100000: episode: 342, duration: 0.032s, episode steps: 85, steps per second: 2666, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.044 [-0.978, 1.538], mean_best_reward: --\n",
      " 10861/100000: episode: 343, duration: 0.008s, episode steps: 23, steps per second: 2846, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.072 [-1.186, 0.572], mean_best_reward: --\n",
      " 10884/100000: episode: 344, duration: 0.009s, episode steps: 23, steps per second: 2570, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.047 [-2.064, 1.329], mean_best_reward: --\n",
      " 10923/100000: episode: 345, duration: 0.013s, episode steps: 39, steps per second: 3060, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.083 [-0.853, 1.393], mean_best_reward: --\n",
      " 10966/100000: episode: 346, duration: 0.013s, episode steps: 43, steps per second: 3185, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.096 [-0.663, 1.297], mean_best_reward: --\n",
      " 10984/100000: episode: 347, duration: 0.006s, episode steps: 18, steps per second: 2973, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.117 [-0.781, 1.597], mean_best_reward: --\n",
      " 11009/100000: episode: 348, duration: 0.008s, episode steps: 25, steps per second: 3092, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.072 [-0.927, 1.432], mean_best_reward: --\n",
      " 11035/100000: episode: 349, duration: 0.008s, episode steps: 26, steps per second: 3097, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.552, 0.912], mean_best_reward: --\n",
      " 11051/100000: episode: 350, duration: 0.006s, episode steps: 16, steps per second: 2904, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.065 [-0.817, 1.155], mean_best_reward: --\n",
      " 11093/100000: episode: 351, duration: 0.014s, episode steps: 42, steps per second: 3042, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.784, 1.044], mean_best_reward: 125.000000\n",
      " 11113/100000: episode: 352, duration: 0.007s, episode steps: 20, steps per second: 2844, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.012, 0.407], mean_best_reward: --\n",
      " 11138/100000: episode: 353, duration: 0.008s, episode steps: 25, steps per second: 3127, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.065 [-1.088, 0.753], mean_best_reward: --\n",
      " 11157/100000: episode: 354, duration: 0.006s, episode steps: 19, steps per second: 2978, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.093 [-0.562, 1.392], mean_best_reward: --\n",
      " 11206/100000: episode: 355, duration: 0.014s, episode steps: 49, steps per second: 3402, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.061 [-1.187, 1.620], mean_best_reward: --\n",
      " 11249/100000: episode: 356, duration: 0.013s, episode steps: 43, steps per second: 3351, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.024 [-0.783, 1.619], mean_best_reward: --\n",
      " 11268/100000: episode: 357, duration: 0.006s, episode steps: 19, steps per second: 3012, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.094 [-0.542, 1.029], mean_best_reward: --\n",
      " 11285/100000: episode: 358, duration: 0.006s, episode steps: 17, steps per second: 3023, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.106 [-1.267, 0.742], mean_best_reward: --\n",
      " 11386/100000: episode: 359, duration: 0.028s, episode steps: 101, steps per second: 3548, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.208 [-1.697, 2.649], mean_best_reward: --\n",
      " 11398/100000: episode: 360, duration: 0.005s, episode steps: 12, steps per second: 2642, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.108 [-1.463, 0.837], mean_best_reward: --\n",
      " 11434/100000: episode: 361, duration: 0.016s, episode steps: 36, steps per second: 2303, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.002 [-1.381, 1.779], mean_best_reward: --\n",
      " 11472/100000: episode: 362, duration: 0.016s, episode steps: 38, steps per second: 2400, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.074 [-1.170, 0.559], mean_best_reward: --\n",
      " 11504/100000: episode: 363, duration: 0.013s, episode steps: 32, steps per second: 2443, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.123 [-0.645, 1.030], mean_best_reward: --\n",
      " 11517/100000: episode: 364, duration: 0.005s, episode steps: 13, steps per second: 2762, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.099 [-0.948, 1.739], mean_best_reward: --\n",
      " 11536/100000: episode: 365, duration: 0.007s, episode steps: 19, steps per second: 2701, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.082 [-0.962, 0.633], mean_best_reward: --\n",
      " 11585/100000: episode: 366, duration: 0.018s, episode steps: 49, steps per second: 2754, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.212 [-1.195, 0.793], mean_best_reward: --\n",
      " 11599/100000: episode: 367, duration: 0.005s, episode steps: 14, steps per second: 2828, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.547, 1.019], mean_best_reward: --\n",
      " 11651/100000: episode: 368, duration: 0.016s, episode steps: 52, steps per second: 3202, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.023 [-1.499, 1.158], mean_best_reward: --\n",
      " 11670/100000: episode: 369, duration: 0.006s, episode steps: 19, steps per second: 3022, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.060 [-1.313, 0.823], mean_best_reward: --\n",
      " 11686/100000: episode: 370, duration: 0.005s, episode steps: 16, steps per second: 3021, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.078 [-1.217, 2.067], mean_best_reward: --\n",
      " 11724/100000: episode: 371, duration: 0.012s, episode steps: 38, steps per second: 3278, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.049 [-0.898, 1.452], mean_best_reward: --\n",
      " 11749/100000: episode: 372, duration: 0.009s, episode steps: 25, steps per second: 2935, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.086 [-1.196, 0.769], mean_best_reward: --\n",
      " 11808/100000: episode: 373, duration: 0.017s, episode steps: 59, steps per second: 3440, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.121 [-1.367, 1.355], mean_best_reward: --\n",
      " 11868/100000: episode: 374, duration: 0.018s, episode steps: 60, steps per second: 3360, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.042 [-0.908, 1.651], mean_best_reward: --\n",
      " 12016/100000: episode: 375, duration: 0.042s, episode steps: 148, steps per second: 3499, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.267 [-1.628, 1.083], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12089/100000: episode: 376, duration: 0.025s, episode steps: 73, steps per second: 2890, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.095 [-1.077, 1.256], mean_best_reward: --\n",
      " 12120/100000: episode: 377, duration: 0.013s, episode steps: 31, steps per second: 2400, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.056 [-1.011, 1.254], mean_best_reward: --\n",
      " 12156/100000: episode: 378, duration: 0.012s, episode steps: 36, steps per second: 2985, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.197 [-0.599, 1.129], mean_best_reward: --\n",
      " 12183/100000: episode: 379, duration: 0.009s, episode steps: 27, steps per second: 3012, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.104 [-1.215, 0.786], mean_best_reward: --\n",
      " 12197/100000: episode: 380, duration: 0.005s, episode steps: 14, steps per second: 2755, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.079 [-2.134, 1.363], mean_best_reward: --\n",
      " 12227/100000: episode: 381, duration: 0.010s, episode steps: 30, steps per second: 2961, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.050 [-1.296, 0.740], mean_best_reward: --\n",
      " 12293/100000: episode: 382, duration: 0.020s, episode steps: 66, steps per second: 3306, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.121 [-1.179, 0.778], mean_best_reward: --\n",
      " 12323/100000: episode: 383, duration: 0.010s, episode steps: 30, steps per second: 3142, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.121 [-1.641, 0.609], mean_best_reward: --\n",
      " 12391/100000: episode: 384, duration: 0.020s, episode steps: 68, steps per second: 3386, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.016 [-1.215, 0.907], mean_best_reward: --\n",
      " 12408/100000: episode: 385, duration: 0.006s, episode steps: 17, steps per second: 2997, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.083 [-1.570, 0.980], mean_best_reward: --\n",
      " 12433/100000: episode: 386, duration: 0.008s, episode steps: 25, steps per second: 3057, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.064 [-0.755, 1.511], mean_best_reward: --\n",
      " 12449/100000: episode: 387, duration: 0.006s, episode steps: 16, steps per second: 2755, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.095 [-1.413, 0.970], mean_best_reward: --\n",
      " 12501/100000: episode: 388, duration: 0.016s, episode steps: 52, steps per second: 3190, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.179 [-1.114, 1.431], mean_best_reward: --\n",
      " 12515/100000: episode: 389, duration: 0.005s, episode steps: 14, steps per second: 2864, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.316, 0.762], mean_best_reward: --\n",
      " 12613/100000: episode: 390, duration: 0.029s, episode steps: 98, steps per second: 3416, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.367 [-2.084, 1.035], mean_best_reward: --\n",
      " 12641/100000: episode: 391, duration: 0.009s, episode steps: 28, steps per second: 3169, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.181, 0.443], mean_best_reward: --\n",
      " 12669/100000: episode: 392, duration: 0.009s, episode steps: 28, steps per second: 3186, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: 0.011 [-1.954, 1.576], mean_best_reward: --\n",
      " 12684/100000: episode: 393, duration: 0.006s, episode steps: 15, steps per second: 2721, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.103 [-0.766, 1.385], mean_best_reward: --\n",
      " 12737/100000: episode: 394, duration: 0.019s, episode steps: 53, steps per second: 2834, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.145 [-1.121, 1.313], mean_best_reward: --\n",
      " 12819/100000: episode: 395, duration: 0.028s, episode steps: 82, steps per second: 2906, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-1.116, 1.204], mean_best_reward: --\n",
      " 12902/100000: episode: 396, duration: 0.024s, episode steps: 83, steps per second: 3390, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.208 [-1.698, 1.025], mean_best_reward: --\n",
      " 12930/100000: episode: 397, duration: 0.009s, episode steps: 28, steps per second: 2996, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.161, 0.634], mean_best_reward: --\n",
      " 12983/100000: episode: 398, duration: 0.018s, episode steps: 53, steps per second: 2924, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.142 [-0.577, 1.000], mean_best_reward: --\n",
      " 13032/100000: episode: 399, duration: 0.017s, episode steps: 49, steps per second: 2830, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.095 [-0.571, 1.076], mean_best_reward: --\n",
      " 13073/100000: episode: 400, duration: 0.016s, episode steps: 41, steps per second: 2569, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.137 [-0.655, 1.189], mean_best_reward: --\n",
      " 13106/100000: episode: 401, duration: 0.018s, episode steps: 33, steps per second: 1885, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.118 [-0.442, 0.832], mean_best_reward: 117.500000\n",
      " 13135/100000: episode: 402, duration: 0.014s, episode steps: 29, steps per second: 2012, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.070 [-0.667, 1.525], mean_best_reward: --\n",
      " 13181/100000: episode: 403, duration: 0.017s, episode steps: 46, steps per second: 2646, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.064 [-0.774, 1.294], mean_best_reward: --\n",
      " 13242/100000: episode: 404, duration: 0.022s, episode steps: 61, steps per second: 2786, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.184 [-0.752, 1.441], mean_best_reward: --\n",
      " 13276/100000: episode: 405, duration: 0.015s, episode steps: 34, steps per second: 2303, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.039 [-1.401, 0.803], mean_best_reward: --\n",
      " 13322/100000: episode: 406, duration: 0.023s, episode steps: 46, steps per second: 2031, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.135 [-1.515, 1.334], mean_best_reward: --\n",
      " 13336/100000: episode: 407, duration: 0.006s, episode steps: 14, steps per second: 2319, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.076 [-1.210, 1.924], mean_best_reward: --\n",
      " 13396/100000: episode: 408, duration: 0.019s, episode steps: 60, steps per second: 3138, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.118 [-1.098, 0.853], mean_best_reward: --\n",
      " 13422/100000: episode: 409, duration: 0.009s, episode steps: 26, steps per second: 2748, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.620, 1.108], mean_best_reward: --\n",
      " 13465/100000: episode: 410, duration: 0.013s, episode steps: 43, steps per second: 3340, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.101 [-0.989, 0.470], mean_best_reward: --\n",
      " 13483/100000: episode: 411, duration: 0.006s, episode steps: 18, steps per second: 2862, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.554, 1.136], mean_best_reward: --\n",
      " 13509/100000: episode: 412, duration: 0.009s, episode steps: 26, steps per second: 3058, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.387, 1.110], mean_best_reward: --\n",
      " 13530/100000: episode: 413, duration: 0.007s, episode steps: 21, steps per second: 3053, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.086 [-0.756, 1.207], mean_best_reward: --\n",
      " 13567/100000: episode: 414, duration: 0.012s, episode steps: 37, steps per second: 3027, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.114 [-0.931, 1.451], mean_best_reward: --\n",
      " 13628/100000: episode: 415, duration: 0.022s, episode steps: 61, steps per second: 2826, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.041 [-1.324, 1.137], mean_best_reward: --\n",
      " 13656/100000: episode: 416, duration: 0.009s, episode steps: 28, steps per second: 3027, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.039 [-1.491, 0.997], mean_best_reward: --\n",
      " 13721/100000: episode: 417, duration: 0.020s, episode steps: 65, steps per second: 3244, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.170 [-1.302, 1.069], mean_best_reward: --\n",
      " 13741/100000: episode: 418, duration: 0.007s, episode steps: 20, steps per second: 3038, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.048 [-2.478, 1.587], mean_best_reward: --\n",
      " 13758/100000: episode: 419, duration: 0.006s, episode steps: 17, steps per second: 3050, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.119 [-0.543, 0.991], mean_best_reward: --\n",
      " 13771/100000: episode: 420, duration: 0.005s, episode steps: 13, steps per second: 2864, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.084 [-1.015, 1.651], mean_best_reward: --\n",
      " 13796/100000: episode: 421, duration: 0.008s, episode steps: 25, steps per second: 3049, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.112 [-0.583, 1.562], mean_best_reward: --\n",
      " 13827/100000: episode: 422, duration: 0.010s, episode steps: 31, steps per second: 3261, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.115 [-0.765, 1.810], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13845/100000: episode: 423, duration: 0.006s, episode steps: 18, steps per second: 2967, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.051 [-0.984, 1.544], mean_best_reward: --\n",
      " 13889/100000: episode: 424, duration: 0.014s, episode steps: 44, steps per second: 3068, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.768, 1.363], mean_best_reward: --\n",
      " 13927/100000: episode: 425, duration: 0.012s, episode steps: 38, steps per second: 3126, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.063 [-0.620, 1.597], mean_best_reward: --\n",
      " 13973/100000: episode: 426, duration: 0.016s, episode steps: 46, steps per second: 2854, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.069 [-1.455, 0.594], mean_best_reward: --\n",
      " 14000/100000: episode: 427, duration: 0.009s, episode steps: 27, steps per second: 3086, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.053 [-1.642, 1.009], mean_best_reward: --\n",
      " 14033/100000: episode: 428, duration: 0.010s, episode steps: 33, steps per second: 3234, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.019 [-1.230, 0.971], mean_best_reward: --\n",
      " 14052/100000: episode: 429, duration: 0.006s, episode steps: 19, steps per second: 2935, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.106 [-0.579, 1.123], mean_best_reward: --\n",
      " 14139/100000: episode: 430, duration: 0.026s, episode steps: 87, steps per second: 3373, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.008 [-1.527, 1.484], mean_best_reward: --\n",
      " 14276/100000: episode: 431, duration: 0.039s, episode steps: 137, steps per second: 3480, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.086 [-1.348, 1.466], mean_best_reward: --\n",
      " 14291/100000: episode: 432, duration: 0.005s, episode steps: 15, steps per second: 2784, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.107 [-0.564, 1.320], mean_best_reward: --\n",
      " 14321/100000: episode: 433, duration: 0.009s, episode steps: 30, steps per second: 3166, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.040 [-1.694, 0.952], mean_best_reward: --\n",
      " 14338/100000: episode: 434, duration: 0.006s, episode steps: 17, steps per second: 2911, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.097 [-1.240, 0.750], mean_best_reward: --\n",
      " 14400/100000: episode: 435, duration: 0.019s, episode steps: 62, steps per second: 3318, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.167 [-1.255, 0.622], mean_best_reward: --\n",
      " 14416/100000: episode: 436, duration: 0.006s, episode steps: 16, steps per second: 2898, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.114 [-1.839, 0.995], mean_best_reward: --\n",
      " 14439/100000: episode: 437, duration: 0.008s, episode steps: 23, steps per second: 2991, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.093 [-0.777, 1.156], mean_best_reward: --\n",
      " 14466/100000: episode: 438, duration: 0.010s, episode steps: 27, steps per second: 2814, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.028 [-1.665, 1.198], mean_best_reward: --\n",
      " 14486/100000: episode: 439, duration: 0.011s, episode steps: 20, steps per second: 1877, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.066 [-0.751, 1.254], mean_best_reward: --\n",
      " 14500/100000: episode: 440, duration: 0.007s, episode steps: 14, steps per second: 1943, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.104 [-1.589, 0.959], mean_best_reward: --\n",
      " 14542/100000: episode: 441, duration: 0.016s, episode steps: 42, steps per second: 2558, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-1.143, 0.816], mean_best_reward: --\n",
      " 14567/100000: episode: 442, duration: 0.009s, episode steps: 25, steps per second: 2682, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.090 [-0.623, 1.148], mean_best_reward: --\n",
      " 14595/100000: episode: 443, duration: 0.014s, episode steps: 28, steps per second: 2060, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-0.979, 1.421], mean_best_reward: --\n",
      " 14615/100000: episode: 444, duration: 0.010s, episode steps: 20, steps per second: 1955, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.081 [-0.752, 1.223], mean_best_reward: --\n",
      " 14655/100000: episode: 445, duration: 0.018s, episode steps: 40, steps per second: 2178, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.800, 1.100], mean_best_reward: --\n",
      " 14702/100000: episode: 446, duration: 0.020s, episode steps: 47, steps per second: 2372, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.044 [-0.846, 0.881], mean_best_reward: --\n",
      " 14734/100000: episode: 447, duration: 0.014s, episode steps: 32, steps per second: 2219, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.057 [-0.757, 1.227], mean_best_reward: --\n",
      " 14786/100000: episode: 448, duration: 0.017s, episode steps: 52, steps per second: 2976, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.046 [-0.746, 1.080], mean_best_reward: --\n",
      " 14821/100000: episode: 449, duration: 0.011s, episode steps: 35, steps per second: 3073, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.010 [-1.500, 2.012], mean_best_reward: --\n",
      " 14883/100000: episode: 450, duration: 0.019s, episode steps: 62, steps per second: 3280, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-1.121, 1.247], mean_best_reward: --\n",
      " 14907/100000: episode: 451, duration: 0.008s, episode steps: 24, steps per second: 2864, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.080 [-1.120, 0.437], mean_best_reward: 99.500000\n",
      " 14934/100000: episode: 452, duration: 0.008s, episode steps: 27, steps per second: 3320, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.095 [-0.420, 0.763], mean_best_reward: --\n",
      " 14951/100000: episode: 453, duration: 0.006s, episode steps: 17, steps per second: 2871, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.078 [-0.751, 1.234], mean_best_reward: --\n",
      " 14999/100000: episode: 454, duration: 0.017s, episode steps: 48, steps per second: 2871, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.396 [0.000, 1.000], mean observation: -0.072 [-1.957, 2.099], mean_best_reward: --\n",
      " 15124/100000: episode: 455, duration: 0.038s, episode steps: 125, steps per second: 3287, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.068 [-0.966, 1.280], mean_best_reward: --\n",
      " 15159/100000: episode: 456, duration: 0.011s, episode steps: 35, steps per second: 3125, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.094 [-1.434, 0.574], mean_best_reward: --\n",
      " 15241/100000: episode: 457, duration: 0.025s, episode steps: 82, steps per second: 3326, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.122 [-1.919, 1.525], mean_best_reward: --\n",
      " 15274/100000: episode: 458, duration: 0.010s, episode steps: 33, steps per second: 3144, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.065 [-1.250, 0.746], mean_best_reward: --\n",
      " 15336/100000: episode: 459, duration: 0.018s, episode steps: 62, steps per second: 3454, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.195 [-1.700, 0.810], mean_best_reward: --\n",
      " 15353/100000: episode: 460, duration: 0.006s, episode steps: 17, steps per second: 2866, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.110 [-1.352, 0.753], mean_best_reward: --\n",
      " 15376/100000: episode: 461, duration: 0.007s, episode steps: 23, steps per second: 3257, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.129 [-1.252, 0.396], mean_best_reward: --\n",
      " 15411/100000: episode: 462, duration: 0.010s, episode steps: 35, steps per second: 3359, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.132 [-0.398, 1.100], mean_best_reward: --\n",
      " 15431/100000: episode: 463, duration: 0.006s, episode steps: 20, steps per second: 3112, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.053 [-0.816, 1.282], mean_best_reward: --\n",
      " 15460/100000: episode: 464, duration: 0.009s, episode steps: 29, steps per second: 3296, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.069 [-0.550, 1.115], mean_best_reward: --\n",
      " 15522/100000: episode: 465, duration: 0.018s, episode steps: 62, steps per second: 3513, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.058 [-1.317, 1.555], mean_best_reward: --\n",
      " 15582/100000: episode: 466, duration: 0.018s, episode steps: 60, steps per second: 3366, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.151 [-0.752, 1.387], mean_best_reward: --\n",
      " 15619/100000: episode: 467, duration: 0.012s, episode steps: 37, steps per second: 3038, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.052 [-0.785, 1.156], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15649/100000: episode: 468, duration: 0.010s, episode steps: 30, steps per second: 3063, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.776, 1.370], mean_best_reward: --\n",
      " 15690/100000: episode: 469, duration: 0.019s, episode steps: 41, steps per second: 2215, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.072 [-1.033, 0.638], mean_best_reward: --\n",
      " 15721/100000: episode: 470, duration: 0.012s, episode steps: 31, steps per second: 2640, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.134 [-0.924, 0.551], mean_best_reward: --\n",
      " 15731/100000: episode: 471, duration: 0.004s, episode steps: 10, steps per second: 2533, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.127 [-0.788, 1.387], mean_best_reward: --\n",
      " 15758/100000: episode: 472, duration: 0.009s, episode steps: 27, steps per second: 3011, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.040 [-1.115, 1.478], mean_best_reward: --\n",
      " 15787/100000: episode: 473, duration: 0.009s, episode steps: 29, steps per second: 3057, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.118 [-0.590, 0.998], mean_best_reward: --\n",
      " 15854/100000: episode: 474, duration: 0.020s, episode steps: 67, steps per second: 3304, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.234 [-0.899, 1.585], mean_best_reward: --\n",
      " 15869/100000: episode: 475, duration: 0.005s, episode steps: 15, steps per second: 2774, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.080 [-1.007, 1.495], mean_best_reward: --\n",
      " 15890/100000: episode: 476, duration: 0.007s, episode steps: 21, steps per second: 3147, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.063 [-0.992, 1.543], mean_best_reward: --\n",
      " 15918/100000: episode: 477, duration: 0.009s, episode steps: 28, steps per second: 3115, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.770, 1.324], mean_best_reward: --\n",
      " 15988/100000: episode: 478, duration: 0.022s, episode steps: 70, steps per second: 3208, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.084 [-1.167, 1.580], mean_best_reward: --\n",
      " 16034/100000: episode: 479, duration: 0.015s, episode steps: 46, steps per second: 3167, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.573, 1.246], mean_best_reward: --\n",
      " 16053/100000: episode: 480, duration: 0.006s, episode steps: 19, steps per second: 3077, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.130 [-0.540, 0.952], mean_best_reward: --\n",
      " 16071/100000: episode: 481, duration: 0.006s, episode steps: 18, steps per second: 3070, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.076 [-0.608, 1.155], mean_best_reward: --\n",
      " 16086/100000: episode: 482, duration: 0.005s, episode steps: 15, steps per second: 3318, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.080 [-0.817, 1.347], mean_best_reward: --\n",
      " 16132/100000: episode: 483, duration: 0.013s, episode steps: 46, steps per second: 3453, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.034 [-1.072, 1.374], mean_best_reward: --\n",
      " 16194/100000: episode: 484, duration: 0.019s, episode steps: 62, steps per second: 3320, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.094 [-0.809, 1.144], mean_best_reward: --\n",
      " 16209/100000: episode: 485, duration: 0.005s, episode steps: 15, steps per second: 3035, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.111 [-0.604, 1.085], mean_best_reward: --\n",
      " 16287/100000: episode: 486, duration: 0.025s, episode steps: 78, steps per second: 3149, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.916, 1.085], mean_best_reward: --\n",
      " 16302/100000: episode: 487, duration: 0.005s, episode steps: 15, steps per second: 3148, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.097 [-0.787, 1.417], mean_best_reward: --\n",
      " 16427/100000: episode: 488, duration: 0.038s, episode steps: 125, steps per second: 3293, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.107 [-2.808, 2.177], mean_best_reward: --\n",
      " 16456/100000: episode: 489, duration: 0.009s, episode steps: 29, steps per second: 3141, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.012 [-1.993, 1.411], mean_best_reward: --\n",
      " 16473/100000: episode: 490, duration: 0.006s, episode steps: 17, steps per second: 2823, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.103 [-1.488, 0.775], mean_best_reward: --\n",
      " 16534/100000: episode: 491, duration: 0.018s, episode steps: 61, steps per second: 3320, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.071 [-0.776, 1.008], mean_best_reward: --\n",
      " 16604/100000: episode: 492, duration: 0.018s, episode steps: 70, steps per second: 3907, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.147 [-1.346, 1.410], mean_best_reward: --\n",
      " 16629/100000: episode: 493, duration: 0.008s, episode steps: 25, steps per second: 3260, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.066 [-1.195, 0.831], mean_best_reward: --\n",
      " 16645/100000: episode: 494, duration: 0.005s, episode steps: 16, steps per second: 3069, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.088 [-0.754, 1.365], mean_best_reward: --\n",
      " 16678/100000: episode: 495, duration: 0.010s, episode steps: 33, steps per second: 3236, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.576 [0.000, 1.000], mean observation: -0.053 [-1.768, 0.943], mean_best_reward: --\n",
      " 16695/100000: episode: 496, duration: 0.005s, episode steps: 17, steps per second: 3335, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.120 [-0.558, 1.332], mean_best_reward: --\n",
      " 16715/100000: episode: 497, duration: 0.007s, episode steps: 20, steps per second: 3011, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.091 [-0.628, 1.021], mean_best_reward: --\n",
      " 16730/100000: episode: 498, duration: 0.005s, episode steps: 15, steps per second: 3077, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.086 [-1.015, 1.610], mean_best_reward: --\n",
      " 16747/100000: episode: 499, duration: 0.005s, episode steps: 17, steps per second: 3274, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.073 [-1.777, 0.985], mean_best_reward: --\n",
      " 16780/100000: episode: 500, duration: 0.009s, episode steps: 33, steps per second: 3545, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.082 [-0.427, 1.099], mean_best_reward: --\n",
      " 16858/100000: episode: 501, duration: 0.023s, episode steps: 78, steps per second: 3384, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.952, 1.022], mean_best_reward: 134.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16955/100000: episode: 502, duration: 0.027s, episode steps: 97, steps per second: 3548, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.546 [0.000, 1.000], mean observation: 0.272 [-2.001, 2.853], mean_best_reward: --\n",
      " 17006/100000: episode: 503, duration: 0.019s, episode steps: 51, steps per second: 2676, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.036 [-1.489, 1.182], mean_best_reward: --\n",
      " 17103/100000: episode: 504, duration: 0.029s, episode steps: 97, steps per second: 3359, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.062 [-1.186, 1.283], mean_best_reward: --\n",
      " 17147/100000: episode: 505, duration: 0.014s, episode steps: 44, steps per second: 3238, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.754, 1.189], mean_best_reward: --\n",
      " 17249/100000: episode: 506, duration: 0.030s, episode steps: 102, steps per second: 3395, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.055 [-1.485, 1.292], mean_best_reward: --\n",
      " 17266/100000: episode: 507, duration: 0.006s, episode steps: 17, steps per second: 2930, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.103 [-0.594, 1.208], mean_best_reward: --\n",
      " 17344/100000: episode: 508, duration: 0.023s, episode steps: 78, steps per second: 3415, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.209 [-1.004, 1.267], mean_best_reward: --\n",
      " 17400/100000: episode: 509, duration: 0.017s, episode steps: 56, steps per second: 3310, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.102 [-1.167, 0.544], mean_best_reward: --\n",
      " 17488/100000: episode: 510, duration: 0.026s, episode steps: 88, steps per second: 3356, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.116 [-0.992, 1.075], mean_best_reward: --\n",
      " 17514/100000: episode: 511, duration: 0.008s, episode steps: 26, steps per second: 3074, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.066 [-0.613, 1.060], mean_best_reward: --\n",
      " 17564/100000: episode: 512, duration: 0.014s, episode steps: 50, steps per second: 3501, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.143 [-1.466, 1.695], mean_best_reward: --\n",
      " 17574/100000: episode: 513, duration: 0.004s, episode steps: 10, steps per second: 2476, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.126 [-0.951, 1.558], mean_best_reward: --\n",
      " 17606/100000: episode: 514, duration: 0.012s, episode steps: 32, steps per second: 2675, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.124 [-0.582, 0.877], mean_best_reward: --\n",
      " 17625/100000: episode: 515, duration: 0.008s, episode steps: 19, steps per second: 2426, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.079 [-0.771, 1.258], mean_best_reward: --\n",
      " 17646/100000: episode: 516, duration: 0.009s, episode steps: 21, steps per second: 2467, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.676, 0.975], mean_best_reward: --\n",
      " 17674/100000: episode: 517, duration: 0.009s, episode steps: 28, steps per second: 3017, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.157 [-1.382, 0.539], mean_best_reward: --\n",
      " 17691/100000: episode: 518, duration: 0.009s, episode steps: 17, steps per second: 1972, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.108 [-1.050, 0.566], mean_best_reward: --\n",
      " 17758/100000: episode: 519, duration: 0.022s, episode steps: 67, steps per second: 3066, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.029 [-1.002, 1.362], mean_best_reward: --\n",
      " 17789/100000: episode: 520, duration: 0.010s, episode steps: 31, steps per second: 2985, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.355 [0.000, 1.000], mean observation: -0.034 [-1.791, 2.426], mean_best_reward: --\n",
      " 17805/100000: episode: 521, duration: 0.006s, episode steps: 16, steps per second: 2800, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.063 [-0.983, 1.550], mean_best_reward: --\n",
      " 17825/100000: episode: 522, duration: 0.007s, episode steps: 20, steps per second: 3050, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.041 [-1.179, 1.801], mean_best_reward: --\n",
      " 17859/100000: episode: 523, duration: 0.011s, episode steps: 34, steps per second: 3234, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.015 [-0.947, 1.569], mean_best_reward: --\n",
      " 17917/100000: episode: 524, duration: 0.017s, episode steps: 58, steps per second: 3460, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: 0.009 [-1.055, 1.392], mean_best_reward: --\n",
      " 17939/100000: episode: 525, duration: 0.007s, episode steps: 22, steps per second: 3213, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.039 [-1.010, 1.666], mean_best_reward: --\n",
      " 17953/100000: episode: 526, duration: 0.005s, episode steps: 14, steps per second: 2906, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.112 [-1.146, 2.042], mean_best_reward: --\n",
      " 17970/100000: episode: 527, duration: 0.006s, episode steps: 17, steps per second: 3001, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.093 [-0.625, 0.995], mean_best_reward: --\n",
      " 18004/100000: episode: 528, duration: 0.011s, episode steps: 34, steps per second: 3214, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.593, 1.179], mean_best_reward: --\n",
      " 18027/100000: episode: 529, duration: 0.008s, episode steps: 23, steps per second: 3043, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.096 [-1.106, 0.616], mean_best_reward: --\n",
      " 18087/100000: episode: 530, duration: 0.018s, episode steps: 60, steps per second: 3314, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.014 [-1.370, 2.109], mean_best_reward: --\n",
      " 18145/100000: episode: 531, duration: 0.017s, episode steps: 58, steps per second: 3347, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-0.577, 1.213], mean_best_reward: --\n",
      " 18214/100000: episode: 532, duration: 0.021s, episode steps: 69, steps per second: 3240, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.006 [-0.833, 1.287], mean_best_reward: --\n",
      " 18250/100000: episode: 533, duration: 0.012s, episode steps: 36, steps per second: 3002, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.140 [-0.631, 1.070], mean_best_reward: --\n",
      " 18271/100000: episode: 534, duration: 0.006s, episode steps: 21, steps per second: 3320, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.074 [-0.630, 0.912], mean_best_reward: --\n",
      " 18288/100000: episode: 535, duration: 0.008s, episode steps: 17, steps per second: 2013, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.091 [-1.314, 0.815], mean_best_reward: --\n",
      " 18321/100000: episode: 536, duration: 0.011s, episode steps: 33, steps per second: 3108, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.004 [-1.031, 1.700], mean_best_reward: --\n",
      " 18343/100000: episode: 537, duration: 0.008s, episode steps: 22, steps per second: 2901, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.068 [-1.194, 2.139], mean_best_reward: --\n",
      " 18362/100000: episode: 538, duration: 0.007s, episode steps: 19, steps per second: 2824, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.103 [-1.098, 0.590], mean_best_reward: --\n",
      " 18386/100000: episode: 539, duration: 0.008s, episode steps: 24, steps per second: 3195, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.120 [-0.608, 0.982], mean_best_reward: --\n",
      " 18445/100000: episode: 540, duration: 0.018s, episode steps: 59, steps per second: 3253, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.014 [-1.227, 1.098], mean_best_reward: --\n",
      " 18462/100000: episode: 541, duration: 0.006s, episode steps: 17, steps per second: 2876, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.099 [-0.575, 1.316], mean_best_reward: --\n",
      " 18517/100000: episode: 542, duration: 0.017s, episode steps: 55, steps per second: 3262, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.019 [-0.989, 1.537], mean_best_reward: --\n",
      " 18550/100000: episode: 543, duration: 0.010s, episode steps: 33, steps per second: 3249, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.068 [-1.167, 0.878], mean_best_reward: --\n",
      " 18571/100000: episode: 544, duration: 0.007s, episode steps: 21, steps per second: 3067, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.072 [-0.764, 1.295], mean_best_reward: --\n",
      " 18590/100000: episode: 545, duration: 0.006s, episode steps: 19, steps per second: 3053, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.088 [-1.136, 0.573], mean_best_reward: --\n",
      " 18617/100000: episode: 546, duration: 0.008s, episode steps: 27, steps per second: 3206, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.071 [-0.617, 1.456], mean_best_reward: --\n",
      " 18646/100000: episode: 547, duration: 0.009s, episode steps: 29, steps per second: 3152, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.080 [-0.556, 1.350], mean_best_reward: --\n",
      " 18658/100000: episode: 548, duration: 0.004s, episode steps: 12, steps per second: 2847, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.115 [-0.974, 1.601], mean_best_reward: --\n",
      " 18673/100000: episode: 549, duration: 0.005s, episode steps: 15, steps per second: 2956, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.111 [-1.274, 0.743], mean_best_reward: --\n",
      " 18718/100000: episode: 550, duration: 0.014s, episode steps: 45, steps per second: 3189, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.016 [-1.785, 0.982], mean_best_reward: --\n",
      " 18766/100000: episode: 551, duration: 0.015s, episode steps: 48, steps per second: 3167, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.065 [-1.327, 1.619], mean_best_reward: 105.500000\n",
      " 18789/100000: episode: 552, duration: 0.008s, episode steps: 23, steps per second: 2788, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.093 [-0.644, 1.489], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18808/100000: episode: 553, duration: 0.007s, episode steps: 19, steps per second: 2851, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.078 [-0.821, 1.465], mean_best_reward: --\n",
      " 18865/100000: episode: 554, duration: 0.019s, episode steps: 57, steps per second: 2964, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.061 [-1.096, 1.025], mean_best_reward: --\n",
      " 18898/100000: episode: 555, duration: 0.012s, episode steps: 33, steps per second: 2845, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: -0.076 [-1.647, 0.810], mean_best_reward: --\n",
      " 18954/100000: episode: 556, duration: 0.017s, episode steps: 56, steps per second: 3219, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.106 [-1.763, 0.582], mean_best_reward: --\n",
      " 18999/100000: episode: 557, duration: 0.014s, episode steps: 45, steps per second: 3127, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.014 [-1.342, 0.888], mean_best_reward: --\n",
      " 19102/100000: episode: 558, duration: 0.030s, episode steps: 103, steps per second: 3456, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.089 [-1.004, 1.088], mean_best_reward: --\n",
      " 19121/100000: episode: 559, duration: 0.006s, episode steps: 19, steps per second: 3025, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.087 [-1.019, 0.598], mean_best_reward: --\n",
      " 19177/100000: episode: 560, duration: 0.016s, episode steps: 56, steps per second: 3493, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.090 [-1.215, 0.812], mean_best_reward: --\n",
      " 19191/100000: episode: 561, duration: 0.005s, episode steps: 14, steps per second: 2907, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.109 [-0.769, 1.289], mean_best_reward: --\n",
      " 19206/100000: episode: 562, duration: 0.005s, episode steps: 15, steps per second: 2942, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.079 [-1.155, 1.903], mean_best_reward: --\n",
      " 19250/100000: episode: 563, duration: 0.014s, episode steps: 44, steps per second: 3141, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-0.818, 1.210], mean_best_reward: --\n",
      " 19290/100000: episode: 564, duration: 0.015s, episode steps: 40, steps per second: 2713, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.002 [-0.632, 1.091], mean_best_reward: --\n",
      " 19323/100000: episode: 565, duration: 0.010s, episode steps: 33, steps per second: 3291, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.083 [-0.542, 0.984], mean_best_reward: --\n",
      " 19341/100000: episode: 566, duration: 0.006s, episode steps: 18, steps per second: 3138, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.633, 1.059], mean_best_reward: --\n",
      " 19491/100000: episode: 567, duration: 0.045s, episode steps: 150, steps per second: 3302, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.082 [-1.334, 1.355], mean_best_reward: --\n",
      " 19527/100000: episode: 568, duration: 0.013s, episode steps: 36, steps per second: 2810, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.048 [-1.795, 0.853], mean_best_reward: --\n",
      " 19570/100000: episode: 569, duration: 0.014s, episode steps: 43, steps per second: 3124, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.074 [-0.562, 1.496], mean_best_reward: --\n",
      " 19592/100000: episode: 570, duration: 0.008s, episode steps: 22, steps per second: 2697, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.101, 0.742], mean_best_reward: --\n",
      " 19656/100000: episode: 571, duration: 0.020s, episode steps: 64, steps per second: 3211, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.120 [-0.785, 1.682], mean_best_reward: --\n",
      " 19689/100000: episode: 572, duration: 0.010s, episode steps: 33, steps per second: 3353, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.075 [-0.804, 1.210], mean_best_reward: --\n",
      " 19716/100000: episode: 573, duration: 0.009s, episode steps: 27, steps per second: 3152, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.054 [-0.926, 0.603], mean_best_reward: --\n",
      " 19737/100000: episode: 574, duration: 0.007s, episode steps: 21, steps per second: 3037, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.111 [-0.540, 1.155], mean_best_reward: --\n",
      " 19760/100000: episode: 575, duration: 0.007s, episode steps: 23, steps per second: 3112, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.067 [-1.102, 0.632], mean_best_reward: --\n",
      " 19807/100000: episode: 576, duration: 0.014s, episode steps: 47, steps per second: 3302, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.075 [-0.765, 1.548], mean_best_reward: --\n",
      " 19823/100000: episode: 577, duration: 0.006s, episode steps: 16, steps per second: 2902, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.043 [-1.205, 1.704], mean_best_reward: --\n",
      " 19838/100000: episode: 578, duration: 0.005s, episode steps: 15, steps per second: 2842, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.057 [-1.031, 1.596], mean_best_reward: --\n",
      " 19911/100000: episode: 579, duration: 0.027s, episode steps: 73, steps per second: 2731, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.339 [-1.821, 0.712], mean_best_reward: --\n",
      " 19945/100000: episode: 580, duration: 0.011s, episode steps: 34, steps per second: 3046, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.583, 1.045], mean_best_reward: --\n",
      " 19972/100000: episode: 581, duration: 0.009s, episode steps: 27, steps per second: 3156, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.045 [-0.960, 1.318], mean_best_reward: --\n",
      " 19995/100000: episode: 582, duration: 0.007s, episode steps: 23, steps per second: 3223, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.034 [-1.378, 1.007], mean_best_reward: --\n",
      " 20033/100000: episode: 583, duration: 0.012s, episode steps: 38, steps per second: 3086, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.039 [-1.011, 1.314], mean_best_reward: --\n",
      " 20077/100000: episode: 584, duration: 0.015s, episode steps: 44, steps per second: 2945, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-1.112, 1.691], mean_best_reward: --\n",
      " 20119/100000: episode: 585, duration: 0.021s, episode steps: 42, steps per second: 2004, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.131 [-1.078, 0.736], mean_best_reward: --\n",
      " 20137/100000: episode: 586, duration: 0.008s, episode steps: 18, steps per second: 2377, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.948, 1.489], mean_best_reward: --\n",
      " 20211/100000: episode: 587, duration: 0.024s, episode steps: 74, steps per second: 3135, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.166 [-1.446, 0.753], mean_best_reward: --\n",
      " 20228/100000: episode: 588, duration: 0.006s, episode steps: 17, steps per second: 2967, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.074 [-1.006, 1.537], mean_best_reward: --\n",
      " 20260/100000: episode: 589, duration: 0.009s, episode steps: 32, steps per second: 3604, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.033 [-1.416, 2.088], mean_best_reward: --\n",
      " 20350/100000: episode: 590, duration: 0.038s, episode steps: 90, steps per second: 2380, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.205 [-1.282, 1.058], mean_best_reward: --\n",
      " 20480/100000: episode: 591, duration: 0.052s, episode steps: 130, steps per second: 2522, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.345 [-1.546, 1.333], mean_best_reward: --\n",
      " 20499/100000: episode: 592, duration: 0.013s, episode steps: 19, steps per second: 1505, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.094 [-1.162, 0.585], mean_best_reward: --\n",
      " 20516/100000: episode: 593, duration: 0.008s, episode steps: 17, steps per second: 2044, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.092 [-0.745, 1.268], mean_best_reward: --\n",
      " 20583/100000: episode: 594, duration: 0.036s, episode steps: 67, steps per second: 1855, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.256 [-0.661, 1.794], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20605/100000: episode: 595, duration: 0.010s, episode steps: 22, steps per second: 2211, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.108 [-1.341, 0.744], mean_best_reward: --\n",
      " 20634/100000: episode: 596, duration: 0.011s, episode steps: 29, steps per second: 2677, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.105 [-0.538, 0.929], mean_best_reward: --\n",
      " 20676/100000: episode: 597, duration: 0.024s, episode steps: 42, steps per second: 1760, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.130 [-1.320, 0.603], mean_best_reward: --\n",
      " 20759/100000: episode: 598, duration: 0.036s, episode steps: 83, steps per second: 2290, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.070 [-1.852, 1.182], mean_best_reward: --\n",
      " 20773/100000: episode: 599, duration: 0.006s, episode steps: 14, steps per second: 2426, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.109 [-1.274, 0.751], mean_best_reward: --\n",
      " 20796/100000: episode: 600, duration: 0.008s, episode steps: 23, steps per second: 2742, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.128 [-0.583, 1.258], mean_best_reward: --\n",
      " 20821/100000: episode: 601, duration: 0.011s, episode steps: 25, steps per second: 2282, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.035 [-0.812, 1.211], mean_best_reward: 100.000000\n",
      " 20968/100000: episode: 602, duration: 0.047s, episode steps: 147, steps per second: 3141, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.234 [-2.224, 1.142], mean_best_reward: --\n",
      " 20978/100000: episode: 603, duration: 0.004s, episode steps: 10, steps per second: 2632, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-0.964, 1.640], mean_best_reward: --\n",
      " 21074/100000: episode: 604, duration: 0.033s, episode steps: 96, steps per second: 2934, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.078 [-1.342, 1.467], mean_best_reward: --\n",
      " 21097/100000: episode: 605, duration: 0.026s, episode steps: 23, steps per second: 902, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.050 [-1.136, 1.709], mean_best_reward: --\n",
      " 21156/100000: episode: 606, duration: 0.030s, episode steps: 59, steps per second: 1985, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.003 [-0.686, 0.871], mean_best_reward: --\n",
      " 21185/100000: episode: 607, duration: 0.011s, episode steps: 29, steps per second: 2601, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.075 [-0.581, 1.192], mean_best_reward: --\n",
      " 21201/100000: episode: 608, duration: 0.008s, episode steps: 16, steps per second: 2067, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.087 [-1.246, 0.765], mean_best_reward: --\n",
      " 21226/100000: episode: 609, duration: 0.009s, episode steps: 25, steps per second: 2811, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.058 [-1.038, 1.760], mean_best_reward: --\n",
      " 21280/100000: episode: 610, duration: 0.017s, episode steps: 54, steps per second: 3184, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.024 [-1.209, 0.631], mean_best_reward: --\n",
      " 21308/100000: episode: 611, duration: 0.010s, episode steps: 28, steps per second: 2725, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-1.525, 1.117], mean_best_reward: --\n",
      " 21334/100000: episode: 612, duration: 0.010s, episode steps: 26, steps per second: 2497, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.752, 1.169], mean_best_reward: --\n",
      " 21395/100000: episode: 613, duration: 0.020s, episode steps: 61, steps per second: 3016, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.192 [-0.707, 1.436], mean_best_reward: --\n",
      " 21424/100000: episode: 614, duration: 0.011s, episode steps: 29, steps per second: 2546, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.106 [-0.439, 0.861], mean_best_reward: --\n",
      " 21451/100000: episode: 615, duration: 0.009s, episode steps: 27, steps per second: 2859, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.125 [-0.552, 0.964], mean_best_reward: --\n",
      " 21563/100000: episode: 616, duration: 0.034s, episode steps: 112, steps per second: 3321, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.075 [-0.792, 1.146], mean_best_reward: --\n",
      " 21593/100000: episode: 617, duration: 0.009s, episode steps: 30, steps per second: 3194, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.570, 1.070], mean_best_reward: --\n",
      " 21705/100000: episode: 618, duration: 0.036s, episode steps: 112, steps per second: 3094, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.066 [-1.105, 1.685], mean_best_reward: --\n",
      " 21722/100000: episode: 619, duration: 0.009s, episode steps: 17, steps per second: 1863, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.092 [-0.618, 1.290], mean_best_reward: --\n",
      " 21790/100000: episode: 620, duration: 0.023s, episode steps: 68, steps per second: 2951, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.115 [-0.659, 1.173], mean_best_reward: --\n",
      " 21814/100000: episode: 621, duration: 0.008s, episode steps: 24, steps per second: 2843, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.029 [-1.225, 1.920], mean_best_reward: --\n",
      " 21843/100000: episode: 622, duration: 0.010s, episode steps: 29, steps per second: 2900, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.130 [-0.377, 1.005], mean_best_reward: --\n",
      " 21861/100000: episode: 623, duration: 0.006s, episode steps: 18, steps per second: 2991, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.059 [-1.144, 1.660], mean_best_reward: --\n",
      " 21882/100000: episode: 624, duration: 0.007s, episode steps: 21, steps per second: 3076, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.073 [-2.037, 1.203], mean_best_reward: --\n",
      " 21899/100000: episode: 625, duration: 0.006s, episode steps: 17, steps per second: 2995, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.082 [-0.948, 1.455], mean_best_reward: --\n",
      " 21928/100000: episode: 626, duration: 0.009s, episode steps: 29, steps per second: 3074, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.015 [-1.184, 1.492], mean_best_reward: --\n",
      " 21945/100000: episode: 627, duration: 0.006s, episode steps: 17, steps per second: 3025, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.101 [-0.586, 1.154], mean_best_reward: --\n",
      " 22033/100000: episode: 628, duration: 0.026s, episode steps: 88, steps per second: 3338, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.236 [-1.260, 1.678], mean_best_reward: --\n",
      " 22048/100000: episode: 629, duration: 0.006s, episode steps: 15, steps per second: 2674, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.099 [-1.807, 0.959], mean_best_reward: --\n",
      " 22087/100000: episode: 630, duration: 0.012s, episode steps: 39, steps per second: 3221, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.055 [-1.267, 0.764], mean_best_reward: --\n",
      " 22129/100000: episode: 631, duration: 0.013s, episode steps: 42, steps per second: 3141, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.126 [-0.427, 1.745], mean_best_reward: --\n",
      " 22142/100000: episode: 632, duration: 0.005s, episode steps: 13, steps per second: 2590, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.096 [-1.531, 0.969], mean_best_reward: --\n",
      " 22158/100000: episode: 633, duration: 0.006s, episode steps: 16, steps per second: 2536, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.065 [-1.002, 1.570], mean_best_reward: --\n",
      " 22187/100000: episode: 634, duration: 0.010s, episode steps: 29, steps per second: 2859, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.154 [-1.066, 0.589], mean_best_reward: --\n",
      " 22238/100000: episode: 635, duration: 0.016s, episode steps: 51, steps per second: 3282, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.033 [-1.196, 0.623], mean_best_reward: --\n",
      " 22277/100000: episode: 636, duration: 0.013s, episode steps: 39, steps per second: 3003, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.008 [-1.172, 1.415], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22358/100000: episode: 637, duration: 0.030s, episode steps: 81, steps per second: 2673, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.247 [-1.329, 0.943], mean_best_reward: --\n",
      " 22421/100000: episode: 638, duration: 0.020s, episode steps: 63, steps per second: 3102, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.028 [-0.656, 0.912], mean_best_reward: --\n",
      " 22468/100000: episode: 639, duration: 0.015s, episode steps: 47, steps per second: 3035, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.054 [-1.525, 0.761], mean_best_reward: --\n",
      " 22521/100000: episode: 640, duration: 0.017s, episode steps: 53, steps per second: 3151, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.052 [-1.494, 1.370], mean_best_reward: --\n",
      " 22542/100000: episode: 641, duration: 0.008s, episode steps: 21, steps per second: 2667, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.081 [-0.630, 1.200], mean_best_reward: --\n",
      " 22569/100000: episode: 642, duration: 0.009s, episode steps: 27, steps per second: 2960, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.027 [-0.939, 1.320], mean_best_reward: --\n",
      " 22602/100000: episode: 643, duration: 0.011s, episode steps: 33, steps per second: 3021, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.000 [-1.208, 1.551], mean_best_reward: --\n",
      " 22656/100000: episode: 644, duration: 0.016s, episode steps: 54, steps per second: 3345, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.012 [-1.439, 0.755], mean_best_reward: --\n",
      " 22707/100000: episode: 645, duration: 0.016s, episode steps: 51, steps per second: 3169, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.023 [-1.348, 0.997], mean_best_reward: --\n",
      " 22749/100000: episode: 646, duration: 0.013s, episode steps: 42, steps per second: 3201, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.056 [-0.793, 1.456], mean_best_reward: --\n",
      " 22798/100000: episode: 647, duration: 0.015s, episode steps: 49, steps per second: 3216, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.154 [-0.658, 0.953], mean_best_reward: --\n",
      " 22821/100000: episode: 648, duration: 0.007s, episode steps: 23, steps per second: 3080, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.031 [-0.819, 1.212], mean_best_reward: --\n",
      " 22852/100000: episode: 649, duration: 0.010s, episode steps: 31, steps per second: 3211, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.087 [-1.160, 0.584], mean_best_reward: --\n",
      " 22884/100000: episode: 650, duration: 0.010s, episode steps: 32, steps per second: 3210, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.544, 1.243], mean_best_reward: --\n",
      " 22978/100000: episode: 651, duration: 0.029s, episode steps: 94, steps per second: 3278, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.360 [-1.149, 2.187], mean_best_reward: 94.000000\n",
      " 23014/100000: episode: 652, duration: 0.014s, episode steps: 36, steps per second: 2600, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.592, 1.022], mean_best_reward: --\n",
      " 23061/100000: episode: 653, duration: 0.018s, episode steps: 47, steps per second: 2583, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.135 [-1.009, 0.489], mean_best_reward: --\n",
      " 23077/100000: episode: 654, duration: 0.006s, episode steps: 16, steps per second: 2645, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.767, 1.290], mean_best_reward: --\n",
      " 23101/100000: episode: 655, duration: 0.008s, episode steps: 24, steps per second: 3137, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.076 [-1.462, 0.993], mean_best_reward: --\n",
      " 23133/100000: episode: 656, duration: 0.013s, episode steps: 32, steps per second: 2465, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-1.602, 0.831], mean_best_reward: --\n",
      " 23162/100000: episode: 657, duration: 0.015s, episode steps: 29, steps per second: 1969, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.102 [-0.614, 1.059], mean_best_reward: --\n",
      " 23187/100000: episode: 658, duration: 0.010s, episode steps: 25, steps per second: 2492, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.012 [-2.116, 1.398], mean_best_reward: --\n",
      " 23228/100000: episode: 659, duration: 0.019s, episode steps: 41, steps per second: 2114, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.122 [-0.401, 0.915], mean_best_reward: --\n",
      " 23253/100000: episode: 660, duration: 0.014s, episode steps: 25, steps per second: 1776, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.034 [-1.179, 1.578], mean_best_reward: --\n",
      " 23281/100000: episode: 661, duration: 0.014s, episode steps: 28, steps per second: 2061, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.083 [-1.215, 0.803], mean_best_reward: --\n",
      " 23319/100000: episode: 662, duration: 0.015s, episode steps: 38, steps per second: 2605, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.132 [-0.775, 1.358], mean_best_reward: --\n",
      " 23350/100000: episode: 663, duration: 0.010s, episode steps: 31, steps per second: 3137, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.066 [-1.956, 1.028], mean_best_reward: --\n",
      " 23385/100000: episode: 664, duration: 0.012s, episode steps: 35, steps per second: 3022, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.149 [-0.721, 1.112], mean_best_reward: --\n",
      " 23403/100000: episode: 665, duration: 0.006s, episode steps: 18, steps per second: 2962, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.117, 0.564], mean_best_reward: --\n",
      " 23427/100000: episode: 666, duration: 0.008s, episode steps: 24, steps per second: 3112, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.079 [-0.766, 1.524], mean_best_reward: --\n",
      " 23478/100000: episode: 667, duration: 0.015s, episode steps: 51, steps per second: 3321, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.184 [-1.399, 0.590], mean_best_reward: --\n",
      " 23508/100000: episode: 668, duration: 0.014s, episode steps: 30, steps per second: 2088, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.049 [-0.620, 1.197], mean_best_reward: --\n",
      " 23542/100000: episode: 669, duration: 0.014s, episode steps: 34, steps per second: 2442, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.576, 1.275], mean_best_reward: --\n",
      " 23633/100000: episode: 670, duration: 0.028s, episode steps: 91, steps per second: 3260, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.185 [-1.501, 1.064], mean_best_reward: --\n",
      " 23650/100000: episode: 671, duration: 0.006s, episode steps: 17, steps per second: 2979, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.097 [-0.762, 1.524], mean_best_reward: --\n",
      " 23691/100000: episode: 672, duration: 0.012s, episode steps: 41, steps per second: 3349, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.108 [-1.091, 0.690], mean_best_reward: --\n",
      " 23730/100000: episode: 673, duration: 0.011s, episode steps: 39, steps per second: 3559, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.029 [-1.403, 1.682], mean_best_reward: --\n",
      " 23746/100000: episode: 674, duration: 0.005s, episode steps: 16, steps per second: 3201, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.086 [-1.517, 0.806], mean_best_reward: --\n",
      " 23758/100000: episode: 675, duration: 0.004s, episode steps: 12, steps per second: 3017, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.123 [-1.724, 0.987], mean_best_reward: --\n",
      " 23777/100000: episode: 676, duration: 0.006s, episode steps: 19, steps per second: 3054, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.076 [-1.002, 1.693], mean_best_reward: --\n",
      " 23828/100000: episode: 677, duration: 0.015s, episode steps: 51, steps per second: 3423, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.103 [-1.387, 0.944], mean_best_reward: --\n",
      " 23855/100000: episode: 678, duration: 0.008s, episode steps: 27, steps per second: 3374, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.096 [-0.649, 1.718], mean_best_reward: --\n",
      " 23878/100000: episode: 679, duration: 0.007s, episode steps: 23, steps per second: 3322, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.097 [-0.813, 1.332], mean_best_reward: --\n",
      " 23898/100000: episode: 680, duration: 0.007s, episode steps: 20, steps per second: 3038, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.577, 1.025], mean_best_reward: --\n",
      " 23941/100000: episode: 681, duration: 0.013s, episode steps: 43, steps per second: 3283, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.098 [-1.361, 1.016], mean_best_reward: --\n",
      " 23981/100000: episode: 682, duration: 0.012s, episode steps: 40, steps per second: 3283, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.186 [-0.597, 1.127], mean_best_reward: --\n",
      " 24041/100000: episode: 683, duration: 0.019s, episode steps: 60, steps per second: 3092, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.157 [-0.898, 1.346], mean_best_reward: --\n",
      " 24056/100000: episode: 684, duration: 0.005s, episode steps: 15, steps per second: 3194, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.061 [-1.247, 0.812], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24112/100000: episode: 685, duration: 0.018s, episode steps: 56, steps per second: 3160, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: -0.017 [-1.832, 1.517], mean_best_reward: --\n",
      " 24137/100000: episode: 686, duration: 0.014s, episode steps: 25, steps per second: 1776, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.081 [-1.362, 0.747], mean_best_reward: --\n",
      " 24169/100000: episode: 687, duration: 0.009s, episode steps: 32, steps per second: 3375, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-1.236, 0.806], mean_best_reward: --\n",
      " 24216/100000: episode: 688, duration: 0.015s, episode steps: 47, steps per second: 3147, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.045 [-0.746, 1.377], mean_best_reward: --\n",
      " 24252/100000: episode: 689, duration: 0.011s, episode steps: 36, steps per second: 3202, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.052 [-1.180, 2.230], mean_best_reward: --\n",
      " 24279/100000: episode: 690, duration: 0.009s, episode steps: 27, steps per second: 3132, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.153 [-0.743, 1.193], mean_best_reward: --\n",
      " 24341/100000: episode: 691, duration: 0.019s, episode steps: 62, steps per second: 3344, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-1.155, 1.778], mean_best_reward: --\n",
      " 24355/100000: episode: 692, duration: 0.005s, episode steps: 14, steps per second: 2804, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.080 [-0.967, 1.624], mean_best_reward: --\n",
      " 24381/100000: episode: 693, duration: 0.008s, episode steps: 26, steps per second: 3062, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.591, 0.915], mean_best_reward: --\n",
      " 24414/100000: episode: 694, duration: 0.010s, episode steps: 33, steps per second: 3249, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.055 [-1.023, 1.403], mean_best_reward: --\n",
      " 24428/100000: episode: 695, duration: 0.005s, episode steps: 14, steps per second: 2889, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.134 [-0.545, 1.098], mean_best_reward: --\n",
      " 24455/100000: episode: 696, duration: 0.008s, episode steps: 27, steps per second: 3183, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.009 [-0.818, 1.242], mean_best_reward: --\n",
      " 24528/100000: episode: 697, duration: 0.022s, episode steps: 73, steps per second: 3270, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.122 [-0.969, 1.117], mean_best_reward: --\n",
      " 24556/100000: episode: 698, duration: 0.009s, episode steps: 28, steps per second: 3125, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.082 [-0.761, 1.785], mean_best_reward: --\n",
      " 24593/100000: episode: 699, duration: 0.012s, episode steps: 37, steps per second: 3132, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.022 [-1.304, 1.072], mean_best_reward: --\n",
      " 24622/100000: episode: 700, duration: 0.010s, episode steps: 29, steps per second: 2886, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.081 [-0.974, 1.845], mean_best_reward: --\n",
      " 24641/100000: episode: 701, duration: 0.007s, episode steps: 19, steps per second: 2765, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.089 [-0.600, 1.196], mean_best_reward: 102.500000\n",
      " 24685/100000: episode: 702, duration: 0.015s, episode steps: 44, steps per second: 2958, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.149 [-0.383, 1.116], mean_best_reward: --\n",
      " 24705/100000: episode: 703, duration: 0.011s, episode steps: 20, steps per second: 1832, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-0.631, 0.980], mean_best_reward: --\n",
      " 24729/100000: episode: 704, duration: 0.012s, episode steps: 24, steps per second: 2013, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.073 [-1.413, 0.616], mean_best_reward: --\n",
      " 24762/100000: episode: 705, duration: 0.015s, episode steps: 33, steps per second: 2217, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.083 [-0.578, 1.326], mean_best_reward: --\n",
      " 24782/100000: episode: 706, duration: 0.010s, episode steps: 20, steps per second: 1990, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.082 [-0.590, 1.194], mean_best_reward: --\n",
      " 24804/100000: episode: 707, duration: 0.009s, episode steps: 22, steps per second: 2568, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.428, 0.931], mean_best_reward: --\n",
      " 24829/100000: episode: 708, duration: 0.009s, episode steps: 25, steps per second: 2692, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.076 [-1.032, 1.986], mean_best_reward: --\n",
      " 24930/100000: episode: 709, duration: 0.031s, episode steps: 101, steps per second: 3210, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.049 [-1.208, 1.103], mean_best_reward: --\n",
      " 25023/100000: episode: 710, duration: 0.028s, episode steps: 93, steps per second: 3333, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.177 [-1.510, 0.730], mean_best_reward: --\n",
      " 25079/100000: episode: 711, duration: 0.022s, episode steps: 56, steps per second: 2546, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.088 [-1.315, 1.381], mean_best_reward: --\n",
      " 25109/100000: episode: 712, duration: 0.011s, episode steps: 30, steps per second: 2678, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.385, 0.966], mean_best_reward: --\n",
      " 25136/100000: episode: 713, duration: 0.010s, episode steps: 27, steps per second: 2617, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.090 [-0.573, 1.209], mean_best_reward: --\n",
      " 25161/100000: episode: 714, duration: 0.009s, episode steps: 25, steps per second: 2940, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.052 [-0.984, 0.621], mean_best_reward: --\n",
      " 25185/100000: episode: 715, duration: 0.008s, episode steps: 24, steps per second: 2894, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.415, 0.944], mean_best_reward: --\n",
      " 25232/100000: episode: 716, duration: 0.025s, episode steps: 47, steps per second: 1887, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.128 [-0.442, 1.659], mean_best_reward: --\n",
      " 25263/100000: episode: 717, duration: 0.019s, episode steps: 31, steps per second: 1666, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.101 [-0.382, 0.939], mean_best_reward: --\n",
      " 25287/100000: episode: 718, duration: 0.017s, episode steps: 24, steps per second: 1452, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.112 [-1.180, 0.760], mean_best_reward: --\n",
      " 25316/100000: episode: 719, duration: 0.013s, episode steps: 29, steps per second: 2227, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.057 [-0.795, 1.530], mean_best_reward: --\n",
      " 25367/100000: episode: 720, duration: 0.026s, episode steps: 51, steps per second: 1936, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.093 [-0.789, 1.880], mean_best_reward: --\n",
      " 25460/100000: episode: 721, duration: 0.040s, episode steps: 93, steps per second: 2342, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.040 [-1.302, 1.231], mean_best_reward: --\n",
      " 25534/100000: episode: 722, duration: 0.040s, episode steps: 74, steps per second: 1853, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.166 [-0.940, 1.359], mean_best_reward: --\n",
      " 25564/100000: episode: 723, duration: 0.014s, episode steps: 30, steps per second: 2114, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.021 [-0.984, 1.546], mean_best_reward: --\n",
      " 25597/100000: episode: 724, duration: 0.017s, episode steps: 33, steps per second: 1962, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.039 [-0.647, 1.380], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25681/100000: episode: 725, duration: 0.039s, episode steps: 84, steps per second: 2151, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-1.138, 1.481], mean_best_reward: --\n",
      " 25722/100000: episode: 726, duration: 0.023s, episode steps: 41, steps per second: 1768, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.070 [-0.546, 0.983], mean_best_reward: --\n",
      " 25799/100000: episode: 727, duration: 0.030s, episode steps: 77, steps per second: 2593, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.063 [-1.398, 0.834], mean_best_reward: --\n",
      " 25926/100000: episode: 728, duration: 0.060s, episode steps: 127, steps per second: 2128, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.030 [-0.948, 1.286], mean_best_reward: --\n",
      " 25958/100000: episode: 729, duration: 0.015s, episode steps: 32, steps per second: 2190, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.119 [-0.558, 1.128], mean_best_reward: --\n",
      " 25978/100000: episode: 730, duration: 0.015s, episode steps: 20, steps per second: 1302, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.082 [-1.182, 0.733], mean_best_reward: --\n",
      " 26003/100000: episode: 731, duration: 0.013s, episode steps: 25, steps per second: 1989, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.081 [-0.751, 1.333], mean_best_reward: --\n",
      " 26037/100000: episode: 732, duration: 0.015s, episode steps: 34, steps per second: 2226, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.581, 1.107], mean_best_reward: --\n",
      " 26080/100000: episode: 733, duration: 0.019s, episode steps: 43, steps per second: 2310, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.129 [-1.346, 0.599], mean_best_reward: --\n",
      " 26098/100000: episode: 734, duration: 0.008s, episode steps: 18, steps per second: 2331, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.086 [-1.573, 0.946], mean_best_reward: --\n",
      " 26115/100000: episode: 735, duration: 0.007s, episode steps: 17, steps per second: 2442, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.094 [-0.601, 1.034], mean_best_reward: --\n",
      " 26157/100000: episode: 736, duration: 0.022s, episode steps: 42, steps per second: 1936, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.101 [-0.611, 1.015], mean_best_reward: --\n",
      " 26211/100000: episode: 737, duration: 0.021s, episode steps: 54, steps per second: 2585, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.040 [-1.271, 0.613], mean_best_reward: --\n",
      " 26278/100000: episode: 738, duration: 0.023s, episode steps: 67, steps per second: 2952, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.003 [-1.027, 1.898], mean_best_reward: --\n",
      " 26302/100000: episode: 739, duration: 0.007s, episode steps: 24, steps per second: 3202, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.079 [-0.415, 1.233], mean_best_reward: --\n",
      " 26385/100000: episode: 740, duration: 0.026s, episode steps: 83, steps per second: 3213, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.245 [-1.354, 0.782], mean_best_reward: --\n",
      " 26425/100000: episode: 741, duration: 0.013s, episode steps: 40, steps per second: 3146, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.080 [-0.772, 1.129], mean_best_reward: --\n",
      " 26465/100000: episode: 742, duration: 0.012s, episode steps: 40, steps per second: 3233, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.161 [-0.842, 1.592], mean_best_reward: --\n",
      " 26506/100000: episode: 743, duration: 0.013s, episode steps: 41, steps per second: 3159, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.093 [-0.547, 0.892], mean_best_reward: --\n",
      " 26576/100000: episode: 744, duration: 0.021s, episode steps: 70, steps per second: 3336, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.011 [-1.515, 1.164], mean_best_reward: --\n",
      " 26593/100000: episode: 745, duration: 0.006s, episode steps: 17, steps per second: 2892, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.098 [-0.752, 1.374], mean_best_reward: --\n",
      " 26655/100000: episode: 746, duration: 0.019s, episode steps: 62, steps per second: 3334, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.561, 0.966], mean_best_reward: --\n",
      " 26701/100000: episode: 747, duration: 0.014s, episode steps: 46, steps per second: 3261, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.352, 1.160], mean_best_reward: --\n",
      " 26724/100000: episode: 748, duration: 0.008s, episode steps: 23, steps per second: 2929, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.090 [-0.775, 1.667], mean_best_reward: --\n",
      " 26745/100000: episode: 749, duration: 0.009s, episode steps: 21, steps per second: 2299, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.097 [-1.014, 0.612], mean_best_reward: --\n",
      " 26779/100000: episode: 750, duration: 0.015s, episode steps: 34, steps per second: 2288, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.551, 1.130], mean_best_reward: --\n",
      " 26795/100000: episode: 751, duration: 0.006s, episode steps: 16, steps per second: 2630, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-0.980, 0.617], mean_best_reward: 90.500000\n",
      " 26813/100000: episode: 752, duration: 0.006s, episode steps: 18, steps per second: 2793, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.042 [-1.189, 1.650], mean_best_reward: --\n",
      " 26862/100000: episode: 753, duration: 0.015s, episode steps: 49, steps per second: 3267, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.098 [-1.224, 0.642], mean_best_reward: --\n",
      " 26874/100000: episode: 754, duration: 0.004s, episode steps: 12, steps per second: 2806, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-0.820, 1.305], mean_best_reward: --\n",
      " 26898/100000: episode: 755, duration: 0.007s, episode steps: 24, steps per second: 3458, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.046 [-2.013, 1.229], mean_best_reward: --\n",
      " 26910/100000: episode: 756, duration: 0.004s, episode steps: 12, steps per second: 2754, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.806, 1.148], mean_best_reward: --\n",
      " 26975/100000: episode: 757, duration: 0.019s, episode steps: 65, steps per second: 3395, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.090 [-1.115, 0.578], mean_best_reward: --\n",
      " 27017/100000: episode: 758, duration: 0.012s, episode steps: 42, steps per second: 3364, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.061 [-0.917, 1.537], mean_best_reward: --\n",
      " 27127/100000: episode: 759, duration: 0.032s, episode steps: 110, steps per second: 3476, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.490 [-2.655, 1.542], mean_best_reward: --\n",
      " 27152/100000: episode: 760, duration: 0.008s, episode steps: 25, steps per second: 3313, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.080 [-0.795, 1.157], mean_best_reward: --\n",
      " 27185/100000: episode: 761, duration: 0.010s, episode steps: 33, steps per second: 3274, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.052 [-0.599, 1.189], mean_best_reward: --\n",
      " 27215/100000: episode: 762, duration: 0.009s, episode steps: 30, steps per second: 3189, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.111 [-0.886, 0.568], mean_best_reward: --\n",
      " 27230/100000: episode: 763, duration: 0.005s, episode steps: 15, steps per second: 2943, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.072 [-0.824, 1.246], mean_best_reward: --\n",
      " 27257/100000: episode: 764, duration: 0.008s, episode steps: 27, steps per second: 3226, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.138 [-0.554, 1.373], mean_best_reward: --\n",
      " 27284/100000: episode: 765, duration: 0.008s, episode steps: 27, steps per second: 3238, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.062 [-0.613, 1.169], mean_best_reward: --\n",
      " 27329/100000: episode: 766, duration: 0.012s, episode steps: 45, steps per second: 3613, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.134 [-0.346, 0.783], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27384/100000: episode: 767, duration: 0.020s, episode steps: 55, steps per second: 2819, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.172 [-1.385, 0.713], mean_best_reward: --\n",
      " 27422/100000: episode: 768, duration: 0.015s, episode steps: 38, steps per second: 2598, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.089 [-0.815, 0.458], mean_best_reward: --\n",
      " 27464/100000: episode: 769, duration: 0.019s, episode steps: 42, steps per second: 2189, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.039 [-0.958, 1.352], mean_best_reward: --\n",
      " 27547/100000: episode: 770, duration: 0.041s, episode steps: 83, steps per second: 2037, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.283 [-1.602, 0.802], mean_best_reward: --\n",
      " 27589/100000: episode: 771, duration: 0.020s, episode steps: 42, steps per second: 2099, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.098 [-1.890, 1.769], mean_best_reward: --\n",
      " 27615/100000: episode: 772, duration: 0.015s, episode steps: 26, steps per second: 1777, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.064 [-0.589, 1.323], mean_best_reward: --\n",
      " 27643/100000: episode: 773, duration: 0.013s, episode steps: 28, steps per second: 2203, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.085 [-1.212, 0.589], mean_best_reward: --\n",
      " 27702/100000: episode: 774, duration: 0.022s, episode steps: 59, steps per second: 2693, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.132 [-0.681, 1.417], mean_best_reward: --\n",
      " 27753/100000: episode: 775, duration: 0.017s, episode steps: 51, steps per second: 2979, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.093 [-1.091, 0.735], mean_best_reward: --\n",
      " 27789/100000: episode: 776, duration: 0.012s, episode steps: 36, steps per second: 3018, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.574, 1.132], mean_best_reward: --\n",
      " 27800/100000: episode: 777, duration: 0.004s, episode steps: 11, steps per second: 2491, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-1.186, 1.838], mean_best_reward: --\n",
      " 27825/100000: episode: 778, duration: 0.008s, episode steps: 25, steps per second: 3021, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.036 [-0.643, 1.063], mean_best_reward: --\n",
      " 27846/100000: episode: 779, duration: 0.007s, episode steps: 21, steps per second: 2903, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.099 [-0.761, 1.361], mean_best_reward: --\n",
      " 27901/100000: episode: 780, duration: 0.025s, episode steps: 55, steps per second: 2194, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.017 [-1.034, 1.808], mean_best_reward: --\n",
      " 27914/100000: episode: 781, duration: 0.006s, episode steps: 13, steps per second: 2299, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.116 [-0.752, 1.458], mean_best_reward: --\n",
      " 27941/100000: episode: 782, duration: 0.010s, episode steps: 27, steps per second: 2739, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.086 [-0.606, 1.355], mean_best_reward: --\n",
      " 28003/100000: episode: 783, duration: 0.019s, episode steps: 62, steps per second: 3258, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.182 [-1.177, 0.957], mean_best_reward: --\n",
      " 28019/100000: episode: 784, duration: 0.006s, episode steps: 16, steps per second: 2782, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.100 [-1.696, 0.954], mean_best_reward: --\n",
      " 28044/100000: episode: 785, duration: 0.008s, episode steps: 25, steps per second: 3003, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.066 [-0.622, 0.953], mean_best_reward: --\n",
      " 28102/100000: episode: 786, duration: 0.017s, episode steps: 58, steps per second: 3440, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.039 [-0.976, 1.218], mean_best_reward: --\n",
      " 28145/100000: episode: 787, duration: 0.013s, episode steps: 43, steps per second: 3351, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.195 [-1.162, 0.938], mean_best_reward: --\n",
      " 28203/100000: episode: 788, duration: 0.017s, episode steps: 58, steps per second: 3398, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.112 [-0.611, 1.122], mean_best_reward: --\n",
      " 28260/100000: episode: 789, duration: 0.017s, episode steps: 57, steps per second: 3395, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.080 [-0.551, 1.415], mean_best_reward: --\n",
      " 28282/100000: episode: 790, duration: 0.007s, episode steps: 22, steps per second: 3108, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.807, 1.164], mean_best_reward: --\n",
      " 28316/100000: episode: 791, duration: 0.010s, episode steps: 34, steps per second: 3323, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.601, 1.331], mean_best_reward: --\n",
      " 28361/100000: episode: 792, duration: 0.013s, episode steps: 45, steps per second: 3369, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.021 [-0.992, 1.238], mean_best_reward: --\n",
      " 28421/100000: episode: 793, duration: 0.018s, episode steps: 60, steps per second: 3350, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.003 [-0.863, 1.454], mean_best_reward: --\n",
      " 28620/100000: episode: 794, duration: 0.064s, episode steps: 199, steps per second: 3088, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.047 [-1.466, 1.269], mean_best_reward: --\n",
      " 28684/100000: episode: 795, duration: 0.018s, episode steps: 64, steps per second: 3470, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.825, 1.168], mean_best_reward: --\n",
      " 28718/100000: episode: 796, duration: 0.011s, episode steps: 34, steps per second: 3133, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.057 [-1.001, 1.916], mean_best_reward: --\n",
      " 28733/100000: episode: 797, duration: 0.005s, episode steps: 15, steps per second: 2768, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.091 [-0.808, 1.236], mean_best_reward: --\n",
      " 28763/100000: episode: 798, duration: 0.010s, episode steps: 30, steps per second: 2970, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.041 [-0.606, 1.114], mean_best_reward: --\n",
      " 28784/100000: episode: 799, duration: 0.011s, episode steps: 21, steps per second: 1895, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.093 [-1.003, 1.919], mean_best_reward: --\n",
      " 28815/100000: episode: 800, duration: 0.015s, episode steps: 31, steps per second: 2131, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.079 [-0.732, 1.288], mean_best_reward: --\n",
      " 28831/100000: episode: 801, duration: 0.007s, episode steps: 16, steps per second: 2176, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.075 [-0.981, 1.534], mean_best_reward: 158.500000\n",
      " 28857/100000: episode: 802, duration: 0.018s, episode steps: 26, steps per second: 1453, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.129 [-1.092, 0.560], mean_best_reward: --\n",
      " 28881/100000: episode: 803, duration: 0.015s, episode steps: 24, steps per second: 1601, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.139 [-0.567, 1.128], mean_best_reward: --\n",
      " 28982/100000: episode: 804, duration: 0.055s, episode steps: 101, steps per second: 1828, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.017 [-1.370, 1.422], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29091/100000: episode: 805, duration: 0.038s, episode steps: 109, steps per second: 2889, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.217 [-1.536, 1.083], mean_best_reward: --\n",
      " 29278/100000: episode: 806, duration: 0.068s, episode steps: 187, steps per second: 2734, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.024 [-1.141, 1.668], mean_best_reward: --\n",
      " 29298/100000: episode: 807, duration: 0.007s, episode steps: 20, steps per second: 2930, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.071 [-1.166, 2.038], mean_best_reward: --\n",
      " 29328/100000: episode: 808, duration: 0.011s, episode steps: 30, steps per second: 2727, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.096 [-1.025, 0.603], mean_best_reward: --\n",
      " 29361/100000: episode: 809, duration: 0.011s, episode steps: 33, steps per second: 2935, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.070 [-0.569, 1.055], mean_best_reward: --\n",
      " 29399/100000: episode: 810, duration: 0.013s, episode steps: 38, steps per second: 2865, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.097 [-0.545, 0.842], mean_best_reward: --\n",
      " 29426/100000: episode: 811, duration: 0.016s, episode steps: 27, steps per second: 1658, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.067 [-1.088, 0.580], mean_best_reward: --\n",
      " 29464/100000: episode: 812, duration: 0.019s, episode steps: 38, steps per second: 2018, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.105 [-0.962, 1.620], mean_best_reward: --\n",
      " 29480/100000: episode: 813, duration: 0.010s, episode steps: 16, steps per second: 1586, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.112 [-0.763, 1.392], mean_best_reward: --\n",
      " 29489/100000: episode: 814, duration: 0.005s, episode steps: 9, steps per second: 1687, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.141 [-0.933, 1.594], mean_best_reward: --\n",
      " 29551/100000: episode: 815, duration: 0.028s, episode steps: 62, steps per second: 2218, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.147 [-1.380, 0.924], mean_best_reward: --\n",
      " 29603/100000: episode: 816, duration: 0.026s, episode steps: 52, steps per second: 2016, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.080 [-1.537, 1.161], mean_best_reward: --\n",
      " 29648/100000: episode: 817, duration: 0.023s, episode steps: 45, steps per second: 1951, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.061 [-1.319, 0.603], mean_best_reward: --\n",
      " 29662/100000: episode: 818, duration: 0.008s, episode steps: 14, steps per second: 1827, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-1.008, 1.590], mean_best_reward: --\n",
      " 29692/100000: episode: 819, duration: 0.011s, episode steps: 30, steps per second: 2718, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: -0.034 [-1.374, 1.590], mean_best_reward: --\n",
      " 29738/100000: episode: 820, duration: 0.015s, episode steps: 46, steps per second: 3126, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.145 [-0.769, 1.136], mean_best_reward: --\n",
      " 29779/100000: episode: 821, duration: 0.016s, episode steps: 41, steps per second: 2527, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.002 [-1.299, 0.649], mean_best_reward: --\n",
      " 29796/100000: episode: 822, duration: 0.010s, episode steps: 17, steps per second: 1736, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.097 [-0.599, 1.148], mean_best_reward: --\n",
      " 29865/100000: episode: 823, duration: 0.043s, episode steps: 69, steps per second: 1620, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.251 [-0.916, 1.333], mean_best_reward: --\n",
      " 29925/100000: episode: 824, duration: 0.029s, episode steps: 60, steps per second: 2044, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-1.016, 0.624], mean_best_reward: --\n",
      " 29945/100000: episode: 825, duration: 0.010s, episode steps: 20, steps per second: 1916, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.086 [-2.210, 1.188], mean_best_reward: --\n",
      " 29977/100000: episode: 826, duration: 0.016s, episode steps: 32, steps per second: 2043, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.129 [-0.551, 1.023], mean_best_reward: --\n",
      " 29990/100000: episode: 827, duration: 0.006s, episode steps: 13, steps per second: 2205, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.109 [-0.813, 1.379], mean_best_reward: --\n",
      " 30058/100000: episode: 828, duration: 0.027s, episode steps: 68, steps per second: 2528, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.062 [-1.741, 1.556], mean_best_reward: --\n",
      " 30099/100000: episode: 829, duration: 0.020s, episode steps: 41, steps per second: 2052, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.010 [-0.948, 1.357], mean_best_reward: --\n",
      " 30157/100000: episode: 830, duration: 0.019s, episode steps: 58, steps per second: 3014, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.099 [-0.800, 1.279], mean_best_reward: --\n",
      " 30174/100000: episode: 831, duration: 0.008s, episode steps: 17, steps per second: 2007, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.071 [-0.828, 1.355], mean_best_reward: --\n",
      " 30197/100000: episode: 832, duration: 0.014s, episode steps: 23, steps per second: 1604, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.016 [-1.449, 1.003], mean_best_reward: --\n",
      " 30227/100000: episode: 833, duration: 0.013s, episode steps: 30, steps per second: 2386, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.156 [-0.619, 0.943], mean_best_reward: --\n",
      " 30309/100000: episode: 834, duration: 0.034s, episode steps: 82, steps per second: 2433, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.141 [-1.391, 1.106], mean_best_reward: --\n",
      " 30403/100000: episode: 835, duration: 0.040s, episode steps: 94, steps per second: 2358, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.284 [-1.474, 1.017], mean_best_reward: --\n",
      " 30463/100000: episode: 836, duration: 0.031s, episode steps: 60, steps per second: 1935, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.016 [-1.055, 1.316], mean_best_reward: --\n",
      " 30478/100000: episode: 837, duration: 0.006s, episode steps: 15, steps per second: 2528, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.085 [-1.423, 0.823], mean_best_reward: --\n",
      " 30498/100000: episode: 838, duration: 0.017s, episode steps: 20, steps per second: 1205, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.435, 0.927], mean_best_reward: --\n",
      " 30523/100000: episode: 839, duration: 0.021s, episode steps: 25, steps per second: 1181, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.078 [-0.946, 0.577], mean_best_reward: --\n",
      " 30589/100000: episode: 840, duration: 0.028s, episode steps: 66, steps per second: 2374, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: 0.010 [-1.153, 1.836], mean_best_reward: --\n",
      " 30603/100000: episode: 841, duration: 0.006s, episode steps: 14, steps per second: 2180, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.116 [-1.801, 0.998], mean_best_reward: --\n",
      " 30619/100000: episode: 842, duration: 0.007s, episode steps: 16, steps per second: 2378, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.090 [-0.806, 1.506], mean_best_reward: --\n",
      " 30657/100000: episode: 843, duration: 0.017s, episode steps: 38, steps per second: 2210, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.047 [-0.810, 1.244], mean_best_reward: --\n",
      " 30689/100000: episode: 844, duration: 0.015s, episode steps: 32, steps per second: 2113, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.077 [-1.026, 0.578], mean_best_reward: --\n",
      " 30730/100000: episode: 845, duration: 0.017s, episode steps: 41, steps per second: 2380, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.023 [-1.351, 1.135], mean_best_reward: --\n",
      " 30759/100000: episode: 846, duration: 0.012s, episode steps: 29, steps per second: 2513, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.130 [-0.487, 1.306], mean_best_reward: --\n",
      " 30778/100000: episode: 847, duration: 0.007s, episode steps: 19, steps per second: 2585, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.067 [-1.398, 0.796], mean_best_reward: --\n",
      " 30866/100000: episode: 848, duration: 0.031s, episode steps: 88, steps per second: 2810, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.148 [-1.629, 1.228], mean_best_reward: --\n",
      " 30899/100000: episode: 849, duration: 0.011s, episode steps: 33, steps per second: 2878, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.048 [-1.123, 1.425], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30998/100000: episode: 850, duration: 0.035s, episode steps: 99, steps per second: 2847, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.141 [-1.152, 1.537], mean_best_reward: --\n",
      " 31029/100000: episode: 851, duration: 0.013s, episode steps: 31, steps per second: 2336, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.039 [-0.785, 1.432], mean_best_reward: 112.500000\n",
      " 31044/100000: episode: 852, duration: 0.010s, episode steps: 15, steps per second: 1544, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.087 [-1.212, 0.813], mean_best_reward: --\n",
      " 31067/100000: episode: 853, duration: 0.009s, episode steps: 23, steps per second: 2648, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.078 [-1.397, 0.614], mean_best_reward: --\n",
      " 31079/100000: episode: 854, duration: 0.005s, episode steps: 12, steps per second: 2329, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.099 [-1.410, 0.824], mean_best_reward: --\n",
      " 31139/100000: episode: 855, duration: 0.019s, episode steps: 60, steps per second: 3193, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.177 [-0.712, 1.388], mean_best_reward: --\n",
      " 31162/100000: episode: 856, duration: 0.007s, episode steps: 23, steps per second: 3101, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.077 [-1.606, 0.951], mean_best_reward: --\n",
      " 31187/100000: episode: 857, duration: 0.008s, episode steps: 25, steps per second: 3095, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.072 [-0.806, 1.170], mean_best_reward: --\n",
      " 31217/100000: episode: 858, duration: 0.009s, episode steps: 30, steps per second: 3216, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.066 [-1.515, 0.960], mean_best_reward: --\n",
      " 31240/100000: episode: 859, duration: 0.007s, episode steps: 23, steps per second: 3088, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.026 [-0.813, 1.251], mean_best_reward: --\n",
      " 31269/100000: episode: 860, duration: 0.009s, episode steps: 29, steps per second: 3243, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.022 [-0.776, 1.355], mean_best_reward: --\n",
      " 31298/100000: episode: 861, duration: 0.009s, episode steps: 29, steps per second: 3221, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.073 [-1.169, 0.762], mean_best_reward: --\n",
      " 31358/100000: episode: 862, duration: 0.017s, episode steps: 60, steps per second: 3441, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.165 [-1.884, 1.442], mean_best_reward: --\n",
      " 31406/100000: episode: 863, duration: 0.014s, episode steps: 48, steps per second: 3350, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.131 [-0.944, 0.542], mean_best_reward: --\n",
      " 31485/100000: episode: 864, duration: 0.023s, episode steps: 79, steps per second: 3476, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.037 [-0.600, 0.898], mean_best_reward: --\n",
      " 31595/100000: episode: 865, duration: 0.032s, episode steps: 110, steps per second: 3451, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.229 [-1.522, 1.261], mean_best_reward: --\n",
      " 31694/100000: episode: 866, duration: 0.039s, episode steps: 99, steps per second: 2507, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.318 [-0.987, 1.917], mean_best_reward: --\n",
      " 31765/100000: episode: 867, duration: 0.023s, episode steps: 71, steps per second: 3050, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.107 [-1.434, 0.739], mean_best_reward: --\n",
      " 31877/100000: episode: 868, duration: 0.034s, episode steps: 112, steps per second: 3280, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-1.237, 1.332], mean_best_reward: --\n",
      " 31930/100000: episode: 869, duration: 0.017s, episode steps: 53, steps per second: 3104, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.057 [-0.748, 1.043], mean_best_reward: --\n",
      " 31954/100000: episode: 870, duration: 0.008s, episode steps: 24, steps per second: 2997, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.084 [-0.789, 1.635], mean_best_reward: --\n",
      " 31988/100000: episode: 871, duration: 0.011s, episode steps: 34, steps per second: 3203, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.072 [-0.582, 1.164], mean_best_reward: --\n",
      " 32008/100000: episode: 872, duration: 0.007s, episode steps: 20, steps per second: 3036, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.084 [-0.827, 1.218], mean_best_reward: --\n",
      " 32060/100000: episode: 873, duration: 0.015s, episode steps: 52, steps per second: 3374, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.148 [-0.934, 0.678], mean_best_reward: --\n",
      " 32074/100000: episode: 874, duration: 0.005s, episode steps: 14, steps per second: 2813, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.115 [-1.611, 0.757], mean_best_reward: --\n",
      " 32094/100000: episode: 875, duration: 0.007s, episode steps: 20, steps per second: 2916, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.051 [-0.982, 1.424], mean_best_reward: --\n",
      " 32112/100000: episode: 876, duration: 0.006s, episode steps: 18, steps per second: 2807, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.107 [-0.554, 1.249], mean_best_reward: --\n",
      " 32146/100000: episode: 877, duration: 0.011s, episode steps: 34, steps per second: 3229, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.195, 0.559], mean_best_reward: --\n",
      " 32167/100000: episode: 878, duration: 0.007s, episode steps: 21, steps per second: 2922, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.046 [-1.391, 0.986], mean_best_reward: --\n",
      " 32195/100000: episode: 879, duration: 0.010s, episode steps: 28, steps per second: 2833, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.092 [-1.285, 0.475], mean_best_reward: --\n",
      " 32230/100000: episode: 880, duration: 0.010s, episode steps: 35, steps per second: 3463, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.142 [-0.534, 0.816], mean_best_reward: --\n",
      " 32255/100000: episode: 881, duration: 0.008s, episode steps: 25, steps per second: 3083, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.091 [-0.593, 1.311], mean_best_reward: --\n",
      " 32285/100000: episode: 882, duration: 0.010s, episode steps: 30, steps per second: 3014, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-1.278, 0.927], mean_best_reward: --\n",
      " 32396/100000: episode: 883, duration: 0.039s, episode steps: 111, steps per second: 2857, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.184 [-1.756, 1.102], mean_best_reward: --\n",
      " 32658/100000: episode: 884, duration: 0.076s, episode steps: 262, steps per second: 3458, episode reward: 262.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.116 [-1.343, 1.554], mean_best_reward: --\n",
      " 32750/100000: episode: 885, duration: 0.027s, episode steps: 92, steps per second: 3406, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.318 [-2.048, 0.682], mean_best_reward: --\n",
      " 32766/100000: episode: 886, duration: 0.005s, episode steps: 16, steps per second: 2929, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.095 [-1.425, 0.929], mean_best_reward: --\n",
      " 32819/100000: episode: 887, duration: 0.016s, episode steps: 53, steps per second: 3370, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.048 [-1.355, 1.410], mean_best_reward: --\n",
      " 32851/100000: episode: 888, duration: 0.010s, episode steps: 32, steps per second: 3226, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.000 [-0.814, 1.266], mean_best_reward: --\n",
      " 32867/100000: episode: 889, duration: 0.006s, episode steps: 16, steps per second: 2700, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.111 [-0.949, 0.622], mean_best_reward: --\n",
      " 32890/100000: episode: 890, duration: 0.007s, episode steps: 23, steps per second: 3091, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.055 [-1.719, 0.975], mean_best_reward: --\n",
      " 32919/100000: episode: 891, duration: 0.009s, episode steps: 29, steps per second: 3058, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.063 [-0.425, 0.884], mean_best_reward: --\n",
      " 33056/100000: episode: 892, duration: 0.039s, episode steps: 137, steps per second: 3546, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.047 [-1.321, 1.111], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33108/100000: episode: 893, duration: 0.023s, episode steps: 52, steps per second: 2225, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.133 [-0.649, 1.115], mean_best_reward: --\n",
      " 33180/100000: episode: 894, duration: 0.029s, episode steps: 72, steps per second: 2448, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.294 [-2.068, 0.925], mean_best_reward: --\n",
      " 33214/100000: episode: 895, duration: 0.011s, episode steps: 34, steps per second: 3043, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.387, 1.060], mean_best_reward: --\n",
      " 33225/100000: episode: 896, duration: 0.004s, episode steps: 11, steps per second: 2536, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.121 [-0.774, 1.346], mean_best_reward: --\n",
      " 33278/100000: episode: 897, duration: 0.017s, episode steps: 53, steps per second: 3159, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.028 [-1.264, 0.809], mean_best_reward: --\n",
      " 33291/100000: episode: 898, duration: 0.005s, episode steps: 13, steps per second: 2570, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.080 [-1.394, 2.144], mean_best_reward: --\n",
      " 33327/100000: episode: 899, duration: 0.012s, episode steps: 36, steps per second: 3069, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.814, 1.190], mean_best_reward: --\n",
      " 33361/100000: episode: 900, duration: 0.012s, episode steps: 34, steps per second: 2854, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.157, 0.533], mean_best_reward: --\n",
      " 33430/100000: episode: 901, duration: 0.021s, episode steps: 69, steps per second: 3222, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.104 [-0.777, 1.079], mean_best_reward: 82.500000\n",
      " 33470/100000: episode: 902, duration: 0.013s, episode steps: 40, steps per second: 3117, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.009 [-1.073, 0.741], mean_best_reward: --\n",
      " 33556/100000: episode: 903, duration: 0.031s, episode steps: 86, steps per second: 2768, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.198 [-1.027, 1.475], mean_best_reward: --\n",
      " 33599/100000: episode: 904, duration: 0.018s, episode steps: 43, steps per second: 2432, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.008 [-1.321, 1.598], mean_best_reward: --\n",
      " 33709/100000: episode: 905, duration: 0.038s, episode steps: 110, steps per second: 2922, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: -0.178 [-2.118, 1.691], mean_best_reward: --\n",
      " 33736/100000: episode: 906, duration: 0.014s, episode steps: 27, steps per second: 1864, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.019 [-1.159, 1.697], mean_best_reward: --\n",
      " 33885/100000: episode: 907, duration: 0.060s, episode steps: 149, steps per second: 2492, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.238 [-1.953, 0.866], mean_best_reward: --\n",
      " 33904/100000: episode: 908, duration: 0.009s, episode steps: 19, steps per second: 2111, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.088 [-0.749, 1.446], mean_best_reward: --\n",
      " 33915/100000: episode: 909, duration: 0.006s, episode steps: 11, steps per second: 1796, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.099 [-1.016, 1.760], mean_best_reward: --\n",
      " 33929/100000: episode: 910, duration: 0.007s, episode steps: 14, steps per second: 1926, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.108 [-1.595, 0.980], mean_best_reward: --\n",
      " 33956/100000: episode: 911, duration: 0.019s, episode steps: 27, steps per second: 1425, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.065 [-0.782, 1.567], mean_best_reward: --\n",
      " 33973/100000: episode: 912, duration: 0.009s, episode steps: 17, steps per second: 1860, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.114 [-0.751, 1.479], mean_best_reward: --\n",
      " 34042/100000: episode: 913, duration: 0.036s, episode steps: 69, steps per second: 1898, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.229 [-0.873, 1.292], mean_best_reward: --\n",
      " 34086/100000: episode: 914, duration: 0.016s, episode steps: 44, steps per second: 2697, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-1.002, 1.353], mean_best_reward: --\n",
      " 34131/100000: episode: 915, duration: 0.019s, episode steps: 45, steps per second: 2315, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.126 [-0.955, 0.388], mean_best_reward: --\n",
      " 34147/100000: episode: 916, duration: 0.008s, episode steps: 16, steps per second: 1975, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.122 [-1.612, 0.750], mean_best_reward: --\n",
      " 34260/100000: episode: 917, duration: 0.058s, episode steps: 113, steps per second: 1934, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.124 [-1.511, 1.342], mean_best_reward: --\n",
      " 34287/100000: episode: 918, duration: 0.014s, episode steps: 27, steps per second: 1904, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.056 [-0.819, 1.323], mean_best_reward: --\n",
      " 34358/100000: episode: 919, duration: 0.035s, episode steps: 71, steps per second: 2047, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.437 [0.000, 1.000], mean observation: -0.262 [-1.603, 0.876], mean_best_reward: --\n",
      " 34375/100000: episode: 920, duration: 0.010s, episode steps: 17, steps per second: 1737, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.079 [-1.977, 1.181], mean_best_reward: --\n",
      " 34401/100000: episode: 921, duration: 0.016s, episode steps: 26, steps per second: 1580, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.081 [-0.655, 1.468], mean_best_reward: --\n",
      " 34423/100000: episode: 922, duration: 0.013s, episode steps: 22, steps per second: 1647, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.097 [-0.842, 0.437], mean_best_reward: --\n",
      " 34481/100000: episode: 923, duration: 0.026s, episode steps: 58, steps per second: 2273, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.200 [-0.795, 1.189], mean_best_reward: --\n",
      " 34513/100000: episode: 924, duration: 0.017s, episode steps: 32, steps per second: 1923, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.148 [-1.090, 1.949], mean_best_reward: --\n",
      " 34557/100000: episode: 925, duration: 0.026s, episode steps: 44, steps per second: 1677, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.073 [-0.871, 1.111], mean_best_reward: --\n",
      " 34670/100000: episode: 926, duration: 0.055s, episode steps: 113, steps per second: 2045, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.087 [-1.131, 1.220], mean_best_reward: --\n",
      " 34717/100000: episode: 927, duration: 0.015s, episode steps: 47, steps per second: 3156, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.013 [-1.647, 1.360], mean_best_reward: --\n",
      " 34746/100000: episode: 928, duration: 0.009s, episode steps: 29, steps per second: 3216, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.070 [-0.375, 1.010], mean_best_reward: --\n",
      " 34764/100000: episode: 929, duration: 0.006s, episode steps: 18, steps per second: 2972, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.110 [-1.600, 0.766], mean_best_reward: --\n",
      " 34834/100000: episode: 930, duration: 0.021s, episode steps: 70, steps per second: 3365, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.094 [-1.300, 1.513], mean_best_reward: --\n",
      " 34849/100000: episode: 931, duration: 0.005s, episode steps: 15, steps per second: 2857, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.092 [-1.196, 0.832], mean_best_reward: --\n",
      " 34920/100000: episode: 932, duration: 0.025s, episode steps: 71, steps per second: 2834, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.218 [-1.526, 0.810], mean_best_reward: --\n",
      " 34966/100000: episode: 933, duration: 0.014s, episode steps: 46, steps per second: 3195, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.905, 1.180], mean_best_reward: --\n",
      " 34988/100000: episode: 934, duration: 0.010s, episode steps: 22, steps per second: 2160, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.572, 1.055], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35071/100000: episode: 935, duration: 0.041s, episode steps: 83, steps per second: 2018, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.044 [-1.431, 1.359], mean_best_reward: --\n",
      " 35081/100000: episode: 936, duration: 0.006s, episode steps: 10, steps per second: 1707, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.116 [-1.823, 1.181], mean_best_reward: --\n",
      " 35129/100000: episode: 937, duration: 0.025s, episode steps: 48, steps per second: 1942, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.164 [-0.539, 1.396], mean_best_reward: --\n",
      " 35175/100000: episode: 938, duration: 0.022s, episode steps: 46, steps per second: 2104, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-1.543, 0.830], mean_best_reward: --\n",
      " 35244/100000: episode: 939, duration: 0.023s, episode steps: 69, steps per second: 3046, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.183 [-1.293, 0.846], mean_best_reward: --\n",
      " 35339/100000: episode: 940, duration: 0.028s, episode steps: 95, steps per second: 3372, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.222 [-1.320, 1.518], mean_best_reward: --\n",
      " 35397/100000: episode: 941, duration: 0.018s, episode steps: 58, steps per second: 3291, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.202 [-0.435, 1.292], mean_best_reward: --\n",
      " 35412/100000: episode: 942, duration: 0.005s, episode steps: 15, steps per second: 2941, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.123 [-0.758, 1.238], mean_best_reward: --\n",
      " 35481/100000: episode: 943, duration: 0.020s, episode steps: 69, steps per second: 3527, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.137 [-1.434, 1.221], mean_best_reward: --\n",
      " 35504/100000: episode: 944, duration: 0.007s, episode steps: 23, steps per second: 3183, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.067 [-1.204, 2.068], mean_best_reward: --\n",
      " 35558/100000: episode: 945, duration: 0.016s, episode steps: 54, steps per second: 3277, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.221 [-0.718, 1.501], mean_best_reward: --\n",
      " 35577/100000: episode: 946, duration: 0.007s, episode steps: 19, steps per second: 2914, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.110 [-1.192, 0.766], mean_best_reward: --\n",
      " 35611/100000: episode: 947, duration: 0.015s, episode steps: 34, steps per second: 2301, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.116 [-0.665, 0.894], mean_best_reward: --\n",
      " 35627/100000: episode: 948, duration: 0.010s, episode steps: 16, steps per second: 1547, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.107 [-0.794, 1.605], mean_best_reward: --\n",
      " 35672/100000: episode: 949, duration: 0.027s, episode steps: 45, steps per second: 1668, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.050 [-0.629, 1.193], mean_best_reward: --\n",
      " 35704/100000: episode: 950, duration: 0.023s, episode steps: 32, steps per second: 1415, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.076 [-0.923, 0.495], mean_best_reward: --\n",
      " 35765/100000: episode: 951, duration: 0.032s, episode steps: 61, steps per second: 1906, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.007 [-1.504, 0.771], mean_best_reward: 111.000000\n",
      " 35782/100000: episode: 952, duration: 0.008s, episode steps: 17, steps per second: 2076, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.115 [-0.753, 1.521], mean_best_reward: --\n",
      " 35809/100000: episode: 953, duration: 0.010s, episode steps: 27, steps per second: 2747, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.118 [-0.948, 0.607], mean_best_reward: --\n",
      " 35829/100000: episode: 954, duration: 0.008s, episode steps: 20, steps per second: 2508, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.814, 1.149], mean_best_reward: --\n",
      " 35845/100000: episode: 955, duration: 0.006s, episode steps: 16, steps per second: 2576, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.084 [-1.607, 0.982], mean_best_reward: --\n",
      " 35868/100000: episode: 956, duration: 0.008s, episode steps: 23, steps per second: 2873, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.077 [-0.800, 1.170], mean_best_reward: --\n",
      " 35878/100000: episode: 957, duration: 0.004s, episode steps: 10, steps per second: 2610, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.128 [-1.007, 1.564], mean_best_reward: --\n",
      " 36010/100000: episode: 958, duration: 0.039s, episode steps: 132, steps per second: 3420, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.248 [-1.853, 1.051], mean_best_reward: --\n",
      " 36037/100000: episode: 959, duration: 0.009s, episode steps: 27, steps per second: 2909, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.079 [-0.445, 0.853], mean_best_reward: --\n",
      " 36055/100000: episode: 960, duration: 0.007s, episode steps: 18, steps per second: 2590, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.104 [-0.551, 1.165], mean_best_reward: --\n",
      " 36085/100000: episode: 961, duration: 0.014s, episode steps: 30, steps per second: 2096, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.074 [-1.026, 1.917], mean_best_reward: --\n",
      " 36107/100000: episode: 962, duration: 0.011s, episode steps: 22, steps per second: 2079, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.364, 0.852], mean_best_reward: --\n",
      " 36167/100000: episode: 963, duration: 0.024s, episode steps: 60, steps per second: 2476, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.021 [-1.001, 1.246], mean_best_reward: --\n",
      " 36219/100000: episode: 964, duration: 0.017s, episode steps: 52, steps per second: 2986, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.054 [-0.777, 1.289], mean_best_reward: --\n",
      " 36254/100000: episode: 965, duration: 0.010s, episode steps: 35, steps per second: 3378, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.004 [-1.925, 1.372], mean_best_reward: --\n",
      " 36376/100000: episode: 966, duration: 0.039s, episode steps: 122, steps per second: 3104, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.522, 1.258], mean_best_reward: --\n",
      " 36389/100000: episode: 967, duration: 0.004s, episode steps: 13, steps per second: 2895, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.085 [-1.188, 1.750], mean_best_reward: --\n",
      " 36436/100000: episode: 968, duration: 0.021s, episode steps: 47, steps per second: 2205, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.078 [-0.584, 1.250], mean_best_reward: --\n",
      " 36487/100000: episode: 969, duration: 0.018s, episode steps: 51, steps per second: 2763, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: 0.044 [-1.328, 2.144], mean_best_reward: --\n",
      " 36524/100000: episode: 970, duration: 0.019s, episode steps: 37, steps per second: 1962, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.033 [-1.147, 0.971], mean_best_reward: --\n",
      " 36544/100000: episode: 971, duration: 0.010s, episode steps: 20, steps per second: 2075, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.051 [-0.985, 1.492], mean_best_reward: --\n",
      " 36569/100000: episode: 972, duration: 0.013s, episode steps: 25, steps per second: 1918, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.019 [-1.010, 1.377], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36615/100000: episode: 973, duration: 0.025s, episode steps: 46, steps per second: 1823, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.103 [-0.507, 1.472], mean_best_reward: --\n",
      " 36642/100000: episode: 974, duration: 0.012s, episode steps: 27, steps per second: 2273, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.609, 1.408], mean_best_reward: --\n",
      " 36691/100000: episode: 975, duration: 0.031s, episode steps: 49, steps per second: 1579, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.018 [-1.414, 2.355], mean_best_reward: --\n",
      " 36715/100000: episode: 976, duration: 0.012s, episode steps: 24, steps per second: 2079, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.043 [-0.946, 1.458], mean_best_reward: --\n",
      " 36770/100000: episode: 977, duration: 0.028s, episode steps: 55, steps per second: 1940, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.041 [-1.724, 1.804], mean_best_reward: --\n",
      " 36830/100000: episode: 978, duration: 0.031s, episode steps: 60, steps per second: 1923, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: 0.036 [-1.689, 1.733], mean_best_reward: --\n",
      " 36876/100000: episode: 979, duration: 0.023s, episode steps: 46, steps per second: 2003, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.967, 1.542], mean_best_reward: --\n",
      " 36897/100000: episode: 980, duration: 0.013s, episode steps: 21, steps per second: 1674, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.105 [-0.999, 0.605], mean_best_reward: --\n",
      " 36925/100000: episode: 981, duration: 0.012s, episode steps: 28, steps per second: 2295, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.028 [-1.036, 0.643], mean_best_reward: --\n",
      " 36969/100000: episode: 982, duration: 0.023s, episode steps: 44, steps per second: 1902, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.114 [-0.437, 0.899], mean_best_reward: --\n",
      " 36995/100000: episode: 983, duration: 0.012s, episode steps: 26, steps per second: 2116, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.040 [-0.836, 1.443], mean_best_reward: --\n",
      " 37053/100000: episode: 984, duration: 0.029s, episode steps: 58, steps per second: 2026, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.060 [-0.560, 1.004], mean_best_reward: --\n",
      " 37105/100000: episode: 985, duration: 0.019s, episode steps: 52, steps per second: 2688, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.040 [-1.359, 1.765], mean_best_reward: --\n",
      " 37213/100000: episode: 986, duration: 0.041s, episode steps: 108, steps per second: 2610, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.233 [-1.509, 0.945], mean_best_reward: --\n",
      " 37258/100000: episode: 987, duration: 0.014s, episode steps: 45, steps per second: 3330, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.141 [-1.603, 0.667], mean_best_reward: --\n",
      " 37282/100000: episode: 988, duration: 0.008s, episode steps: 24, steps per second: 3090, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.100 [-1.344, 0.594], mean_best_reward: --\n",
      " 37325/100000: episode: 989, duration: 0.013s, episode steps: 43, steps per second: 3205, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.115 [-1.125, 0.761], mean_best_reward: --\n",
      " 37426/100000: episode: 990, duration: 0.030s, episode steps: 101, steps per second: 3420, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.132 [-1.331, 0.936], mean_best_reward: --\n",
      " 37479/100000: episode: 991, duration: 0.016s, episode steps: 53, steps per second: 3357, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.111 [-1.082, 0.561], mean_best_reward: --\n",
      " 37536/100000: episode: 992, duration: 0.017s, episode steps: 57, steps per second: 3364, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.047 [-0.839, 1.330], mean_best_reward: --\n",
      " 37605/100000: episode: 993, duration: 0.021s, episode steps: 69, steps per second: 3291, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.104 [-0.539, 0.703], mean_best_reward: --\n",
      " 37626/100000: episode: 994, duration: 0.012s, episode steps: 21, steps per second: 1774, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.074 [-0.779, 1.553], mean_best_reward: --\n",
      " 37638/100000: episode: 995, duration: 0.005s, episode steps: 12, steps per second: 2552, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.089 [-1.367, 0.805], mean_best_reward: --\n",
      " 37650/100000: episode: 996, duration: 0.004s, episode steps: 12, steps per second: 2686, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.135 [-0.750, 1.313], mean_best_reward: --\n",
      " 37663/100000: episode: 997, duration: 0.005s, episode steps: 13, steps per second: 2633, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.114 [-0.598, 1.337], mean_best_reward: --\n",
      " 37679/100000: episode: 998, duration: 0.005s, episode steps: 16, steps per second: 2919, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.117 [-1.665, 0.788], mean_best_reward: --\n",
      " 37697/100000: episode: 999, duration: 0.006s, episode steps: 18, steps per second: 3064, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.096 [-0.805, 1.425], mean_best_reward: --\n",
      " 37730/100000: episode: 1000, duration: 0.010s, episode steps: 33, steps per second: 3253, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.096 [-0.624, 1.096], mean_best_reward: --\n",
      " 37763/100000: episode: 1001, duration: 0.011s, episode steps: 33, steps per second: 2915, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.110 [-0.360, 0.983], mean_best_reward: 90.000000\n",
      " 37797/100000: episode: 1002, duration: 0.011s, episode steps: 34, steps per second: 2975, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.076 [-1.355, 0.981], mean_best_reward: --\n",
      " 37820/100000: episode: 1003, duration: 0.007s, episode steps: 23, steps per second: 3082, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.089 [-0.421, 1.122], mean_best_reward: --\n",
      " 37946/100000: episode: 1004, duration: 0.037s, episode steps: 126, steps per second: 3435, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-1.116, 1.355], mean_best_reward: --\n",
      " 37962/100000: episode: 1005, duration: 0.006s, episode steps: 16, steps per second: 2526, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.093 [-0.609, 1.189], mean_best_reward: --\n",
      " 37988/100000: episode: 1006, duration: 0.009s, episode steps: 26, steps per second: 3012, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.066 [-0.819, 1.616], mean_best_reward: --\n",
      " 38032/100000: episode: 1007, duration: 0.014s, episode steps: 44, steps per second: 3231, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-1.127, 0.682], mean_best_reward: --\n",
      " 38053/100000: episode: 1008, duration: 0.007s, episode steps: 21, steps per second: 2857, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.096 [-0.585, 1.121], mean_best_reward: --\n",
      " 38174/100000: episode: 1009, duration: 0.036s, episode steps: 121, steps per second: 3343, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.281 [-1.352, 2.037], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38286/100000: episode: 1010, duration: 0.041s, episode steps: 112, steps per second: 2756, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.084 [-1.109, 1.441], mean_best_reward: --\n",
      " 38352/100000: episode: 1011, duration: 0.021s, episode steps: 66, steps per second: 3081, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.171 [-0.707, 1.159], mean_best_reward: --\n",
      " 38416/100000: episode: 1012, duration: 0.019s, episode steps: 64, steps per second: 3403, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.173 [-0.590, 1.164], mean_best_reward: --\n",
      " 38441/100000: episode: 1013, duration: 0.008s, episode steps: 25, steps per second: 3159, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.099 [-0.612, 1.066], mean_best_reward: --\n",
      " 38507/100000: episode: 1014, duration: 0.019s, episode steps: 66, steps per second: 3398, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.148 [-0.540, 1.159], mean_best_reward: --\n",
      " 38540/100000: episode: 1015, duration: 0.010s, episode steps: 33, steps per second: 3162, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.006 [-0.945, 1.247], mean_best_reward: --\n",
      " 38686/100000: episode: 1016, duration: 0.047s, episode steps: 146, steps per second: 3109, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.063 [-1.771, 0.960], mean_best_reward: --\n",
      " 38724/100000: episode: 1017, duration: 0.016s, episode steps: 38, steps per second: 2391, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.024 [-0.821, 1.591], mean_best_reward: --\n",
      " 38766/100000: episode: 1018, duration: 0.021s, episode steps: 42, steps per second: 1962, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.044 [-0.661, 1.523], mean_best_reward: --\n",
      " 38800/100000: episode: 1019, duration: 0.016s, episode steps: 34, steps per second: 2061, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.618 [0.000, 1.000], mean observation: -0.034 [-2.335, 1.554], mean_best_reward: --\n",
      " 38820/100000: episode: 1020, duration: 0.014s, episode steps: 20, steps per second: 1412, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.079 [-0.615, 1.190], mean_best_reward: --\n",
      " 38837/100000: episode: 1021, duration: 0.012s, episode steps: 17, steps per second: 1472, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.081 [-0.610, 1.136], mean_best_reward: --\n",
      " 38910/100000: episode: 1022, duration: 0.034s, episode steps: 73, steps per second: 2128, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.142 [-1.700, 1.097], mean_best_reward: --\n",
      " 38947/100000: episode: 1023, duration: 0.015s, episode steps: 37, steps per second: 2446, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.096 [-1.043, 1.603], mean_best_reward: --\n",
      " 38971/100000: episode: 1024, duration: 0.010s, episode steps: 24, steps per second: 2464, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.050 [-0.954, 1.434], mean_best_reward: --\n",
      " 38993/100000: episode: 1025, duration: 0.008s, episode steps: 22, steps per second: 2902, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.051 [-1.152, 1.708], mean_best_reward: --\n",
      " 39076/100000: episode: 1026, duration: 0.031s, episode steps: 83, steps per second: 2649, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.045 [-0.842, 1.047], mean_best_reward: --\n",
      " 39092/100000: episode: 1027, duration: 0.006s, episode steps: 16, steps per second: 2537, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.076 [-1.014, 1.770], mean_best_reward: --\n",
      " 39156/100000: episode: 1028, duration: 0.027s, episode steps: 64, steps per second: 2390, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.173 [-1.312, 0.589], mean_best_reward: --\n",
      " 39188/100000: episode: 1029, duration: 0.011s, episode steps: 32, steps per second: 2989, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.063 [-0.956, 1.732], mean_best_reward: --\n",
      " 39208/100000: episode: 1030, duration: 0.007s, episode steps: 20, steps per second: 2899, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.076 [-1.537, 0.930], mean_best_reward: --\n",
      " 39230/100000: episode: 1031, duration: 0.008s, episode steps: 22, steps per second: 2919, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.100 [-1.085, 0.391], mean_best_reward: --\n",
      " 39251/100000: episode: 1032, duration: 0.007s, episode steps: 21, steps per second: 2904, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.087 [-0.765, 1.300], mean_best_reward: --\n",
      " 39291/100000: episode: 1033, duration: 0.012s, episode steps: 40, steps per second: 3274, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.628, 1.216], mean_best_reward: --\n",
      " 39319/100000: episode: 1034, duration: 0.009s, episode steps: 28, steps per second: 3053, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.746, 1.450], mean_best_reward: --\n",
      " 39365/100000: episode: 1035, duration: 0.015s, episode steps: 46, steps per second: 3136, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.004 [-0.965, 1.096], mean_best_reward: --\n",
      " 39524/100000: episode: 1036, duration: 0.054s, episode steps: 159, steps per second: 2951, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.062 [-1.274, 1.060], mean_best_reward: --\n",
      " 39554/100000: episode: 1037, duration: 0.010s, episode steps: 30, steps per second: 3058, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.095 [-0.407, 1.258], mean_best_reward: --\n",
      " 39591/100000: episode: 1038, duration: 0.012s, episode steps: 37, steps per second: 3180, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.094 [-0.887, 1.089], mean_best_reward: --\n",
      " 39614/100000: episode: 1039, duration: 0.008s, episode steps: 23, steps per second: 3059, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.096 [-2.133, 1.144], mean_best_reward: --\n",
      " 39635/100000: episode: 1040, duration: 0.008s, episode steps: 21, steps per second: 2742, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.081 [-1.995, 1.176], mean_best_reward: --\n",
      " 39658/100000: episode: 1041, duration: 0.008s, episode steps: 23, steps per second: 2915, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.084 [-1.462, 0.954], mean_best_reward: --\n",
      " 39670/100000: episode: 1042, duration: 0.004s, episode steps: 12, steps per second: 2730, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.126 [-1.334, 2.146], mean_best_reward: --\n",
      " 39791/100000: episode: 1043, duration: 0.035s, episode steps: 121, steps per second: 3506, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.232 [-0.996, 1.685], mean_best_reward: --\n",
      " 39847/100000: episode: 1044, duration: 0.017s, episode steps: 56, steps per second: 3259, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.062 [-1.576, 0.923], mean_best_reward: --\n",
      " 39903/100000: episode: 1045, duration: 0.016s, episode steps: 56, steps per second: 3405, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.093 [-1.701, 1.735], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40001/100000: episode: 1046, duration: 0.029s, episode steps: 98, steps per second: 3377, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.271 [-1.520, 0.803], mean_best_reward: --\n",
      " 40016/100000: episode: 1047, duration: 0.006s, episode steps: 15, steps per second: 2710, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-1.194, 1.932], mean_best_reward: --\n",
      " 40027/100000: episode: 1048, duration: 0.006s, episode steps: 11, steps per second: 1942, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.131 [-1.665, 0.995], mean_best_reward: --\n",
      " 40099/100000: episode: 1049, duration: 0.025s, episode steps: 72, steps per second: 2908, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.088 [-1.017, 1.096], mean_best_reward: --\n",
      " 40125/100000: episode: 1050, duration: 0.009s, episode steps: 26, steps per second: 2940, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.027 [-0.988, 1.308], mean_best_reward: --\n",
      " 40141/100000: episode: 1051, duration: 0.006s, episode steps: 16, steps per second: 2620, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-1.181, 1.705], mean_best_reward: 112.000000\n",
      " 40152/100000: episode: 1052, duration: 0.004s, episode steps: 11, steps per second: 2694, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.123 [-1.124, 1.776], mean_best_reward: --\n",
      " 40164/100000: episode: 1053, duration: 0.004s, episode steps: 12, steps per second: 2807, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.095 [-0.980, 1.626], mean_best_reward: --\n",
      " 40196/100000: episode: 1054, duration: 0.010s, episode steps: 32, steps per second: 3117, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.059 [-1.268, 0.597], mean_best_reward: --\n",
      " 40286/100000: episode: 1055, duration: 0.028s, episode steps: 90, steps per second: 3250, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.316 [-1.077, 1.862], mean_best_reward: --\n",
      " 40355/100000: episode: 1056, duration: 0.023s, episode steps: 69, steps per second: 3049, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.045 [-1.187, 1.445], mean_best_reward: --\n",
      " 40454/100000: episode: 1057, duration: 0.029s, episode steps: 99, steps per second: 3420, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.224 [-1.335, 0.846], mean_best_reward: --\n",
      " 40495/100000: episode: 1058, duration: 0.013s, episode steps: 41, steps per second: 3143, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.106 [-1.180, 0.807], mean_best_reward: --\n",
      " 40512/100000: episode: 1059, duration: 0.006s, episode steps: 17, steps per second: 2800, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.097 [-0.571, 1.227], mean_best_reward: --\n",
      " 40534/100000: episode: 1060, duration: 0.007s, episode steps: 22, steps per second: 3039, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.064 [-1.155, 1.870], mean_best_reward: --\n",
      " 40555/100000: episode: 1061, duration: 0.007s, episode steps: 21, steps per second: 3011, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.076 [-0.795, 1.312], mean_best_reward: --\n",
      " 40574/100000: episode: 1062, duration: 0.006s, episode steps: 19, steps per second: 2947, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.094 [-1.352, 0.655], mean_best_reward: --\n",
      " 40591/100000: episode: 1063, duration: 0.006s, episode steps: 17, steps per second: 2798, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.097 [-0.628, 1.037], mean_best_reward: --\n",
      " 40626/100000: episode: 1064, duration: 0.013s, episode steps: 35, steps per second: 2735, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.102 [-0.407, 1.266], mean_best_reward: --\n",
      " 40675/100000: episode: 1065, duration: 0.017s, episode steps: 49, steps per second: 2836, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.119 [-0.974, 0.891], mean_best_reward: --\n",
      " 40707/100000: episode: 1066, duration: 0.010s, episode steps: 32, steps per second: 3065, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.079 [-1.132, 0.548], mean_best_reward: --\n",
      " 40718/100000: episode: 1067, duration: 0.004s, episode steps: 11, steps per second: 2544, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.125 [-0.950, 1.517], mean_best_reward: --\n",
      " 40746/100000: episode: 1068, duration: 0.009s, episode steps: 28, steps per second: 3003, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.399, 1.124], mean_best_reward: --\n",
      " 40758/100000: episode: 1069, duration: 0.004s, episode steps: 12, steps per second: 2718, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.117 [-1.146, 1.769], mean_best_reward: --\n",
      " 40793/100000: episode: 1070, duration: 0.011s, episode steps: 35, steps per second: 3284, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.038 [-1.035, 0.891], mean_best_reward: --\n",
      " 40805/100000: episode: 1071, duration: 0.005s, episode steps: 12, steps per second: 2639, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.759, 1.354], mean_best_reward: --\n",
      " 40835/100000: episode: 1072, duration: 0.010s, episode steps: 30, steps per second: 3140, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.746, 1.186], mean_best_reward: --\n",
      " 40859/100000: episode: 1073, duration: 0.008s, episode steps: 24, steps per second: 2889, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.068 [-0.806, 1.641], mean_best_reward: --\n",
      " 40931/100000: episode: 1074, duration: 0.021s, episode steps: 72, steps per second: 3371, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.730, 1.173], mean_best_reward: --\n",
      " 40955/100000: episode: 1075, duration: 0.008s, episode steps: 24, steps per second: 2970, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.079 [-0.590, 1.140], mean_best_reward: --\n",
      " 41018/100000: episode: 1076, duration: 0.019s, episode steps: 63, steps per second: 3257, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: -0.067 [-1.909, 1.050], mean_best_reward: --\n",
      " 41051/100000: episode: 1077, duration: 0.010s, episode steps: 33, steps per second: 3207, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.004 [-0.774, 1.103], mean_best_reward: --\n",
      " 41115/100000: episode: 1078, duration: 0.019s, episode steps: 64, steps per second: 3333, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.084 [-1.327, 1.379], mean_best_reward: --\n",
      " 41166/100000: episode: 1079, duration: 0.015s, episode steps: 51, steps per second: 3324, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.044 [-1.142, 0.993], mean_best_reward: --\n",
      " 41229/100000: episode: 1080, duration: 0.019s, episode steps: 63, steps per second: 3393, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.154 [-1.433, 0.743], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41278/100000: episode: 1081, duration: 0.016s, episode steps: 49, steps per second: 3027, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.112 [-0.773, 1.258], mean_best_reward: --\n",
      " 41295/100000: episode: 1082, duration: 0.008s, episode steps: 17, steps per second: 2029, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.086 [-1.536, 1.021], mean_best_reward: --\n",
      " 41315/100000: episode: 1083, duration: 0.009s, episode steps: 20, steps per second: 2284, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.961, 1.378], mean_best_reward: --\n",
      " 41328/100000: episode: 1084, duration: 0.005s, episode steps: 13, steps per second: 2673, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.100 [-1.394, 0.819], mean_best_reward: --\n",
      " 41343/100000: episode: 1085, duration: 0.005s, episode steps: 15, steps per second: 2870, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.080 [-1.515, 1.000], mean_best_reward: --\n",
      " 41365/100000: episode: 1086, duration: 0.008s, episode steps: 22, steps per second: 2811, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.049 [-0.809, 1.528], mean_best_reward: --\n",
      " 41410/100000: episode: 1087, duration: 0.014s, episode steps: 45, steps per second: 3200, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.029 [-0.623, 1.141], mean_best_reward: --\n",
      " 41426/100000: episode: 1088, duration: 0.005s, episode steps: 16, steps per second: 2963, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.106 [-0.937, 1.657], mean_best_reward: --\n",
      " 41458/100000: episode: 1089, duration: 0.010s, episode steps: 32, steps per second: 3254, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.050 [-0.642, 1.135], mean_best_reward: --\n",
      " 41479/100000: episode: 1090, duration: 0.007s, episode steps: 21, steps per second: 3114, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.127 [-0.547, 1.133], mean_best_reward: --\n",
      " 41536/100000: episode: 1091, duration: 0.017s, episode steps: 57, steps per second: 3339, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.185 [-0.793, 1.207], mean_best_reward: --\n",
      " 41604/100000: episode: 1092, duration: 0.020s, episode steps: 68, steps per second: 3370, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.045 [-1.052, 1.227], mean_best_reward: --\n",
      " 41647/100000: episode: 1093, duration: 0.013s, episode steps: 43, steps per second: 3320, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.054 [-0.763, 1.252], mean_best_reward: --\n",
      " 41711/100000: episode: 1094, duration: 0.019s, episode steps: 64, steps per second: 3360, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.160 [-1.561, 1.308], mean_best_reward: --\n",
      " 41744/100000: episode: 1095, duration: 0.010s, episode steps: 33, steps per second: 3205, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.034 [-1.378, 0.805], mean_best_reward: --\n",
      " 41762/100000: episode: 1096, duration: 0.006s, episode steps: 18, steps per second: 2877, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.072 [-0.799, 1.483], mean_best_reward: --\n",
      " 41781/100000: episode: 1097, duration: 0.006s, episode steps: 19, steps per second: 3084, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.112 [-1.064, 0.381], mean_best_reward: --\n",
      " 41842/100000: episode: 1098, duration: 0.018s, episode steps: 61, steps per second: 3396, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.157 [-1.084, 0.619], mean_best_reward: --\n",
      " 41859/100000: episode: 1099, duration: 0.006s, episode steps: 17, steps per second: 2963, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.113 [-0.747, 1.245], mean_best_reward: --\n",
      " 41877/100000: episode: 1100, duration: 0.007s, episode steps: 18, steps per second: 2531, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.039 [-1.542, 2.285], mean_best_reward: --\n",
      " 41903/100000: episode: 1101, duration: 0.012s, episode steps: 26, steps per second: 2136, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.550, 1.128], mean_best_reward: 156.000000\n",
      " 41931/100000: episode: 1102, duration: 0.013s, episode steps: 28, steps per second: 2239, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.191, 1.177], mean_best_reward: --\n",
      " 41982/100000: episode: 1103, duration: 0.018s, episode steps: 51, steps per second: 2766, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.104 [-1.652, 0.877], mean_best_reward: --\n",
      " 42006/100000: episode: 1104, duration: 0.008s, episode steps: 24, steps per second: 3004, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.108 [-0.548, 0.973], mean_best_reward: --\n",
      " 42017/100000: episode: 1105, duration: 0.004s, episode steps: 11, steps per second: 2526, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.103 [-1.002, 1.651], mean_best_reward: --\n",
      " 42055/100000: episode: 1106, duration: 0.012s, episode steps: 38, steps per second: 3189, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.059 [-0.586, 0.897], mean_best_reward: --\n",
      " 42124/100000: episode: 1107, duration: 0.020s, episode steps: 69, steps per second: 3401, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.066 [-1.448, 1.351], mean_best_reward: --\n",
      " 42162/100000: episode: 1108, duration: 0.011s, episode steps: 38, steps per second: 3311, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.515, 1.206], mean_best_reward: --\n",
      " 42197/100000: episode: 1109, duration: 0.011s, episode steps: 35, steps per second: 3279, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.090 [-0.812, 1.418], mean_best_reward: --\n",
      " 42229/100000: episode: 1110, duration: 0.010s, episode steps: 32, steps per second: 3250, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.070 [-0.641, 1.554], mean_best_reward: --\n",
      " 42261/100000: episode: 1111, duration: 0.010s, episode steps: 32, steps per second: 3240, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.012 [-0.967, 1.168], mean_best_reward: --\n",
      " 42301/100000: episode: 1112, duration: 0.012s, episode steps: 40, steps per second: 3222, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.117 [-1.184, 0.438], mean_best_reward: --\n",
      " 42345/100000: episode: 1113, duration: 0.014s, episode steps: 44, steps per second: 3090, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.558, 1.200], mean_best_reward: --\n",
      " 42363/100000: episode: 1114, duration: 0.007s, episode steps: 18, steps per second: 2693, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.059 [-1.271, 0.818], mean_best_reward: --\n",
      " 42389/100000: episode: 1115, duration: 0.008s, episode steps: 26, steps per second: 3097, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-1.110, 0.578], mean_best_reward: --\n",
      " 42414/100000: episode: 1116, duration: 0.008s, episode steps: 25, steps per second: 3115, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.108 [-0.988, 0.382], mean_best_reward: --\n",
      " 42433/100000: episode: 1117, duration: 0.006s, episode steps: 19, steps per second: 3057, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.082 [-0.968, 1.828], mean_best_reward: --\n",
      " 42450/100000: episode: 1118, duration: 0.006s, episode steps: 17, steps per second: 2866, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.091 [-1.007, 1.640], mean_best_reward: --\n",
      " 42478/100000: episode: 1119, duration: 0.009s, episode steps: 28, steps per second: 3227, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.129 [-0.540, 0.942], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42506/100000: episode: 1120, duration: 0.009s, episode steps: 28, steps per second: 3046, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.064 [-0.442, 0.823], mean_best_reward: --\n",
      " 42533/100000: episode: 1121, duration: 0.009s, episode steps: 27, steps per second: 3153, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.030 [-1.680, 1.013], mean_best_reward: --\n",
      " 42565/100000: episode: 1122, duration: 0.012s, episode steps: 32, steps per second: 2627, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.055 [-0.438, 1.051], mean_best_reward: --\n",
      " 42581/100000: episode: 1123, duration: 0.008s, episode steps: 16, steps per second: 1909, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.072 [-0.823, 1.436], mean_best_reward: --\n",
      " 42662/100000: episode: 1124, duration: 0.024s, episode steps: 81, steps per second: 3361, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.036 [-1.345, 1.177], mean_best_reward: --\n",
      " 42692/100000: episode: 1125, duration: 0.009s, episode steps: 30, steps per second: 3209, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.148 [-0.752, 1.300], mean_best_reward: --\n",
      " 42760/100000: episode: 1126, duration: 0.020s, episode steps: 68, steps per second: 3350, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.333 [-0.742, 1.695], mean_best_reward: --\n",
      " 42789/100000: episode: 1127, duration: 0.009s, episode steps: 29, steps per second: 3164, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.586 [0.000, 1.000], mean observation: -0.055 [-1.840, 1.172], mean_best_reward: --\n",
      " 42822/100000: episode: 1128, duration: 0.011s, episode steps: 33, steps per second: 3079, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.098 [-0.644, 1.874], mean_best_reward: --\n",
      " 42880/100000: episode: 1129, duration: 0.017s, episode steps: 58, steps per second: 3345, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.057 [-1.378, 0.997], mean_best_reward: --\n",
      " 42893/100000: episode: 1130, duration: 0.005s, episode steps: 13, steps per second: 2805, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.084 [-1.018, 1.675], mean_best_reward: --\n",
      " 42918/100000: episode: 1131, duration: 0.008s, episode steps: 25, steps per second: 3133, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.070 [-1.001, 1.950], mean_best_reward: --\n",
      " 42932/100000: episode: 1132, duration: 0.005s, episode steps: 14, steps per second: 2960, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.107 [-0.944, 1.533], mean_best_reward: --\n",
      " 42954/100000: episode: 1133, duration: 0.007s, episode steps: 22, steps per second: 2995, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.567, 1.192], mean_best_reward: --\n",
      " 42971/100000: episode: 1134, duration: 0.006s, episode steps: 17, steps per second: 2957, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.081 [-0.807, 1.428], mean_best_reward: --\n",
      " 43027/100000: episode: 1135, duration: 0.017s, episode steps: 56, steps per second: 3325, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.121, 1.020], mean_best_reward: --\n",
      " 43036/100000: episode: 1136, duration: 0.003s, episode steps: 9, steps per second: 2584, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.117 [-1.578, 0.963], mean_best_reward: --\n",
      " 43058/100000: episode: 1137, duration: 0.007s, episode steps: 22, steps per second: 3126, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.091 [-0.574, 1.261], mean_best_reward: --\n",
      " 43161/100000: episode: 1138, duration: 0.033s, episode steps: 103, steps per second: 3103, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.338 [-1.725, 1.024], mean_best_reward: --\n",
      " 43184/100000: episode: 1139, duration: 0.011s, episode steps: 23, steps per second: 2124, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.069 [-1.114, 0.620], mean_best_reward: --\n",
      " 43213/100000: episode: 1140, duration: 0.010s, episode steps: 29, steps per second: 3046, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.052 [-1.447, 0.814], mean_best_reward: --\n",
      " 43243/100000: episode: 1141, duration: 0.010s, episode steps: 30, steps per second: 3030, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.093 [-0.605, 0.960], mean_best_reward: --\n",
      " 43255/100000: episode: 1142, duration: 0.004s, episode steps: 12, steps per second: 2707, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.090 [-2.163, 1.402], mean_best_reward: --\n",
      " 43291/100000: episode: 1143, duration: 0.011s, episode steps: 36, steps per second: 3176, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.745, 1.120], mean_best_reward: --\n",
      " 43358/100000: episode: 1144, duration: 0.020s, episode steps: 67, steps per second: 3393, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.083 [-0.914, 1.171], mean_best_reward: --\n",
      " 43405/100000: episode: 1145, duration: 0.014s, episode steps: 47, steps per second: 3336, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.117 [-1.155, 1.142], mean_best_reward: --\n",
      " 43431/100000: episode: 1146, duration: 0.008s, episode steps: 26, steps per second: 3106, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-1.171, 0.781], mean_best_reward: --\n",
      " 43458/100000: episode: 1147, duration: 0.010s, episode steps: 27, steps per second: 2804, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.043 [-0.783, 1.140], mean_best_reward: --\n",
      " 43528/100000: episode: 1148, duration: 0.023s, episode steps: 70, steps per second: 3027, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-1.144, 0.917], mean_best_reward: --\n",
      " 43546/100000: episode: 1149, duration: 0.006s, episode steps: 18, steps per second: 2983, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.397, 0.961], mean_best_reward: --\n",
      " 43576/100000: episode: 1150, duration: 0.010s, episode steps: 30, steps per second: 3130, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.139 [-0.751, 1.268], mean_best_reward: --\n",
      " 43598/100000: episode: 1151, duration: 0.008s, episode steps: 22, steps per second: 2905, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.098 [-1.349, 0.612], mean_best_reward: 128.500000\n",
      " 43655/100000: episode: 1152, duration: 0.017s, episode steps: 57, steps per second: 3338, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.068 [-1.128, 1.592], mean_best_reward: --\n",
      " 43737/100000: episode: 1153, duration: 0.024s, episode steps: 82, steps per second: 3463, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.048 [-0.768, 1.164], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 43878/100000: episode: 1154, duration: 0.050s, episode steps: 141, steps per second: 2838, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.226 [-1.492, 1.341], mean_best_reward: --\n",
      " 43904/100000: episode: 1155, duration: 0.009s, episode steps: 26, steps per second: 2922, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.046 [-2.015, 1.199], mean_best_reward: --\n",
      " 43963/100000: episode: 1156, duration: 0.018s, episode steps: 59, steps per second: 3256, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.044 [-0.609, 1.072], mean_best_reward: --\n",
      " 43990/100000: episode: 1157, duration: 0.010s, episode steps: 27, steps per second: 2737, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.080 [-0.555, 1.119], mean_best_reward: --\n",
      " 44054/100000: episode: 1158, duration: 0.020s, episode steps: 64, steps per second: 3133, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.143 [-0.716, 1.261], mean_best_reward: --\n",
      " 44074/100000: episode: 1159, duration: 0.007s, episode steps: 20, steps per second: 2820, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.118 [-0.543, 0.964], mean_best_reward: --\n",
      " 44097/100000: episode: 1160, duration: 0.008s, episode steps: 23, steps per second: 2891, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.052 [-0.944, 1.402], mean_best_reward: --\n",
      " 44136/100000: episode: 1161, duration: 0.013s, episode steps: 39, steps per second: 3005, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.009 [-1.229, 1.118], mean_best_reward: --\n",
      " 44159/100000: episode: 1162, duration: 0.008s, episode steps: 23, steps per second: 2975, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.091 [-1.586, 0.774], mean_best_reward: --\n",
      " 44175/100000: episode: 1163, duration: 0.006s, episode steps: 16, steps per second: 2754, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.095 [-1.709, 0.983], mean_best_reward: --\n",
      " 44230/100000: episode: 1164, duration: 0.018s, episode steps: 55, steps per second: 3114, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.206 [-1.013, 1.087], mean_best_reward: --\n",
      " 44282/100000: episode: 1165, duration: 0.016s, episode steps: 52, steps per second: 3260, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.076 [-1.030, 0.800], mean_best_reward: --\n",
      " 44302/100000: episode: 1166, duration: 0.007s, episode steps: 20, steps per second: 2970, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.094 [-0.771, 1.388], mean_best_reward: --\n",
      " 44375/100000: episode: 1167, duration: 0.020s, episode steps: 73, steps per second: 3636, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.144 [-1.826, 1.698], mean_best_reward: --\n",
      " 44392/100000: episode: 1168, duration: 0.005s, episode steps: 17, steps per second: 3134, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.074 [-0.996, 1.575], mean_best_reward: --\n",
      " 44406/100000: episode: 1169, duration: 0.005s, episode steps: 14, steps per second: 2643, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.086 [-1.214, 0.646], mean_best_reward: --\n",
      " 44423/100000: episode: 1170, duration: 0.006s, episode steps: 17, steps per second: 2718, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.098 [-0.783, 1.297], mean_best_reward: --\n",
      " 44445/100000: episode: 1171, duration: 0.008s, episode steps: 22, steps per second: 2883, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.254, 0.577], mean_best_reward: --\n",
      " 44491/100000: episode: 1172, duration: 0.015s, episode steps: 46, steps per second: 2989, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.120 [-0.537, 0.984], mean_best_reward: --\n",
      " 44512/100000: episode: 1173, duration: 0.010s, episode steps: 21, steps per second: 2040, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.046 [-0.798, 1.179], mean_best_reward: --\n",
      " 44654/100000: episode: 1174, duration: 0.046s, episode steps: 142, steps per second: 3116, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.267, 0.905], mean_best_reward: --\n",
      " 44714/100000: episode: 1175, duration: 0.018s, episode steps: 60, steps per second: 3395, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.013 [-0.710, 0.603], mean_best_reward: --\n",
      " 44743/100000: episode: 1176, duration: 0.009s, episode steps: 29, steps per second: 3184, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.091 [-0.603, 1.659], mean_best_reward: --\n",
      " 44781/100000: episode: 1177, duration: 0.012s, episode steps: 38, steps per second: 3279, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.129 [-0.594, 1.042], mean_best_reward: --\n",
      " 44847/100000: episode: 1178, duration: 0.020s, episode steps: 66, steps per second: 3328, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.023 [-2.248, 1.726], mean_best_reward: --\n",
      " 44863/100000: episode: 1179, duration: 0.005s, episode steps: 16, steps per second: 2934, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.089 [-1.241, 0.565], mean_best_reward: --\n",
      " 44890/100000: episode: 1180, duration: 0.008s, episode steps: 27, steps per second: 3206, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.121 [-0.589, 1.093], mean_best_reward: --\n",
      " 44912/100000: episode: 1181, duration: 0.007s, episode steps: 22, steps per second: 3176, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.070 [-1.184, 0.587], mean_best_reward: --\n",
      " 44968/100000: episode: 1182, duration: 0.016s, episode steps: 56, steps per second: 3441, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.025 [-1.622, 0.987], mean_best_reward: --\n",
      " 45056/100000: episode: 1183, duration: 0.026s, episode steps: 88, steps per second: 3336, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.051 [-0.992, 1.664], mean_best_reward: --\n",
      " 45102/100000: episode: 1184, duration: 0.019s, episode steps: 46, steps per second: 2397, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.025 [-1.142, 1.606], mean_best_reward: --\n",
      " 45128/100000: episode: 1185, duration: 0.009s, episode steps: 26, steps per second: 2805, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.811, 1.281], mean_best_reward: --\n",
      " 45240/100000: episode: 1186, duration: 0.071s, episode steps: 112, steps per second: 1589, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.082 [-1.279, 0.913], mean_best_reward: --\n",
      " 45251/100000: episode: 1187, duration: 0.008s, episode steps: 11, steps per second: 1351, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.117 [-0.979, 1.691], mean_best_reward: --\n",
      " 45323/100000: episode: 1188, duration: 0.049s, episode steps: 72, steps per second: 1458, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.057 [-0.853, 1.933], mean_best_reward: --\n",
      " 45363/100000: episode: 1189, duration: 0.029s, episode steps: 40, steps per second: 1372, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.017 [-0.971, 1.366], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45442/100000: episode: 1190, duration: 0.038s, episode steps: 79, steps per second: 2059, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.156 [-0.571, 1.415], mean_best_reward: --\n",
      " 45475/100000: episode: 1191, duration: 0.018s, episode steps: 33, steps per second: 1843, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.081 [-1.480, 0.812], mean_best_reward: --\n",
      " 45529/100000: episode: 1192, duration: 0.021s, episode steps: 54, steps per second: 2583, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.165 [-0.743, 1.418], mean_best_reward: --\n",
      " 45569/100000: episode: 1193, duration: 0.014s, episode steps: 40, steps per second: 2838, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.009 [-1.507, 1.185], mean_best_reward: --\n",
      " 45614/100000: episode: 1194, duration: 0.015s, episode steps: 45, steps per second: 2940, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.142 [-0.508, 0.955], mean_best_reward: --\n",
      " 45658/100000: episode: 1195, duration: 0.014s, episode steps: 44, steps per second: 3050, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.114 [-0.398, 0.750], mean_best_reward: --\n",
      " 45711/100000: episode: 1196, duration: 0.016s, episode steps: 53, steps per second: 3278, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.027 [-0.626, 0.866], mean_best_reward: --\n",
      " 45725/100000: episode: 1197, duration: 0.005s, episode steps: 14, steps per second: 2764, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.095 [-0.838, 1.506], mean_best_reward: --\n",
      " 45768/100000: episode: 1198, duration: 0.013s, episode steps: 43, steps per second: 3365, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.053 [-1.326, 1.374], mean_best_reward: --\n",
      " 45788/100000: episode: 1199, duration: 0.007s, episode steps: 20, steps per second: 3037, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.069 [-1.693, 0.976], mean_best_reward: --\n",
      " 45854/100000: episode: 1200, duration: 0.020s, episode steps: 66, steps per second: 3290, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.093 [-0.615, 1.265], mean_best_reward: --\n",
      " 45898/100000: episode: 1201, duration: 0.013s, episode steps: 44, steps per second: 3413, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.075 [-0.529, 1.032], mean_best_reward: 119.000000\n",
      " 45941/100000: episode: 1202, duration: 0.013s, episode steps: 43, steps per second: 3353, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.077 [-1.505, 1.803], mean_best_reward: --\n",
      " 46024/100000: episode: 1203, duration: 0.037s, episode steps: 83, steps per second: 2252, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.083 [-1.138, 1.347], mean_best_reward: --\n",
      " 46082/100000: episode: 1204, duration: 0.042s, episode steps: 58, steps per second: 1382, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-1.039, 1.914], mean_best_reward: --\n",
      " 46096/100000: episode: 1205, duration: 0.011s, episode steps: 14, steps per second: 1258, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.113 [-1.226, 0.781], mean_best_reward: --\n",
      " 46113/100000: episode: 1206, duration: 0.011s, episode steps: 17, steps per second: 1488, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.068 [-1.838, 1.203], mean_best_reward: --\n",
      " 46157/100000: episode: 1207, duration: 0.034s, episode steps: 44, steps per second: 1302, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.130 [-0.538, 0.915], mean_best_reward: --\n",
      " 46198/100000: episode: 1208, duration: 0.024s, episode steps: 41, steps per second: 1698, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.035 [-1.318, 1.813], mean_best_reward: --\n",
      " 46235/100000: episode: 1209, duration: 0.014s, episode steps: 37, steps per second: 2710, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.063 [-0.577, 1.209], mean_best_reward: --\n",
      " 46304/100000: episode: 1210, duration: 0.020s, episode steps: 69, steps per second: 3423, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.058 [-1.313, 1.546], mean_best_reward: --\n",
      " 46321/100000: episode: 1211, duration: 0.006s, episode steps: 17, steps per second: 2830, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.080 [-0.763, 1.203], mean_best_reward: --\n",
      " 46333/100000: episode: 1212, duration: 0.005s, episode steps: 12, steps per second: 2647, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-0.818, 1.313], mean_best_reward: --\n",
      " 46379/100000: episode: 1213, duration: 0.014s, episode steps: 46, steps per second: 3268, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.178 [-1.148, 0.550], mean_best_reward: --\n",
      " 46412/100000: episode: 1214, duration: 0.010s, episode steps: 33, steps per second: 3215, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.011 [-1.427, 0.962], mean_best_reward: --\n",
      " 46430/100000: episode: 1215, duration: 0.007s, episode steps: 18, steps per second: 2599, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.096 [-0.770, 1.517], mean_best_reward: --\n",
      " 46478/100000: episode: 1216, duration: 0.019s, episode steps: 48, steps per second: 2578, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.071 [-1.181, 1.154], mean_best_reward: --\n",
      " 46498/100000: episode: 1217, duration: 0.007s, episode steps: 20, steps per second: 2886, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.084 [-0.554, 1.128], mean_best_reward: --\n",
      " 46591/100000: episode: 1218, duration: 0.028s, episode steps: 93, steps per second: 3354, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.034 [-1.310, 1.181], mean_best_reward: --\n",
      " 46693/100000: episode: 1219, duration: 0.030s, episode steps: 102, steps per second: 3427, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.089 [-1.883, 1.400], mean_best_reward: --\n",
      " 46708/100000: episode: 1220, duration: 0.006s, episode steps: 15, steps per second: 2721, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.128 [-1.136, 2.063], mean_best_reward: --\n",
      " 46758/100000: episode: 1221, duration: 0.015s, episode steps: 50, steps per second: 3226, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.042 [-1.570, 1.325], mean_best_reward: --\n",
      " 46774/100000: episode: 1222, duration: 0.006s, episode steps: 16, steps per second: 2850, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.078 [-0.631, 1.127], mean_best_reward: --\n",
      " 46797/100000: episode: 1223, duration: 0.008s, episode steps: 23, steps per second: 3043, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: 0.074 [-0.613, 1.204], mean_best_reward: --\n",
      " 46827/100000: episode: 1224, duration: 0.009s, episode steps: 30, steps per second: 3232, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.032 [-0.757, 1.251], mean_best_reward: --\n",
      " 46843/100000: episode: 1225, duration: 0.005s, episode steps: 16, steps per second: 2967, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.095 [-1.153, 1.853], mean_best_reward: --\n",
      " 46876/100000: episode: 1226, duration: 0.010s, episode steps: 33, steps per second: 3302, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.076 [-0.638, 1.391], mean_best_reward: --\n",
      " 47011/100000: episode: 1227, duration: 0.043s, episode steps: 135, steps per second: 3136, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.258 [-1.412, 1.320], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47086/100000: episode: 1228, duration: 0.022s, episode steps: 75, steps per second: 3334, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.083 [-0.970, 1.343], mean_best_reward: --\n",
      " 47111/100000: episode: 1229, duration: 0.010s, episode steps: 25, steps per second: 2553, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.094 [-0.610, 1.196], mean_best_reward: --\n",
      " 47149/100000: episode: 1230, duration: 0.016s, episode steps: 38, steps per second: 2407, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.053 [-0.619, 1.131], mean_best_reward: --\n",
      " 47167/100000: episode: 1231, duration: 0.006s, episode steps: 18, steps per second: 2882, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.079 [-1.018, 1.821], mean_best_reward: --\n",
      " 47179/100000: episode: 1232, duration: 0.005s, episode steps: 12, steps per second: 2600, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.116 [-1.873, 1.150], mean_best_reward: --\n",
      " 47207/100000: episode: 1233, duration: 0.009s, episode steps: 28, steps per second: 3120, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.118 [-0.568, 1.102], mean_best_reward: --\n",
      " 47270/100000: episode: 1234, duration: 0.019s, episode steps: 63, steps per second: 3329, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.055 [-0.801, 1.198], mean_best_reward: --\n",
      " 47342/100000: episode: 1235, duration: 0.021s, episode steps: 72, steps per second: 3446, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.178 [-0.658, 1.472], mean_best_reward: --\n",
      " 47359/100000: episode: 1236, duration: 0.006s, episode steps: 17, steps per second: 2995, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.127 [-0.599, 1.493], mean_best_reward: --\n",
      " 47381/100000: episode: 1237, duration: 0.007s, episode steps: 22, steps per second: 3167, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.099 [-1.223, 0.581], mean_best_reward: --\n",
      " 47395/100000: episode: 1238, duration: 0.005s, episode steps: 14, steps per second: 2828, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.106 [-0.609, 1.180], mean_best_reward: --\n",
      " 47409/100000: episode: 1239, duration: 0.005s, episode steps: 14, steps per second: 3108, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.096 [-1.152, 0.743], mean_best_reward: --\n",
      " 47422/100000: episode: 1240, duration: 0.004s, episode steps: 13, steps per second: 2901, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.109 [-0.986, 1.560], mean_best_reward: --\n",
      " 47433/100000: episode: 1241, duration: 0.004s, episode steps: 11, steps per second: 2742, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.144 [-1.163, 1.927], mean_best_reward: --\n",
      " 47472/100000: episode: 1242, duration: 0.012s, episode steps: 39, steps per second: 3313, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.032 [-0.820, 1.616], mean_best_reward: --\n",
      " 47530/100000: episode: 1243, duration: 0.017s, episode steps: 58, steps per second: 3441, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.620, 0.622], mean_best_reward: --\n",
      " 47589/100000: episode: 1244, duration: 0.017s, episode steps: 59, steps per second: 3572, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.111 [-1.399, 0.880], mean_best_reward: --\n",
      " 47611/100000: episode: 1245, duration: 0.007s, episode steps: 22, steps per second: 2944, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.044 [-1.014, 1.515], mean_best_reward: --\n",
      " 47630/100000: episode: 1246, duration: 0.006s, episode steps: 19, steps per second: 3043, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.076 [-1.464, 0.994], mean_best_reward: --\n",
      " 47690/100000: episode: 1247, duration: 0.018s, episode steps: 60, steps per second: 3337, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.174 [-1.240, 0.818], mean_best_reward: --\n",
      " 47710/100000: episode: 1248, duration: 0.009s, episode steps: 20, steps per second: 2138, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.086 [-0.992, 1.829], mean_best_reward: --\n",
      " 47762/100000: episode: 1249, duration: 0.021s, episode steps: 52, steps per second: 2527, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.036 [-1.206, 1.666], mean_best_reward: --\n",
      " 47784/100000: episode: 1250, duration: 0.010s, episode steps: 22, steps per second: 2184, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.059 [-1.195, 0.797], mean_best_reward: --\n",
      " 47802/100000: episode: 1251, duration: 0.007s, episode steps: 18, steps per second: 2555, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.061 [-1.003, 1.505], mean_best_reward: 91.000000\n",
      " 47840/100000: episode: 1252, duration: 0.012s, episode steps: 38, steps per second: 3072, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-1.227, 0.500], mean_best_reward: --\n",
      " 47871/100000: episode: 1253, duration: 0.010s, episode steps: 31, steps per second: 3055, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.117 [-1.225, 0.760], mean_best_reward: --\n",
      " 47926/100000: episode: 1254, duration: 0.017s, episode steps: 55, steps per second: 3264, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.080 [-1.692, 1.709], mean_best_reward: --\n",
      " 47958/100000: episode: 1255, duration: 0.012s, episode steps: 32, steps per second: 2667, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.152 [-1.032, 0.594], mean_best_reward: --\n",
      " 47992/100000: episode: 1256, duration: 0.011s, episode steps: 34, steps per second: 3200, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.003 [-1.997, 1.378], mean_best_reward: --\n",
      " 48027/100000: episode: 1257, duration: 0.011s, episode steps: 35, steps per second: 3236, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.076 [-1.313, 0.762], mean_best_reward: --\n",
      " 48051/100000: episode: 1258, duration: 0.008s, episode steps: 24, steps per second: 3183, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.063 [-1.246, 0.789], mean_best_reward: --\n",
      " 48130/100000: episode: 1259, duration: 0.023s, episode steps: 79, steps per second: 3478, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.069 [-1.852, 1.532], mean_best_reward: --\n",
      " 48150/100000: episode: 1260, duration: 0.006s, episode steps: 20, steps per second: 3125, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.596, 1.166], mean_best_reward: --\n",
      " 48164/100000: episode: 1261, duration: 0.005s, episode steps: 14, steps per second: 2621, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.114 [-0.945, 1.767], mean_best_reward: --\n",
      " 48250/100000: episode: 1262, duration: 0.026s, episode steps: 86, steps per second: 3315, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.008 [-1.689, 1.117], mean_best_reward: --\n",
      " 48279/100000: episode: 1263, duration: 0.009s, episode steps: 29, steps per second: 3228, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.052 [-0.812, 1.143], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48312/100000: episode: 1264, duration: 0.011s, episode steps: 33, steps per second: 2924, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.012 [-1.375, 1.153], mean_best_reward: --\n",
      " 48381/100000: episode: 1265, duration: 0.024s, episode steps: 69, steps per second: 2866, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.301 [-1.501, 0.781], mean_best_reward: --\n",
      " 48503/100000: episode: 1266, duration: 0.035s, episode steps: 122, steps per second: 3470, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.066 [-1.537, 1.162], mean_best_reward: --\n",
      " 48545/100000: episode: 1267, duration: 0.013s, episode steps: 42, steps per second: 3182, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.643, 1.183], mean_best_reward: --\n",
      " 48580/100000: episode: 1268, duration: 0.011s, episode steps: 35, steps per second: 3234, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.013 [-0.996, 1.242], mean_best_reward: --\n",
      " 48638/100000: episode: 1269, duration: 0.017s, episode steps: 58, steps per second: 3371, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.021 [-0.645, 1.000], mean_best_reward: --\n",
      " 48661/100000: episode: 1270, duration: 0.007s, episode steps: 23, steps per second: 3109, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.078 [-1.000, 0.574], mean_best_reward: --\n",
      " 48681/100000: episode: 1271, duration: 0.007s, episode steps: 20, steps per second: 3038, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.069 [-0.823, 1.323], mean_best_reward: --\n",
      " 48751/100000: episode: 1272, duration: 0.021s, episode steps: 70, steps per second: 3399, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.124 [-0.813, 1.258], mean_best_reward: --\n",
      " 48771/100000: episode: 1273, duration: 0.007s, episode steps: 20, steps per second: 2914, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.569, 0.935], mean_best_reward: --\n",
      " 48795/100000: episode: 1274, duration: 0.008s, episode steps: 24, steps per second: 3109, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.063 [-0.792, 1.334], mean_best_reward: --\n",
      " 48822/100000: episode: 1275, duration: 0.009s, episode steps: 27, steps per second: 3026, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.006 [-1.502, 1.133], mean_best_reward: --\n",
      " 48889/100000: episode: 1276, duration: 0.020s, episode steps: 67, steps per second: 3371, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.073 [-1.683, 1.097], mean_best_reward: --\n",
      " 48931/100000: episode: 1277, duration: 0.013s, episode steps: 42, steps per second: 3267, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.096 [-0.435, 0.951], mean_best_reward: --\n",
      " 48984/100000: episode: 1278, duration: 0.020s, episode steps: 53, steps per second: 2650, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.042 [-0.961, 1.200], mean_best_reward: --\n",
      " 49032/100000: episode: 1279, duration: 0.017s, episode steps: 48, steps per second: 2791, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.228, 0.576], mean_best_reward: --\n",
      " 49049/100000: episode: 1280, duration: 0.006s, episode steps: 17, steps per second: 2848, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.069 [-0.830, 1.293], mean_best_reward: --\n",
      " 49074/100000: episode: 1281, duration: 0.007s, episode steps: 25, steps per second: 3502, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.017 [-1.763, 2.410], mean_best_reward: --\n",
      " 49097/100000: episode: 1282, duration: 0.007s, episode steps: 23, steps per second: 3193, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.091 [-1.420, 0.629], mean_best_reward: --\n",
      " 49143/100000: episode: 1283, duration: 0.014s, episode steps: 46, steps per second: 3310, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.118 [-1.001, 1.009], mean_best_reward: --\n",
      " 49179/100000: episode: 1284, duration: 0.012s, episode steps: 36, steps per second: 3031, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.722, 1.087], mean_best_reward: --\n",
      " 49201/100000: episode: 1285, duration: 0.008s, episode steps: 22, steps per second: 2882, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.100 [-0.636, 1.120], mean_best_reward: --\n",
      " 49240/100000: episode: 1286, duration: 0.012s, episode steps: 39, steps per second: 3372, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.131 [-0.385, 0.885], mean_best_reward: --\n",
      " 49251/100000: episode: 1287, duration: 0.004s, episode steps: 11, steps per second: 2597, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.115 [-1.187, 1.816], mean_best_reward: --\n",
      " 49275/100000: episode: 1288, duration: 0.009s, episode steps: 24, steps per second: 2530, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.575, 1.137], mean_best_reward: --\n",
      " 49388/100000: episode: 1289, duration: 0.034s, episode steps: 113, steps per second: 3308, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.147 [-1.692, 1.052], mean_best_reward: --\n",
      " 49435/100000: episode: 1290, duration: 0.014s, episode steps: 47, steps per second: 3363, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.002 [-1.036, 1.793], mean_best_reward: --\n",
      " 49449/100000: episode: 1291, duration: 0.005s, episode steps: 14, steps per second: 2854, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-1.021, 0.568], mean_best_reward: --\n",
      " 49502/100000: episode: 1292, duration: 0.016s, episode steps: 53, steps per second: 3387, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.201 [-0.665, 1.577], mean_best_reward: --\n",
      " 49520/100000: episode: 1293, duration: 0.006s, episode steps: 18, steps per second: 3007, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-1.324, 0.827], mean_best_reward: --\n",
      " 49556/100000: episode: 1294, duration: 0.011s, episode steps: 36, steps per second: 3297, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.042 [-1.687, 0.768], mean_best_reward: --\n",
      " 49601/100000: episode: 1295, duration: 0.014s, episode steps: 45, steps per second: 3319, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.120 [-0.954, 0.697], mean_best_reward: --\n",
      " 49637/100000: episode: 1296, duration: 0.013s, episode steps: 36, steps per second: 2706, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.074 [-1.115, 0.582], mean_best_reward: --\n",
      " 49666/100000: episode: 1297, duration: 0.013s, episode steps: 29, steps per second: 2307, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.053 [-0.451, 1.008], mean_best_reward: --\n",
      " 49684/100000: episode: 1298, duration: 0.006s, episode steps: 18, steps per second: 3118, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.777, 1.193], mean_best_reward: --\n",
      " 49731/100000: episode: 1299, duration: 0.015s, episode steps: 47, steps per second: 3200, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.031 [-0.820, 1.116], mean_best_reward: --\n",
      " 49814/100000: episode: 1300, duration: 0.025s, episode steps: 83, steps per second: 3286, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.169 [-1.168, 1.709], mean_best_reward: --\n",
      " 49858/100000: episode: 1301, duration: 0.014s, episode steps: 44, steps per second: 3152, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.105 [-1.064, 0.553], mean_best_reward: 131.000000\n",
      " 49888/100000: episode: 1302, duration: 0.010s, episode steps: 30, steps per second: 3095, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.026 [-2.096, 1.357], mean_best_reward: --\n",
      " 49902/100000: episode: 1303, duration: 0.005s, episode steps: 14, steps per second: 2952, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.099 [-1.259, 0.800], mean_best_reward: --\n",
      " 49968/100000: episode: 1304, duration: 0.019s, episode steps: 66, steps per second: 3521, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.086 [-1.215, 1.140], mean_best_reward: --\n",
      " 49990/100000: episode: 1305, duration: 0.007s, episode steps: 22, steps per second: 3137, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.081 [-0.801, 1.498], mean_best_reward: --\n",
      " 50005/100000: episode: 1306, duration: 0.005s, episode steps: 15, steps per second: 2928, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.109 [-0.758, 1.373], mean_best_reward: --\n",
      " 50073/100000: episode: 1307, duration: 0.020s, episode steps: 68, steps per second: 3372, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-1.109, 1.492], mean_best_reward: --\n",
      " 50140/100000: episode: 1308, duration: 0.020s, episode steps: 67, steps per second: 3390, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.232 [-0.776, 1.345], mean_best_reward: --\n",
      " 50167/100000: episode: 1309, duration: 0.008s, episode steps: 27, steps per second: 3221, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.040 [-1.564, 0.987], mean_best_reward: --\n",
      " 50183/100000: episode: 1310, duration: 0.005s, episode steps: 16, steps per second: 2991, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.120 [-0.771, 1.613], mean_best_reward: --\n",
      " 50218/100000: episode: 1311, duration: 0.011s, episode steps: 35, steps per second: 3307, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.101 [-0.992, 0.440], mean_best_reward: --\n",
      " 50257/100000: episode: 1312, duration: 0.012s, episode steps: 39, steps per second: 3302, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.010 [-0.928, 1.446], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50390/100000: episode: 1313, duration: 0.046s, episode steps: 133, steps per second: 2920, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.300 [-1.847, 0.846], mean_best_reward: --\n",
      " 50414/100000: episode: 1314, duration: 0.009s, episode steps: 24, steps per second: 2811, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.046 [-1.297, 0.820], mean_best_reward: --\n",
      " 50471/100000: episode: 1315, duration: 0.017s, episode steps: 57, steps per second: 3303, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: 0.034 [-1.243, 1.931], mean_best_reward: --\n",
      " 50532/100000: episode: 1316, duration: 0.018s, episode steps: 61, steps per second: 3404, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.099 [-1.516, 1.677], mean_best_reward: --\n",
      " 50544/100000: episode: 1317, duration: 0.004s, episode steps: 12, steps per second: 2751, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.113 [-1.674, 0.960], mean_best_reward: --\n",
      " 50590/100000: episode: 1318, duration: 0.014s, episode steps: 46, steps per second: 3343, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.089 [-1.291, 1.269], mean_best_reward: --\n",
      " 50653/100000: episode: 1319, duration: 0.019s, episode steps: 63, steps per second: 3266, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.156 [-1.254, 1.836], mean_best_reward: --\n",
      " 50698/100000: episode: 1320, duration: 0.014s, episode steps: 45, steps per second: 3302, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.003 [-0.970, 1.715], mean_best_reward: --\n",
      " 50789/100000: episode: 1321, duration: 0.026s, episode steps: 91, steps per second: 3436, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.124 [-1.043, 1.460], mean_best_reward: --\n",
      " 50839/100000: episode: 1322, duration: 0.015s, episode steps: 50, steps per second: 3282, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.153 [-1.087, 0.601], mean_best_reward: --\n",
      " 50893/100000: episode: 1323, duration: 0.016s, episode steps: 54, steps per second: 3376, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-1.101, 0.992], mean_best_reward: --\n",
      " 50926/100000: episode: 1324, duration: 0.012s, episode steps: 33, steps per second: 2771, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.071 [-0.679, 1.170], mean_best_reward: --\n",
      " 50979/100000: episode: 1325, duration: 0.019s, episode steps: 53, steps per second: 2784, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.089 [-0.910, 1.084], mean_best_reward: --\n",
      " 51035/100000: episode: 1326, duration: 0.018s, episode steps: 56, steps per second: 3189, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.004 [-1.318, 1.198], mean_best_reward: --\n",
      " 51108/100000: episode: 1327, duration: 0.026s, episode steps: 73, steps per second: 2816, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.081 [-1.555, 1.445], mean_best_reward: --\n",
      " 51141/100000: episode: 1328, duration: 0.011s, episode steps: 33, steps per second: 3116, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.083 [-1.645, 0.672], mean_best_reward: --\n",
      " 51189/100000: episode: 1329, duration: 0.014s, episode steps: 48, steps per second: 3319, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.067 [-0.961, 0.602], mean_best_reward: --\n",
      " 51223/100000: episode: 1330, duration: 0.011s, episode steps: 34, steps per second: 3230, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.091 [-0.914, 1.603], mean_best_reward: --\n",
      " 51248/100000: episode: 1331, duration: 0.009s, episode steps: 25, steps per second: 2709, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.083 [-1.521, 0.781], mean_best_reward: --\n",
      " 51292/100000: episode: 1332, duration: 0.015s, episode steps: 44, steps per second: 2880, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.058 [-1.516, 1.352], mean_best_reward: --\n",
      " 51315/100000: episode: 1333, duration: 0.009s, episode steps: 23, steps per second: 2681, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.033 [-0.820, 1.347], mean_best_reward: --\n",
      " 51340/100000: episode: 1334, duration: 0.010s, episode steps: 25, steps per second: 2627, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.056 [-0.828, 1.475], mean_best_reward: --\n",
      " 51368/100000: episode: 1335, duration: 0.010s, episode steps: 28, steps per second: 2677, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-1.476, 0.805], mean_best_reward: --\n",
      " 51404/100000: episode: 1336, duration: 0.012s, episode steps: 36, steps per second: 3048, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.037 [-1.803, 1.185], mean_best_reward: --\n",
      " 51425/100000: episode: 1337, duration: 0.007s, episode steps: 21, steps per second: 2849, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.053 [-1.857, 1.187], mean_best_reward: --\n",
      " 51440/100000: episode: 1338, duration: 0.006s, episode steps: 15, steps per second: 2668, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.082 [-1.470, 1.008], mean_best_reward: --\n",
      " 51456/100000: episode: 1339, duration: 0.006s, episode steps: 16, steps per second: 2680, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.110 [-0.751, 1.623], mean_best_reward: --\n",
      " 51562/100000: episode: 1340, duration: 0.032s, episode steps: 106, steps per second: 3275, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.021 [-1.731, 1.134], mean_best_reward: --\n",
      " 51593/100000: episode: 1341, duration: 0.010s, episode steps: 31, steps per second: 3076, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.081 [-1.138, 0.744], mean_best_reward: --\n",
      " 51613/100000: episode: 1342, duration: 0.007s, episode steps: 20, steps per second: 2913, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.070 [-0.991, 1.578], mean_best_reward: --\n",
      " 51634/100000: episode: 1343, duration: 0.010s, episode steps: 21, steps per second: 2143, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.037 [-1.548, 1.016], mean_best_reward: --\n",
      " 51665/100000: episode: 1344, duration: 0.013s, episode steps: 31, steps per second: 2457, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.025 [-1.246, 0.931], mean_best_reward: --\n",
      " 51685/100000: episode: 1345, duration: 0.007s, episode steps: 20, steps per second: 2965, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-1.140, 0.449], mean_best_reward: --\n",
      " 51746/100000: episode: 1346, duration: 0.023s, episode steps: 61, steps per second: 2706, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.590 [0.000, 1.000], mean observation: 0.226 [-1.783, 2.278], mean_best_reward: --\n",
      " 51778/100000: episode: 1347, duration: 0.011s, episode steps: 32, steps per second: 3012, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.118 [-0.361, 1.032], mean_best_reward: --\n",
      " 51816/100000: episode: 1348, duration: 0.013s, episode steps: 38, steps per second: 3035, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.158 [-1.165, 1.160], mean_best_reward: --\n",
      " 51879/100000: episode: 1349, duration: 0.020s, episode steps: 63, steps per second: 3195, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.048 [-1.451, 1.109], mean_best_reward: --\n",
      " 51901/100000: episode: 1350, duration: 0.007s, episode steps: 22, steps per second: 2964, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.981, 1.472], mean_best_reward: --\n",
      " 51917/100000: episode: 1351, duration: 0.006s, episode steps: 16, steps per second: 2682, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.070 [-1.581, 0.994], mean_best_reward: 165.500000\n",
      " 51941/100000: episode: 1352, duration: 0.008s, episode steps: 24, steps per second: 2907, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.090 [-0.594, 1.250], mean_best_reward: --\n",
      " 51953/100000: episode: 1353, duration: 0.005s, episode steps: 12, steps per second: 2512, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.107 [-1.691, 1.026], mean_best_reward: --\n",
      " 51999/100000: episode: 1354, duration: 0.014s, episode steps: 46, steps per second: 3215, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.117 [-0.953, 0.586], mean_best_reward: --\n",
      " 52072/100000: episode: 1355, duration: 0.022s, episode steps: 73, steps per second: 3339, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.199 [-1.130, 1.110], mean_best_reward: --\n",
      " 52136/100000: episode: 1356, duration: 0.020s, episode steps: 64, steps per second: 3263, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.169 [-1.006, 1.407], mean_best_reward: --\n",
      " 52189/100000: episode: 1357, duration: 0.015s, episode steps: 53, steps per second: 3442, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.055 [-0.705, 1.459], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52253/100000: episode: 1358, duration: 0.020s, episode steps: 64, steps per second: 3191, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.207 [-0.839, 1.264], mean_best_reward: --\n",
      " 52345/100000: episode: 1359, duration: 0.036s, episode steps: 92, steps per second: 2591, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.179 [-0.877, 1.021], mean_best_reward: --\n",
      " 52357/100000: episode: 1360, duration: 0.006s, episode steps: 12, steps per second: 1985, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.787, 1.196], mean_best_reward: --\n",
      " 52396/100000: episode: 1361, duration: 0.015s, episode steps: 39, steps per second: 2570, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.070 [-1.101, 1.482], mean_best_reward: --\n",
      " 52493/100000: episode: 1362, duration: 0.033s, episode steps: 97, steps per second: 2912, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.454 [0.000, 1.000], mean observation: -0.372 [-2.073, 0.906], mean_best_reward: --\n",
      " 52530/100000: episode: 1363, duration: 0.012s, episode steps: 37, steps per second: 3122, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: 0.014 [-0.822, 1.260], mean_best_reward: --\n",
      " 52608/100000: episode: 1364, duration: 0.025s, episode steps: 78, steps per second: 3178, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.991, 1.317], mean_best_reward: --\n",
      " 52627/100000: episode: 1365, duration: 0.007s, episode steps: 19, steps per second: 2819, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.086 [-0.637, 1.119], mean_best_reward: --\n",
      " 52645/100000: episode: 1366, duration: 0.006s, episode steps: 18, steps per second: 2811, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.547, 1.084], mean_best_reward: --\n",
      " 52695/100000: episode: 1367, duration: 0.016s, episode steps: 50, steps per second: 3215, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.132 [-1.169, 0.808], mean_best_reward: --\n",
      " 52707/100000: episode: 1368, duration: 0.005s, episode steps: 12, steps per second: 2606, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.093 [-0.818, 1.335], mean_best_reward: --\n",
      " 52737/100000: episode: 1369, duration: 0.009s, episode steps: 30, steps per second: 3290, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.124 [-0.740, 1.237], mean_best_reward: --\n",
      " 52760/100000: episode: 1370, duration: 0.007s, episode steps: 23, steps per second: 3117, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.085 [-0.879, 1.196], mean_best_reward: --\n",
      " 52816/100000: episode: 1371, duration: 0.017s, episode steps: 56, steps per second: 3302, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-1.144, 1.259], mean_best_reward: --\n",
      " 52852/100000: episode: 1372, duration: 0.014s, episode steps: 36, steps per second: 2581, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.523, 1.026], mean_best_reward: --\n",
      " 52899/100000: episode: 1373, duration: 0.016s, episode steps: 47, steps per second: 2921, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.042 [-1.204, 1.472], mean_best_reward: --\n",
      " 53024/100000: episode: 1374, duration: 0.037s, episode steps: 125, steps per second: 3389, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.029 [-0.954, 1.019], mean_best_reward: --\n",
      " 53062/100000: episode: 1375, duration: 0.011s, episode steps: 38, steps per second: 3332, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.151 [-0.661, 1.466], mean_best_reward: --\n",
      " 53133/100000: episode: 1376, duration: 0.020s, episode steps: 71, steps per second: 3570, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.065 [-1.622, 1.437], mean_best_reward: --\n",
      " 53172/100000: episode: 1377, duration: 0.012s, episode steps: 39, steps per second: 3129, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.044 [-1.335, 0.955], mean_best_reward: --\n",
      " 53197/100000: episode: 1378, duration: 0.008s, episode steps: 25, steps per second: 3012, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.061 [-0.827, 1.406], mean_best_reward: --\n",
      " 53225/100000: episode: 1379, duration: 0.009s, episode steps: 28, steps per second: 3007, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.066 [-1.353, 0.641], mean_best_reward: --\n",
      " 53247/100000: episode: 1380, duration: 0.007s, episode steps: 22, steps per second: 3089, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.799, 1.163], mean_best_reward: --\n",
      " 53290/100000: episode: 1381, duration: 0.013s, episode steps: 43, steps per second: 3225, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.558 [0.000, 1.000], mean observation: 0.147 [-0.623, 0.885], mean_best_reward: --\n",
      " 53324/100000: episode: 1382, duration: 0.011s, episode steps: 34, steps per second: 3185, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.021 [-0.977, 1.407], mean_best_reward: --\n",
      " 53431/100000: episode: 1383, duration: 0.031s, episode steps: 107, steps per second: 3440, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.001 [-0.940, 1.617], mean_best_reward: --\n",
      " 53501/100000: episode: 1384, duration: 0.021s, episode steps: 70, steps per second: 3330, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.732, 1.437], mean_best_reward: --\n",
      " 53568/100000: episode: 1385, duration: 0.027s, episode steps: 67, steps per second: 2525, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.134 [-1.451, 1.691], mean_best_reward: --\n",
      " 53681/100000: episode: 1386, duration: 0.034s, episode steps: 113, steps per second: 3347, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.272 [-1.869, 1.051], mean_best_reward: --\n",
      " 53699/100000: episode: 1387, duration: 0.006s, episode steps: 18, steps per second: 2935, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.576, 1.024], mean_best_reward: --\n",
      " 53771/100000: episode: 1388, duration: 0.021s, episode steps: 72, steps per second: 3389, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.105 [-1.334, 0.785], mean_best_reward: --\n",
      " 53801/100000: episode: 1389, duration: 0.009s, episode steps: 30, steps per second: 3170, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.120 [-1.271, 0.579], mean_best_reward: --\n",
      " 53831/100000: episode: 1390, duration: 0.009s, episode steps: 30, steps per second: 3186, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.057 [-0.766, 1.145], mean_best_reward: --\n",
      " 53855/100000: episode: 1391, duration: 0.008s, episode steps: 24, steps per second: 2828, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.407, 0.827], mean_best_reward: --\n",
      " 53867/100000: episode: 1392, duration: 0.005s, episode steps: 12, steps per second: 2595, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.082 [-0.836, 1.372], mean_best_reward: --\n",
      " 53889/100000: episode: 1393, duration: 0.007s, episode steps: 22, steps per second: 2994, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.069 [-0.839, 1.738], mean_best_reward: --\n",
      " 53954/100000: episode: 1394, duration: 0.019s, episode steps: 65, steps per second: 3419, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.132 [-1.290, 0.923], mean_best_reward: --\n",
      " 53971/100000: episode: 1395, duration: 0.006s, episode steps: 17, steps per second: 2933, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.089 [-0.806, 1.442], mean_best_reward: --\n",
      " 54025/100000: episode: 1396, duration: 0.016s, episode steps: 54, steps per second: 3407, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.117 [-1.869, 0.975], mean_best_reward: --\n",
      " 54036/100000: episode: 1397, duration: 0.004s, episode steps: 11, steps per second: 2677, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.121 [-0.958, 1.611], mean_best_reward: --\n",
      " 54066/100000: episode: 1398, duration: 0.011s, episode steps: 30, steps per second: 2692, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.424, 1.170], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54095/100000: episode: 1399, duration: 0.013s, episode steps: 29, steps per second: 2312, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.081 [-0.536, 1.220], mean_best_reward: --\n",
      " 54137/100000: episode: 1400, duration: 0.016s, episode steps: 42, steps per second: 2703, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.006 [-0.597, 1.007], mean_best_reward: --\n",
      " 54174/100000: episode: 1401, duration: 0.016s, episode steps: 37, steps per second: 2303, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.118 [-0.620, 1.591], mean_best_reward: 131.000000\n",
      " 54190/100000: episode: 1402, duration: 0.007s, episode steps: 16, steps per second: 2440, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.090 [-1.415, 0.936], mean_best_reward: --\n",
      " 54217/100000: episode: 1403, duration: 0.010s, episode steps: 27, steps per second: 2782, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.100 [-0.411, 0.951], mean_best_reward: --\n",
      " 54262/100000: episode: 1404, duration: 0.014s, episode steps: 45, steps per second: 3245, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.179 [-0.954, 0.495], mean_best_reward: --\n",
      " 54282/100000: episode: 1405, duration: 0.007s, episode steps: 20, steps per second: 2991, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.079 [-1.010, 1.811], mean_best_reward: --\n",
      " 54301/100000: episode: 1406, duration: 0.007s, episode steps: 19, steps per second: 2862, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.037 [-1.195, 1.638], mean_best_reward: --\n",
      " 54330/100000: episode: 1407, duration: 0.009s, episode steps: 29, steps per second: 3159, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.128 [-1.404, 0.748], mean_best_reward: --\n",
      " 54383/100000: episode: 1408, duration: 0.016s, episode steps: 53, steps per second: 3297, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.130 [-1.276, 0.600], mean_best_reward: --\n",
      " 54438/100000: episode: 1409, duration: 0.016s, episode steps: 55, steps per second: 3343, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.005 [-0.989, 1.455], mean_best_reward: --\n",
      " 54513/100000: episode: 1410, duration: 0.022s, episode steps: 75, steps per second: 3374, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.427 [0.000, 1.000], mean observation: -0.176 [-2.111, 2.023], mean_best_reward: --\n",
      " 54604/100000: episode: 1411, duration: 0.027s, episode steps: 91, steps per second: 3417, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.016 [-0.957, 0.944], mean_best_reward: --\n",
      " 54680/100000: episode: 1412, duration: 0.023s, episode steps: 76, steps per second: 3372, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.147 [-0.993, 1.475], mean_best_reward: --\n",
      " 54803/100000: episode: 1413, duration: 0.042s, episode steps: 123, steps per second: 2913, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.245 [-1.019, 1.450], mean_best_reward: --\n",
      " 54839/100000: episode: 1414, duration: 0.014s, episode steps: 36, steps per second: 2546, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.045 [-1.175, 1.596], mean_best_reward: --\n",
      " 54886/100000: episode: 1415, duration: 0.015s, episode steps: 47, steps per second: 3160, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.049 [-0.944, 1.649], mean_best_reward: --\n",
      " 54956/100000: episode: 1416, duration: 0.021s, episode steps: 70, steps per second: 3394, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.148 [-1.319, 1.143], mean_best_reward: --\n",
      " 55037/100000: episode: 1417, duration: 0.023s, episode steps: 81, steps per second: 3453, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.225 [-1.504, 0.741], mean_best_reward: --\n",
      " 55057/100000: episode: 1418, duration: 0.007s, episode steps: 20, steps per second: 3050, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.385, 0.879], mean_best_reward: --\n",
      " 55148/100000: episode: 1419, duration: 0.027s, episode steps: 91, steps per second: 3414, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.037 [-2.080, 1.795], mean_best_reward: --\n",
      " 55161/100000: episode: 1420, duration: 0.004s, episode steps: 13, steps per second: 3113, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.119 [-0.602, 1.193], mean_best_reward: --\n",
      " 55194/100000: episode: 1421, duration: 0.010s, episode steps: 33, steps per second: 3248, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.110 [-0.403, 1.145], mean_best_reward: --\n",
      " 55278/100000: episode: 1422, duration: 0.025s, episode steps: 84, steps per second: 3390, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.336 [-0.859, 1.676], mean_best_reward: --\n",
      " 55358/100000: episode: 1423, duration: 0.023s, episode steps: 80, steps per second: 3485, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.067 [-1.014, 1.172], mean_best_reward: --\n",
      " 55368/100000: episode: 1424, duration: 0.004s, episode steps: 10, steps per second: 2624, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.202, 2.020], mean_best_reward: --\n",
      " 55404/100000: episode: 1425, duration: 0.012s, episode steps: 36, steps per second: 2964, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.102 [-1.802, 0.814], mean_best_reward: --\n",
      " 55433/100000: episode: 1426, duration: 0.009s, episode steps: 29, steps per second: 3185, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.115 [-0.439, 0.823], mean_best_reward: --\n",
      " 55453/100000: episode: 1427, duration: 0.007s, episode steps: 20, steps per second: 2789, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.080 [-0.636, 1.160], mean_best_reward: --\n",
      " 55465/100000: episode: 1428, duration: 0.004s, episode steps: 12, steps per second: 2669, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-1.029, 1.577], mean_best_reward: --\n",
      " 55489/100000: episode: 1429, duration: 0.010s, episode steps: 24, steps per second: 2369, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.409, 1.095], mean_best_reward: --\n",
      " 55506/100000: episode: 1430, duration: 0.009s, episode steps: 17, steps per second: 1825, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.103 [-1.230, 0.603], mean_best_reward: --\n",
      " 55526/100000: episode: 1431, duration: 0.007s, episode steps: 20, steps per second: 2799, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.107 [-1.415, 0.797], mean_best_reward: --\n",
      " 55558/100000: episode: 1432, duration: 0.011s, episode steps: 32, steps per second: 2983, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.153 [-1.090, 0.556], mean_best_reward: --\n",
      " 55582/100000: episode: 1433, duration: 0.007s, episode steps: 24, steps per second: 3349, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.065 [-0.555, 0.977], mean_best_reward: --\n",
      " 55612/100000: episode: 1434, duration: 0.009s, episode steps: 30, steps per second: 3223, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.037 [-0.978, 1.580], mean_best_reward: --\n",
      " 55674/100000: episode: 1435, duration: 0.020s, episode steps: 62, steps per second: 3145, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.027 [-0.882, 1.128], mean_best_reward: --\n",
      " 55712/100000: episode: 1436, duration: 0.015s, episode steps: 38, steps per second: 2531, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.013 [-1.130, 0.804], mean_best_reward: --\n",
      " 55728/100000: episode: 1437, duration: 0.006s, episode steps: 16, steps per second: 2892, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.593, 1.049], mean_best_reward: --\n",
      " 55785/100000: episode: 1438, duration: 0.017s, episode steps: 57, steps per second: 3345, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.021 [-0.953, 1.113], mean_best_reward: --\n",
      " 55906/100000: episode: 1439, duration: 0.034s, episode steps: 121, steps per second: 3538, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.471 [-1.316, 2.761], mean_best_reward: --\n",
      " 55949/100000: episode: 1440, duration: 0.015s, episode steps: 43, steps per second: 2785, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.054 [-1.438, 0.813], mean_best_reward: --\n",
      " 55970/100000: episode: 1441, duration: 0.007s, episode steps: 21, steps per second: 2866, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.090 [-1.124, 0.452], mean_best_reward: --\n",
      " 55983/100000: episode: 1442, duration: 0.005s, episode steps: 13, steps per second: 2790, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.087 [-1.591, 1.004], mean_best_reward: --\n",
      " 56019/100000: episode: 1443, duration: 0.011s, episode steps: 36, steps per second: 3253, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.063 [-0.746, 1.351], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56085/100000: episode: 1444, duration: 0.021s, episode steps: 66, steps per second: 3164, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.133 [-1.149, 2.261], mean_best_reward: --\n",
      " 56101/100000: episode: 1445, duration: 0.009s, episode steps: 16, steps per second: 1851, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.086 [-1.272, 0.779], mean_best_reward: --\n",
      " 56127/100000: episode: 1446, duration: 0.009s, episode steps: 26, steps per second: 2985, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.039 [-0.954, 1.311], mean_best_reward: --\n",
      " 56197/100000: episode: 1447, duration: 0.022s, episode steps: 70, steps per second: 3248, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-1.113, 1.344], mean_best_reward: --\n",
      " 56240/100000: episode: 1448, duration: 0.016s, episode steps: 43, steps per second: 2718, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.016 [-0.963, 1.482], mean_best_reward: --\n",
      " 56273/100000: episode: 1449, duration: 0.011s, episode steps: 33, steps per second: 3060, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: -0.067 [-1.694, 1.807], mean_best_reward: --\n",
      " 56298/100000: episode: 1450, duration: 0.008s, episode steps: 25, steps per second: 3114, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.052 [-1.553, 2.418], mean_best_reward: --\n",
      " 56367/100000: episode: 1451, duration: 0.020s, episode steps: 69, steps per second: 3418, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.080 [-0.900, 1.139], mean_best_reward: 125.000000\n",
      " 56413/100000: episode: 1452, duration: 0.014s, episode steps: 46, steps per second: 3290, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.037 [-0.991, 1.773], mean_best_reward: --\n",
      " 56440/100000: episode: 1453, duration: 0.008s, episode steps: 27, steps per second: 3185, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.096 [-0.621, 1.010], mean_best_reward: --\n",
      " 56479/100000: episode: 1454, duration: 0.012s, episode steps: 39, steps per second: 3198, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.065 [-0.934, 0.435], mean_best_reward: --\n",
      " 56494/100000: episode: 1455, duration: 0.005s, episode steps: 15, steps per second: 3242, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.108 [-0.992, 0.621], mean_best_reward: --\n",
      " 56521/100000: episode: 1456, duration: 0.009s, episode steps: 27, steps per second: 3104, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.123 [-1.174, 0.780], mean_best_reward: --\n",
      " 56557/100000: episode: 1457, duration: 0.011s, episode steps: 36, steps per second: 3221, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-1.117, 0.761], mean_best_reward: --\n",
      " 56599/100000: episode: 1458, duration: 0.012s, episode steps: 42, steps per second: 3412, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.151 [-1.112, 0.511], mean_best_reward: --\n",
      " 56612/100000: episode: 1459, duration: 0.005s, episode steps: 13, steps per second: 2881, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.105 [-1.186, 1.895], mean_best_reward: --\n",
      " 56627/100000: episode: 1460, duration: 0.005s, episode steps: 15, steps per second: 2915, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.114 [-0.777, 1.375], mean_best_reward: --\n",
      " 56651/100000: episode: 1461, duration: 0.007s, episode steps: 24, steps per second: 3488, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.082 [-1.457, 0.809], mean_best_reward: --\n",
      " 56684/100000: episode: 1462, duration: 0.010s, episode steps: 33, steps per second: 3290, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.037 [-0.828, 1.172], mean_best_reward: --\n",
      " 56745/100000: episode: 1463, duration: 0.026s, episode steps: 61, steps per second: 2320, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.008 [-0.951, 1.023], mean_best_reward: --\n",
      " 56788/100000: episode: 1464, duration: 0.014s, episode steps: 43, steps per second: 3054, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: -0.080 [-1.832, 0.805], mean_best_reward: --\n",
      " 56834/100000: episode: 1465, duration: 0.014s, episode steps: 46, steps per second: 3251, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.016 [-0.643, 1.080], mean_best_reward: --\n",
      " 56851/100000: episode: 1466, duration: 0.006s, episode steps: 17, steps per second: 2744, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.085 [-0.769, 1.270], mean_best_reward: --\n",
      " 56869/100000: episode: 1467, duration: 0.006s, episode steps: 18, steps per second: 2815, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.551, 1.128], mean_best_reward: --\n",
      " 56930/100000: episode: 1468, duration: 0.018s, episode steps: 61, steps per second: 3331, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.045 [-1.310, 0.593], mean_best_reward: --\n",
      " 56959/100000: episode: 1469, duration: 0.009s, episode steps: 29, steps per second: 3125, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.082 [-1.248, 0.803], mean_best_reward: --\n",
      " 57016/100000: episode: 1470, duration: 0.017s, episode steps: 57, steps per second: 3381, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.544 [0.000, 1.000], mean observation: 0.074 [-0.630, 0.946], mean_best_reward: --\n",
      " 57049/100000: episode: 1471, duration: 0.010s, episode steps: 33, steps per second: 3292, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.014 [-1.307, 1.787], mean_best_reward: --\n",
      " 57088/100000: episode: 1472, duration: 0.012s, episode steps: 39, steps per second: 3265, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: -0.053 [-1.901, 1.992], mean_best_reward: --\n",
      " 57172/100000: episode: 1473, duration: 0.025s, episode steps: 84, steps per second: 3342, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.049 [-1.368, 1.328], mean_best_reward: --\n",
      " 57203/100000: episode: 1474, duration: 0.011s, episode steps: 31, steps per second: 2778, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.063 [-0.786, 1.570], mean_best_reward: --\n",
      " 57255/100000: episode: 1475, duration: 0.022s, episode steps: 52, steps per second: 2376, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.163 [-1.264, 0.736], mean_best_reward: --\n",
      " 57336/100000: episode: 1476, duration: 0.028s, episode steps: 81, steps per second: 2905, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.079 [-0.814, 1.181], mean_best_reward: --\n",
      " 57366/100000: episode: 1477, duration: 0.011s, episode steps: 30, steps per second: 2653, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.084 [-0.920, 0.547], mean_best_reward: --\n",
      " 57419/100000: episode: 1478, duration: 0.024s, episode steps: 53, steps per second: 2223, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.113 [-0.636, 1.310], mean_best_reward: --\n",
      " 57446/100000: episode: 1479, duration: 0.010s, episode steps: 27, steps per second: 2799, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.089 [-0.776, 1.625], mean_best_reward: --\n",
      " 57464/100000: episode: 1480, duration: 0.006s, episode steps: 18, steps per second: 2792, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.834, 1.420], mean_best_reward: --\n",
      " 57478/100000: episode: 1481, duration: 0.005s, episode steps: 14, steps per second: 2721, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.110 [-0.764, 1.368], mean_best_reward: --\n",
      " 57531/100000: episode: 1482, duration: 0.016s, episode steps: 53, steps per second: 3327, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.034 [-1.149, 0.755], mean_best_reward: --\n",
      " 57568/100000: episode: 1483, duration: 0.012s, episode steps: 37, steps per second: 3209, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.067 [-1.087, 0.930], mean_best_reward: --\n",
      " 57666/100000: episode: 1484, duration: 0.030s, episode steps: 98, steps per second: 3315, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.100 [-0.784, 1.415], mean_best_reward: --\n",
      " 57692/100000: episode: 1485, duration: 0.008s, episode steps: 26, steps per second: 3110, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.057 [-0.734, 1.166], mean_best_reward: --\n",
      " 57709/100000: episode: 1486, duration: 0.007s, episode steps: 17, steps per second: 2530, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.090 [-0.750, 1.252], mean_best_reward: --\n",
      " 57750/100000: episode: 1487, duration: 0.014s, episode steps: 41, steps per second: 2866, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.561 [0.000, 1.000], mean observation: -0.066 [-2.064, 0.993], mean_best_reward: --\n",
      " 57809/100000: episode: 1488, duration: 0.021s, episode steps: 59, steps per second: 2876, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.219 [-1.567, 0.617], mean_best_reward: --\n",
      " 57830/100000: episode: 1489, duration: 0.007s, episode steps: 21, steps per second: 2982, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.057 [-0.966, 1.617], mean_best_reward: --\n",
      " 57863/100000: episode: 1490, duration: 0.010s, episode steps: 33, steps per second: 3225, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.087 [-0.554, 0.942], mean_best_reward: --\n",
      " 57896/100000: episode: 1491, duration: 0.010s, episode steps: 33, steps per second: 3163, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.011 [-1.461, 0.980], mean_best_reward: --\n",
      " 57921/100000: episode: 1492, duration: 0.008s, episode steps: 25, steps per second: 3105, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.099 [-0.256, 1.064], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57992/100000: episode: 1493, duration: 0.023s, episode steps: 71, steps per second: 3137, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.188 [-2.015, 0.690], mean_best_reward: --\n",
      " 58054/100000: episode: 1494, duration: 0.027s, episode steps: 62, steps per second: 2311, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.128, 0.723], mean_best_reward: --\n",
      " 58094/100000: episode: 1495, duration: 0.015s, episode steps: 40, steps per second: 2631, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.080 [-0.447, 0.892], mean_best_reward: --\n",
      " 58170/100000: episode: 1496, duration: 0.026s, episode steps: 76, steps per second: 2931, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.026 [-0.950, 1.201], mean_best_reward: --\n",
      " 58215/100000: episode: 1497, duration: 0.016s, episode steps: 45, steps per second: 2829, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.204 [-0.754, 1.610], mean_best_reward: --\n",
      " 58242/100000: episode: 1498, duration: 0.010s, episode steps: 27, steps per second: 2618, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.076 [-1.005, 2.059], mean_best_reward: --\n",
      " 58286/100000: episode: 1499, duration: 0.016s, episode steps: 44, steps per second: 2777, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.024 [-0.788, 1.134], mean_best_reward: --\n",
      " 58363/100000: episode: 1500, duration: 0.024s, episode steps: 77, steps per second: 3274, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.223 [-1.616, 0.694], mean_best_reward: --\n",
      " 58397/100000: episode: 1501, duration: 0.012s, episode steps: 34, steps per second: 2740, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.406, 0.933], mean_best_reward: 100.500000\n",
      " 58488/100000: episode: 1502, duration: 0.030s, episode steps: 91, steps per second: 3016, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.090 [-1.295, 1.172], mean_best_reward: --\n",
      " 58560/100000: episode: 1503, duration: 0.025s, episode steps: 72, steps per second: 2849, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-1.376, 1.393], mean_best_reward: --\n",
      " 58589/100000: episode: 1504, duration: 0.016s, episode steps: 29, steps per second: 1803, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.064 [-0.584, 1.026], mean_best_reward: --\n",
      " 58642/100000: episode: 1505, duration: 0.016s, episode steps: 53, steps per second: 3279, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.028 [-0.735, 1.093], mean_best_reward: --\n",
      " 58668/100000: episode: 1506, duration: 0.008s, episode steps: 26, steps per second: 3131, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.047 [-1.174, 1.849], mean_best_reward: --\n",
      " 58715/100000: episode: 1507, duration: 0.016s, episode steps: 47, steps per second: 2911, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.112 [-0.923, 0.453], mean_best_reward: --\n",
      " 58740/100000: episode: 1508, duration: 0.011s, episode steps: 25, steps per second: 2337, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.082 [-1.196, 0.623], mean_best_reward: --\n",
      " 58779/100000: episode: 1509, duration: 0.012s, episode steps: 39, steps per second: 3217, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.014 [-0.613, 1.051], mean_best_reward: --\n",
      " 58834/100000: episode: 1510, duration: 0.016s, episode steps: 55, steps per second: 3380, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.234 [-1.264, 0.658], mean_best_reward: --\n",
      " 58860/100000: episode: 1511, duration: 0.008s, episode steps: 26, steps per second: 3255, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.075 [-0.749, 1.269], mean_best_reward: --\n",
      " 58943/100000: episode: 1512, duration: 0.038s, episode steps: 83, steps per second: 2212, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.279 [-1.450, 0.793], mean_best_reward: --\n",
      " 58974/100000: episode: 1513, duration: 0.019s, episode steps: 31, steps per second: 1675, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.063 [-1.119, 0.563], mean_best_reward: --\n",
      " 58989/100000: episode: 1514, duration: 0.007s, episode steps: 15, steps per second: 2029, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.095 [-1.128, 0.580], mean_best_reward: --\n",
      " 59034/100000: episode: 1515, duration: 0.018s, episode steps: 45, steps per second: 2548, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.067 [-0.559, 1.288], mean_best_reward: --\n",
      " 59085/100000: episode: 1516, duration: 0.019s, episode steps: 51, steps per second: 2710, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.080 [-0.648, 1.222], mean_best_reward: --\n",
      " 59148/100000: episode: 1517, duration: 0.026s, episode steps: 63, steps per second: 2388, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.094 [-1.267, 0.681], mean_best_reward: --\n",
      " 59170/100000: episode: 1518, duration: 0.008s, episode steps: 22, steps per second: 2720, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.769, 1.233], mean_best_reward: --\n",
      " 59194/100000: episode: 1519, duration: 0.008s, episode steps: 24, steps per second: 2973, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.038 [-1.111, 0.621], mean_best_reward: --\n",
      " 59222/100000: episode: 1520, duration: 0.009s, episode steps: 28, steps per second: 3070, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.081 [-0.406, 1.313], mean_best_reward: --\n",
      " 59244/100000: episode: 1521, duration: 0.007s, episode steps: 22, steps per second: 3015, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.003 [-1.400, 1.802], mean_best_reward: --\n",
      " 59288/100000: episode: 1522, duration: 0.014s, episode steps: 44, steps per second: 3221, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.014 [-0.849, 1.652], mean_best_reward: --\n",
      " 59451/100000: episode: 1523, duration: 0.049s, episode steps: 163, steps per second: 3298, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.051 [-1.018, 1.089], mean_best_reward: --\n",
      " 59479/100000: episode: 1524, duration: 0.009s, episode steps: 28, steps per second: 3272, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.450, 1.112], mean_best_reward: --\n",
      " 59524/100000: episode: 1525, duration: 0.014s, episode steps: 45, steps per second: 3259, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.050 [-1.330, 1.165], mean_best_reward: --\n",
      " 59557/100000: episode: 1526, duration: 0.011s, episode steps: 33, steps per second: 3064, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.107 [-0.430, 1.112], mean_best_reward: --\n",
      " 59582/100000: episode: 1527, duration: 0.008s, episode steps: 25, steps per second: 3031, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.023 [-0.973, 1.222], mean_best_reward: --\n",
      " 59624/100000: episode: 1528, duration: 0.013s, episode steps: 42, steps per second: 3311, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.062 [-0.555, 1.252], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59699/100000: episode: 1529, duration: 0.022s, episode steps: 75, steps per second: 3360, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.042 [-2.126, 2.055], mean_best_reward: --\n",
      " 59716/100000: episode: 1530, duration: 0.006s, episode steps: 17, steps per second: 2662, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.057 [-1.315, 1.877], mean_best_reward: --\n",
      " 59743/100000: episode: 1531, duration: 0.014s, episode steps: 27, steps per second: 1984, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.079 [-1.280, 0.737], mean_best_reward: --\n",
      " 59765/100000: episode: 1532, duration: 0.008s, episode steps: 22, steps per second: 2808, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.017 [-1.013, 1.550], mean_best_reward: --\n",
      " 59820/100000: episode: 1533, duration: 0.017s, episode steps: 55, steps per second: 3161, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.111 [-0.354, 0.967], mean_best_reward: --\n",
      " 59844/100000: episode: 1534, duration: 0.008s, episode steps: 24, steps per second: 3020, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.084 [-0.633, 0.962], mean_best_reward: --\n",
      " 59897/100000: episode: 1535, duration: 0.016s, episode steps: 53, steps per second: 3332, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.116 [-1.100, 0.598], mean_best_reward: --\n",
      " 59930/100000: episode: 1536, duration: 0.010s, episode steps: 33, steps per second: 3271, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.424 [0.000, 1.000], mean observation: 0.005 [-1.106, 1.351], mean_best_reward: --\n",
      " 59948/100000: episode: 1537, duration: 0.006s, episode steps: 18, steps per second: 2986, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.096 [-0.553, 1.200], mean_best_reward: --\n",
      " 59982/100000: episode: 1538, duration: 0.011s, episode steps: 34, steps per second: 3161, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.070 [-0.627, 1.032], mean_best_reward: --\n",
      " 60020/100000: episode: 1539, duration: 0.012s, episode steps: 38, steps per second: 3087, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.076 [-1.159, 0.420], mean_best_reward: --\n",
      " 60060/100000: episode: 1540, duration: 0.013s, episode steps: 40, steps per second: 3167, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.076 [-0.593, 1.572], mean_best_reward: --\n",
      " 60080/100000: episode: 1541, duration: 0.007s, episode steps: 20, steps per second: 3059, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-1.309, 0.765], mean_best_reward: --\n",
      " 60101/100000: episode: 1542, duration: 0.007s, episode steps: 21, steps per second: 3104, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.093 [-1.333, 0.751], mean_best_reward: --\n",
      " 60138/100000: episode: 1543, duration: 0.012s, episode steps: 37, steps per second: 3187, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.057 [-1.455, 0.801], mean_best_reward: --\n",
      " 60162/100000: episode: 1544, duration: 0.008s, episode steps: 24, steps per second: 3115, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.090 [-0.418, 1.283], mean_best_reward: --\n",
      " 60212/100000: episode: 1545, duration: 0.017s, episode steps: 50, steps per second: 2951, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.043 [-1.149, 0.820], mean_best_reward: --\n",
      " 60264/100000: episode: 1546, duration: 0.017s, episode steps: 52, steps per second: 3023, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.074 [-0.792, 1.019], mean_best_reward: --\n",
      " 60283/100000: episode: 1547, duration: 0.006s, episode steps: 19, steps per second: 2964, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.094 [-1.024, 2.037], mean_best_reward: --\n",
      " 60360/100000: episode: 1548, duration: 0.026s, episode steps: 77, steps per second: 3009, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.064 [-0.919, 1.292], mean_best_reward: --\n",
      " 60378/100000: episode: 1549, duration: 0.009s, episode steps: 18, steps per second: 2070, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.092 [-0.816, 1.653], mean_best_reward: --\n",
      " 60417/100000: episode: 1550, duration: 0.012s, episode steps: 39, steps per second: 3193, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.044 [-1.650, 0.962], mean_best_reward: --\n",
      " 60470/100000: episode: 1551, duration: 0.018s, episode steps: 53, steps per second: 3015, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.068 [-1.530, 0.789], mean_best_reward: 224.500000\n",
      " 60519/100000: episode: 1552, duration: 0.015s, episode steps: 49, steps per second: 3340, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.040 [-0.799, 1.487], mean_best_reward: --\n",
      " 60554/100000: episode: 1553, duration: 0.011s, episode steps: 35, steps per second: 3283, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.074 [-1.396, 1.605], mean_best_reward: --\n",
      " 60627/100000: episode: 1554, duration: 0.022s, episode steps: 73, steps per second: 3390, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.026 [-1.569, 1.531], mean_best_reward: --\n",
      " 60642/100000: episode: 1555, duration: 0.005s, episode steps: 15, steps per second: 2898, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.098 [-0.969, 1.835], mean_best_reward: --\n",
      " 60667/100000: episode: 1556, duration: 0.009s, episode steps: 25, steps per second: 2890, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.065 [-0.592, 1.103], mean_best_reward: --\n",
      " 60697/100000: episode: 1557, duration: 0.010s, episode steps: 30, steps per second: 3039, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.041 [-0.742, 1.245], mean_best_reward: --\n",
      " 60716/100000: episode: 1558, duration: 0.007s, episode steps: 19, steps per second: 2920, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.062 [-1.000, 1.386], mean_best_reward: --\n",
      " 60781/100000: episode: 1559, duration: 0.020s, episode steps: 65, steps per second: 3297, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.143 [-0.879, 0.704], mean_best_reward: --\n",
      " 60807/100000: episode: 1560, duration: 0.008s, episode steps: 26, steps per second: 3130, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.093 [-0.538, 0.873], mean_best_reward: --\n",
      " 60849/100000: episode: 1561, duration: 0.013s, episode steps: 42, steps per second: 3134, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.137 [-0.583, 1.308], mean_best_reward: --\n",
      " 60913/100000: episode: 1562, duration: 0.019s, episode steps: 64, steps per second: 3323, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.059 [-0.812, 1.207], mean_best_reward: --\n",
      " 60936/100000: episode: 1563, duration: 0.007s, episode steps: 23, steps per second: 3126, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.079 [-0.788, 1.395], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61042/100000: episode: 1564, duration: 0.036s, episode steps: 106, steps per second: 2980, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.099 [-0.664, 1.172], mean_best_reward: --\n",
      " 61097/100000: episode: 1565, duration: 0.017s, episode steps: 55, steps per second: 3177, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.083 [-0.870, 0.810], mean_best_reward: --\n",
      " 61168/100000: episode: 1566, duration: 0.023s, episode steps: 71, steps per second: 3147, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.131 [-1.798, 1.097], mean_best_reward: --\n",
      " 61183/100000: episode: 1567, duration: 0.005s, episode steps: 15, steps per second: 2801, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.107 [-1.199, 0.641], mean_best_reward: --\n",
      " 61212/100000: episode: 1568, duration: 0.009s, episode steps: 29, steps per second: 3105, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.072 [-0.622, 1.368], mean_best_reward: --\n",
      " 61231/100000: episode: 1569, duration: 0.007s, episode steps: 19, steps per second: 2906, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.105 [-0.777, 1.336], mean_best_reward: --\n",
      " 61247/100000: episode: 1570, duration: 0.006s, episode steps: 16, steps per second: 2888, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.081 [-1.010, 1.555], mean_best_reward: --\n",
      " 61268/100000: episode: 1571, duration: 0.007s, episode steps: 21, steps per second: 3095, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.105 [-0.540, 0.931], mean_best_reward: --\n",
      " 61307/100000: episode: 1572, duration: 0.011s, episode steps: 39, steps per second: 3395, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.045 [-0.637, 1.393], mean_best_reward: --\n",
      " 61331/100000: episode: 1573, duration: 0.008s, episode steps: 24, steps per second: 2892, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.089 [-0.775, 1.560], mean_best_reward: --\n",
      " 61382/100000: episode: 1574, duration: 0.015s, episode steps: 51, steps per second: 3311, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.073 [-1.148, 0.559], mean_best_reward: --\n",
      " 61506/100000: episode: 1575, duration: 0.046s, episode steps: 124, steps per second: 2710, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.248 [-1.618, 0.887], mean_best_reward: --\n",
      " 61619/100000: episode: 1576, duration: 0.081s, episode steps: 113, steps per second: 1399, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.067 [-1.041, 1.749], mean_best_reward: --\n",
      " 61691/100000: episode: 1577, duration: 0.050s, episode steps: 72, steps per second: 1443, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.237 [-0.477, 1.607], mean_best_reward: --\n",
      " 61710/100000: episode: 1578, duration: 0.016s, episode steps: 19, steps per second: 1193, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.071 [-2.134, 1.365], mean_best_reward: --\n",
      " 61728/100000: episode: 1579, duration: 0.015s, episode steps: 18, steps per second: 1201, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.103 [-0.799, 1.363], mean_best_reward: --\n",
      " 61820/100000: episode: 1580, duration: 0.055s, episode steps: 92, steps per second: 1672, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.822, 1.171], mean_best_reward: --\n",
      " 61842/100000: episode: 1581, duration: 0.011s, episode steps: 22, steps per second: 1972, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.095 [-1.419, 0.768], mean_best_reward: --\n",
      " 61876/100000: episode: 1582, duration: 0.013s, episode steps: 34, steps per second: 2682, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.099 [-0.390, 1.154], mean_best_reward: --\n",
      " 61907/100000: episode: 1583, duration: 0.010s, episode steps: 31, steps per second: 3161, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.069 [-0.567, 1.046], mean_best_reward: --\n",
      " 61929/100000: episode: 1584, duration: 0.007s, episode steps: 22, steps per second: 3082, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.024 [-1.001, 1.403], mean_best_reward: --\n",
      " 61974/100000: episode: 1585, duration: 0.013s, episode steps: 45, steps per second: 3346, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.063 [-0.790, 1.819], mean_best_reward: --\n",
      " 62000/100000: episode: 1586, duration: 0.009s, episode steps: 26, steps per second: 2948, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.099 [-0.459, 1.370], mean_best_reward: --\n",
      " 62022/100000: episode: 1587, duration: 0.008s, episode steps: 22, steps per second: 2707, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.069 [-0.835, 1.591], mean_best_reward: --\n",
      " 62042/100000: episode: 1588, duration: 0.008s, episode steps: 20, steps per second: 2365, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.768, 1.559], mean_best_reward: --\n",
      " 62085/100000: episode: 1589, duration: 0.015s, episode steps: 43, steps per second: 2899, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.061 [-0.903, 1.112], mean_best_reward: --\n",
      " 62133/100000: episode: 1590, duration: 0.016s, episode steps: 48, steps per second: 2991, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.271 [-1.730, 1.078], mean_best_reward: --\n",
      " 62171/100000: episode: 1591, duration: 0.012s, episode steps: 38, steps per second: 3103, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-1.012, 1.533], mean_best_reward: --\n",
      " 62189/100000: episode: 1592, duration: 0.006s, episode steps: 18, steps per second: 2949, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.178, 0.602], mean_best_reward: --\n",
      " 62230/100000: episode: 1593, duration: 0.013s, episode steps: 41, steps per second: 3264, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.017 [-1.284, 0.811], mean_best_reward: --\n",
      " 62271/100000: episode: 1594, duration: 0.012s, episode steps: 41, steps per second: 3285, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.039 [-1.152, 0.863], mean_best_reward: --\n",
      " 62385/100000: episode: 1595, duration: 0.033s, episode steps: 114, steps per second: 3432, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.113 [-1.138, 1.095], mean_best_reward: --\n",
      " 62415/100000: episode: 1596, duration: 0.010s, episode steps: 30, steps per second: 3011, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.102 [-0.426, 1.006], mean_best_reward: --\n",
      " 62463/100000: episode: 1597, duration: 0.015s, episode steps: 48, steps per second: 3182, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.598, 1.531], mean_best_reward: --\n",
      " 62583/100000: episode: 1598, duration: 0.035s, episode steps: 120, steps per second: 3438, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.478 [-2.560, 1.033], mean_best_reward: --\n",
      " 62606/100000: episode: 1599, duration: 0.008s, episode steps: 23, steps per second: 2754, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.129 [-0.769, 1.221], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62686/100000: episode: 1600, duration: 0.026s, episode steps: 80, steps per second: 3022, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.053 [-0.984, 1.056], mean_best_reward: --\n",
      " 62731/100000: episode: 1601, duration: 0.020s, episode steps: 45, steps per second: 2222, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.130 [-0.700, 1.375], mean_best_reward: 162.000000\n",
      " 62743/100000: episode: 1602, duration: 0.006s, episode steps: 12, steps per second: 2080, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.250, 0.767], mean_best_reward: --\n",
      " 62823/100000: episode: 1603, duration: 0.026s, episode steps: 80, steps per second: 3042, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.103 [-1.347, 1.432], mean_best_reward: --\n",
      " 62840/100000: episode: 1604, duration: 0.006s, episode steps: 17, steps per second: 2931, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.106 [-0.563, 1.083], mean_best_reward: --\n",
      " 62861/100000: episode: 1605, duration: 0.008s, episode steps: 21, steps per second: 2695, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.123 [-0.565, 1.151], mean_best_reward: --\n",
      " 62939/100000: episode: 1606, duration: 0.025s, episode steps: 78, steps per second: 3145, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.144 [-0.839, 2.254], mean_best_reward: --\n",
      " 62975/100000: episode: 1607, duration: 0.011s, episode steps: 36, steps per second: 3204, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.104 [-0.607, 1.000], mean_best_reward: --\n",
      " 63011/100000: episode: 1608, duration: 0.013s, episode steps: 36, steps per second: 2731, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.069 [-0.928, 1.450], mean_best_reward: --\n",
      " 63085/100000: episode: 1609, duration: 0.027s, episode steps: 74, steps per second: 2709, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.739, 1.405], mean_best_reward: --\n",
      " 63153/100000: episode: 1610, duration: 0.020s, episode steps: 68, steps per second: 3373, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.023 [-1.146, 1.573], mean_best_reward: --\n",
      " 63172/100000: episode: 1611, duration: 0.007s, episode steps: 19, steps per second: 2704, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.077 [-0.596, 1.217], mean_best_reward: --\n",
      " 63216/100000: episode: 1612, duration: 0.014s, episode steps: 44, steps per second: 3050, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.121 [-0.475, 1.665], mean_best_reward: --\n",
      " 63235/100000: episode: 1613, duration: 0.007s, episode steps: 19, steps per second: 2784, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.098 [-0.758, 1.535], mean_best_reward: --\n",
      " 63276/100000: episode: 1614, duration: 0.014s, episode steps: 41, steps per second: 2893, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: 0.018 [-1.188, 1.600], mean_best_reward: --\n",
      " 63329/100000: episode: 1615, duration: 0.022s, episode steps: 53, steps per second: 2409, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.088 [-1.106, 1.419], mean_best_reward: --\n",
      " 63398/100000: episode: 1616, duration: 0.021s, episode steps: 69, steps per second: 3276, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.073 [-1.160, 1.048], mean_best_reward: --\n",
      " 63424/100000: episode: 1617, duration: 0.009s, episode steps: 26, steps per second: 2898, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.635, 1.172], mean_best_reward: --\n",
      " 63443/100000: episode: 1618, duration: 0.006s, episode steps: 19, steps per second: 3164, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.111 [-0.562, 1.077], mean_best_reward: --\n",
      " 63464/100000: episode: 1619, duration: 0.007s, episode steps: 21, steps per second: 3142, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.050 [-1.224, 1.962], mean_best_reward: --\n",
      " 63489/100000: episode: 1620, duration: 0.008s, episode steps: 25, steps per second: 2973, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.119 [-1.386, 0.586], mean_best_reward: --\n",
      " 63515/100000: episode: 1621, duration: 0.008s, episode steps: 26, steps per second: 3104, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.017 [-1.195, 1.895], mean_best_reward: --\n",
      " 63577/100000: episode: 1622, duration: 0.019s, episode steps: 62, steps per second: 3248, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.057 [-0.737, 1.007], mean_best_reward: --\n",
      " 63594/100000: episode: 1623, duration: 0.006s, episode steps: 17, steps per second: 2857, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.137 [-1.128, 0.544], mean_best_reward: --\n",
      " 63627/100000: episode: 1624, duration: 0.011s, episode steps: 33, steps per second: 2988, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.088 [-0.566, 1.234], mean_best_reward: --\n",
      " 63642/100000: episode: 1625, duration: 0.005s, episode steps: 15, steps per second: 2874, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.110 [-0.602, 1.257], mean_best_reward: --\n",
      " 63663/100000: episode: 1626, duration: 0.007s, episode steps: 21, steps per second: 2918, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.759, 1.183], mean_best_reward: --\n",
      " 63709/100000: episode: 1627, duration: 0.015s, episode steps: 46, steps per second: 3085, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.065 [-1.135, 1.626], mean_best_reward: --\n",
      " 63782/100000: episode: 1628, duration: 0.023s, episode steps: 73, steps per second: 3155, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: 0.076 [-0.950, 1.818], mean_best_reward: --\n",
      " 63819/100000: episode: 1629, duration: 0.015s, episode steps: 37, steps per second: 2434, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.090 [-1.005, 0.628], mean_best_reward: --\n",
      " 63849/100000: episode: 1630, duration: 0.010s, episode steps: 30, steps per second: 2990, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.592, 1.040], mean_best_reward: --\n",
      " 63864/100000: episode: 1631, duration: 0.007s, episode steps: 15, steps per second: 2259, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.102 [-1.677, 0.960], mean_best_reward: --\n",
      " 63891/100000: episode: 1632, duration: 0.010s, episode steps: 27, steps per second: 2759, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.102 [-1.059, 0.371], mean_best_reward: --\n",
      " 63927/100000: episode: 1633, duration: 0.017s, episode steps: 36, steps per second: 2061, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.056 [-0.752, 1.047], mean_best_reward: --\n",
      " 63976/100000: episode: 1634, duration: 0.017s, episode steps: 49, steps per second: 2898, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.062 [-0.794, 1.446], mean_best_reward: --\n",
      " 63994/100000: episode: 1635, duration: 0.007s, episode steps: 18, steps per second: 2583, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.105 [-1.367, 0.579], mean_best_reward: --\n",
      " 64014/100000: episode: 1636, duration: 0.007s, episode steps: 20, steps per second: 2721, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.063 [-1.926, 1.139], mean_best_reward: --\n",
      " 64045/100000: episode: 1637, duration: 0.011s, episode steps: 31, steps per second: 2918, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.086 [-0.633, 1.004], mean_best_reward: --\n",
      " 64099/100000: episode: 1638, duration: 0.016s, episode steps: 54, steps per second: 3324, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.053 [-1.021, 0.421], mean_best_reward: --\n",
      " 64191/100000: episode: 1639, duration: 0.026s, episode steps: 92, steps per second: 3490, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.298 [-2.068, 1.200], mean_best_reward: --\n",
      " 64225/100000: episode: 1640, duration: 0.011s, episode steps: 34, steps per second: 2979, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.074 [-1.707, 0.834], mean_best_reward: --\n",
      " 64250/100000: episode: 1641, duration: 0.009s, episode steps: 25, steps per second: 2765, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.029 [-1.461, 0.983], mean_best_reward: --\n",
      " 64295/100000: episode: 1642, duration: 0.014s, episode steps: 45, steps per second: 3323, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.123 [-0.458, 1.032], mean_best_reward: --\n",
      " 64308/100000: episode: 1643, duration: 0.005s, episode steps: 13, steps per second: 2874, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.126 [-0.939, 1.655], mean_best_reward: --\n",
      " 64373/100000: episode: 1644, duration: 0.020s, episode steps: 65, steps per second: 3322, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.016 [-0.955, 1.452], mean_best_reward: --\n",
      " 64428/100000: episode: 1645, duration: 0.016s, episode steps: 55, steps per second: 3354, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.033 [-0.816, 1.205], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64456/100000: episode: 1646, duration: 0.010s, episode steps: 28, steps per second: 2881, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.207, 0.388], mean_best_reward: --\n",
      " 64488/100000: episode: 1647, duration: 0.011s, episode steps: 32, steps per second: 2874, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.076 [-0.608, 1.538], mean_best_reward: --\n",
      " 64602/100000: episode: 1648, duration: 0.046s, episode steps: 114, steps per second: 2464, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.151 [-1.380, 1.118], mean_best_reward: --\n",
      " 64617/100000: episode: 1649, duration: 0.007s, episode steps: 15, steps per second: 2253, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.068 [-1.223, 1.821], mean_best_reward: --\n",
      " 64629/100000: episode: 1650, duration: 0.005s, episode steps: 12, steps per second: 2206, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.112 [-2.025, 1.137], mean_best_reward: --\n",
      " 64677/100000: episode: 1651, duration: 0.019s, episode steps: 48, steps per second: 2570, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: 0.024 [-1.156, 1.728], mean_best_reward: 160.000000\n",
      " 64694/100000: episode: 1652, duration: 0.007s, episode steps: 17, steps per second: 2556, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.125 [-0.546, 1.099], mean_best_reward: --\n",
      " 64722/100000: episode: 1653, duration: 0.012s, episode steps: 28, steps per second: 2366, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.063 [-0.783, 1.697], mean_best_reward: --\n",
      " 64774/100000: episode: 1654, duration: 0.020s, episode steps: 52, steps per second: 2601, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.005 [-0.833, 1.299], mean_best_reward: --\n",
      " 64783/100000: episode: 1655, duration: 0.004s, episode steps: 9, steps per second: 2465, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.144 [-1.779, 0.951], mean_best_reward: --\n",
      " 64821/100000: episode: 1656, duration: 0.013s, episode steps: 38, steps per second: 2971, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.014 [-1.126, 0.753], mean_best_reward: --\n",
      " 64842/100000: episode: 1657, duration: 0.007s, episode steps: 21, steps per second: 2999, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.087 [-0.439, 1.161], mean_best_reward: --\n",
      " 64905/100000: episode: 1658, duration: 0.019s, episode steps: 63, steps per second: 3335, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.106 [-1.206, 1.039], mean_best_reward: --\n",
      " 64929/100000: episode: 1659, duration: 0.008s, episode steps: 24, steps per second: 3006, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.109 [-1.824, 0.767], mean_best_reward: --\n",
      " 64970/100000: episode: 1660, duration: 0.013s, episode steps: 41, steps per second: 3247, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.585 [0.000, 1.000], mean observation: 0.015 [-2.018, 1.553], mean_best_reward: --\n",
      " 64988/100000: episode: 1661, duration: 0.006s, episode steps: 18, steps per second: 2906, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-1.014, 1.486], mean_best_reward: --\n",
      " 65012/100000: episode: 1662, duration: 0.008s, episode steps: 24, steps per second: 2912, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.058 [-0.639, 1.244], mean_best_reward: --\n",
      " 65093/100000: episode: 1663, duration: 0.030s, episode steps: 81, steps per second: 2689, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.006 [-0.723, 1.367], mean_best_reward: --\n",
      " 65104/100000: episode: 1664, duration: 0.004s, episode steps: 11, steps per second: 2532, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.107 [-1.315, 0.797], mean_best_reward: --\n",
      " 65133/100000: episode: 1665, duration: 0.009s, episode steps: 29, steps per second: 3115, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.047 [-0.830, 1.520], mean_best_reward: --\n",
      " 65170/100000: episode: 1666, duration: 0.011s, episode steps: 37, steps per second: 3262, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: -0.021 [-1.194, 0.765], mean_best_reward: --\n",
      " 65325/100000: episode: 1667, duration: 0.045s, episode steps: 155, steps per second: 3468, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.307 [-1.843, 1.023], mean_best_reward: --\n",
      " 65390/100000: episode: 1668, duration: 0.019s, episode steps: 65, steps per second: 3439, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.068 [-1.161, 0.706], mean_best_reward: --\n",
      " 65435/100000: episode: 1669, duration: 0.018s, episode steps: 45, steps per second: 2565, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.048 [-0.634, 1.346], mean_best_reward: --\n",
      " 65490/100000: episode: 1670, duration: 0.035s, episode steps: 55, steps per second: 1550, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.176 [-2.001, 1.132], mean_best_reward: --\n",
      " 65527/100000: episode: 1671, duration: 0.018s, episode steps: 37, steps per second: 2086, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.122 [-0.948, 0.390], mean_best_reward: --\n",
      " 65576/100000: episode: 1672, duration: 0.023s, episode steps: 49, steps per second: 2098, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.003 [-1.246, 0.997], mean_best_reward: --\n",
      " 65601/100000: episode: 1673, duration: 0.010s, episode steps: 25, steps per second: 2605, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.134 [-0.632, 1.079], mean_best_reward: --\n",
      " 65639/100000: episode: 1674, duration: 0.012s, episode steps: 38, steps per second: 3220, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.033, 0.561], mean_best_reward: --\n",
      " 65672/100000: episode: 1675, duration: 0.010s, episode steps: 33, steps per second: 3170, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.069 [-0.632, 1.170], mean_best_reward: --\n",
      " 65699/100000: episode: 1676, duration: 0.009s, episode steps: 27, steps per second: 2982, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.064 [-1.619, 0.823], mean_best_reward: --\n",
      " 65729/100000: episode: 1677, duration: 0.009s, episode steps: 30, steps per second: 3299, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.043 [-0.933, 1.517], mean_best_reward: --\n",
      " 65756/100000: episode: 1678, duration: 0.008s, episode steps: 27, steps per second: 3315, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.057 [-1.190, 0.624], mean_best_reward: --\n",
      " 65863/100000: episode: 1679, duration: 0.030s, episode steps: 107, steps per second: 3513, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.279 [-1.751, 1.167], mean_best_reward: --\n",
      " 65892/100000: episode: 1680, duration: 0.009s, episode steps: 29, steps per second: 3378, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.063 [-0.781, 1.263], mean_best_reward: --\n",
      " 65923/100000: episode: 1681, duration: 0.012s, episode steps: 31, steps per second: 2515, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.079 [-0.906, 0.611], mean_best_reward: --\n",
      " 65981/100000: episode: 1682, duration: 0.024s, episode steps: 58, steps per second: 2445, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.115 [-1.161, 0.729], mean_best_reward: --\n",
      " 66012/100000: episode: 1683, duration: 0.017s, episode steps: 31, steps per second: 1836, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.053 [-0.729, 1.049], mean_best_reward: --\n",
      " 66033/100000: episode: 1684, duration: 0.014s, episode steps: 21, steps per second: 1480, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.102 [-0.568, 1.333], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66047/100000: episode: 1685, duration: 0.010s, episode steps: 14, steps per second: 1367, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.598, 1.134], mean_best_reward: --\n",
      " 66073/100000: episode: 1686, duration: 0.020s, episode steps: 26, steps per second: 1325, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.065 [-1.241, 0.630], mean_best_reward: --\n",
      " 66129/100000: episode: 1687, duration: 0.031s, episode steps: 56, steps per second: 1791, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: -0.004 [-1.432, 0.803], mean_best_reward: --\n",
      " 66174/100000: episode: 1688, duration: 0.025s, episode steps: 45, steps per second: 1815, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.110 [-0.806, 1.055], mean_best_reward: --\n",
      " 66214/100000: episode: 1689, duration: 0.029s, episode steps: 40, steps per second: 1388, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.075 [-1.614, 1.340], mean_best_reward: --\n",
      " 66246/100000: episode: 1690, duration: 0.023s, episode steps: 32, steps per second: 1412, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: 0.012 [-1.747, 1.185], mean_best_reward: --\n",
      " 66295/100000: episode: 1691, duration: 0.021s, episode steps: 49, steps per second: 2281, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: 0.008 [-1.330, 1.911], mean_best_reward: --\n",
      " 66330/100000: episode: 1692, duration: 0.013s, episode steps: 35, steps per second: 2695, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.083 [-1.438, 1.104], mean_best_reward: --\n",
      " 66376/100000: episode: 1693, duration: 0.016s, episode steps: 46, steps per second: 2882, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.013 [-1.026, 1.063], mean_best_reward: --\n",
      " 66395/100000: episode: 1694, duration: 0.007s, episode steps: 19, steps per second: 2746, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.081 [-0.830, 1.265], mean_best_reward: --\n",
      " 66424/100000: episode: 1695, duration: 0.010s, episode steps: 29, steps per second: 2855, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.153 [-0.978, 0.485], mean_best_reward: --\n",
      " 66443/100000: episode: 1696, duration: 0.009s, episode steps: 19, steps per second: 2083, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.058 [-1.455, 0.840], mean_best_reward: --\n",
      " 66478/100000: episode: 1697, duration: 0.016s, episode steps: 35, steps per second: 2197, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.152 [-1.126, 0.588], mean_best_reward: --\n",
      " 66499/100000: episode: 1698, duration: 0.010s, episode steps: 21, steps per second: 2089, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.122 [-0.862, 0.372], mean_best_reward: --\n",
      " 66529/100000: episode: 1699, duration: 0.011s, episode steps: 30, steps per second: 2728, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-1.351, 0.727], mean_best_reward: --\n",
      " 66581/100000: episode: 1700, duration: 0.017s, episode steps: 52, steps per second: 2979, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.103 [-0.904, 0.919], mean_best_reward: --\n",
      " 66613/100000: episode: 1701, duration: 0.011s, episode steps: 32, steps per second: 2817, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.018 [-1.146, 0.772], mean_best_reward: 124.000000\n",
      " 66642/100000: episode: 1702, duration: 0.010s, episode steps: 29, steps per second: 3027, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.119 [-0.437, 1.108], mean_best_reward: --\n",
      " 66695/100000: episode: 1703, duration: 0.017s, episode steps: 53, steps per second: 3138, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.073 [-0.544, 1.161], mean_best_reward: --\n",
      " 66781/100000: episode: 1704, duration: 0.024s, episode steps: 86, steps per second: 3530, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.261 [-1.284, 1.143], mean_best_reward: --\n",
      " 66814/100000: episode: 1705, duration: 0.010s, episode steps: 33, steps per second: 3273, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.042 [-1.021, 0.615], mean_best_reward: --\n",
      " 66868/100000: episode: 1706, duration: 0.016s, episode steps: 54, steps per second: 3437, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.157 [-1.003, 0.881], mean_best_reward: --\n",
      " 66886/100000: episode: 1707, duration: 0.006s, episode steps: 18, steps per second: 2971, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.070 [-1.116, 1.631], mean_best_reward: --\n",
      " 66994/100000: episode: 1708, duration: 0.031s, episode steps: 108, steps per second: 3469, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.113 [-1.172, 0.781], mean_best_reward: --\n",
      " 67018/100000: episode: 1709, duration: 0.008s, episode steps: 24, steps per second: 3188, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.044 [-0.795, 1.439], mean_best_reward: --\n",
      " 67065/100000: episode: 1710, duration: 0.014s, episode steps: 47, steps per second: 3337, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.010 [-1.426, 0.970], mean_best_reward: --\n",
      " 67128/100000: episode: 1711, duration: 0.026s, episode steps: 63, steps per second: 2445, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.157 [-0.549, 1.114], mean_best_reward: --\n",
      " 67183/100000: episode: 1712, duration: 0.022s, episode steps: 55, steps per second: 2479, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.167 [-1.335, 0.714], mean_best_reward: --\n",
      " 67215/100000: episode: 1713, duration: 0.011s, episode steps: 32, steps per second: 2936, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.030 [-0.959, 1.650], mean_best_reward: --\n",
      " 67234/100000: episode: 1714, duration: 0.007s, episode steps: 19, steps per second: 2664, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.060 [-0.620, 1.139], mean_best_reward: --\n",
      " 67293/100000: episode: 1715, duration: 0.019s, episode steps: 59, steps per second: 3139, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.092 [-0.821, 1.068], mean_best_reward: --\n",
      " 67340/100000: episode: 1716, duration: 0.015s, episode steps: 47, steps per second: 3098, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.025 [-0.903, 1.015], mean_best_reward: --\n",
      " 67406/100000: episode: 1717, duration: 0.020s, episode steps: 66, steps per second: 3234, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.063 [-1.340, 1.673], mean_best_reward: --\n",
      " 67418/100000: episode: 1718, duration: 0.005s, episode steps: 12, steps per second: 2616, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.205, 1.806], mean_best_reward: --\n",
      " 67449/100000: episode: 1719, duration: 0.010s, episode steps: 31, steps per second: 3012, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.077 [-0.449, 1.103], mean_best_reward: --\n",
      " 67513/100000: episode: 1720, duration: 0.019s, episode steps: 64, steps per second: 3320, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.127 [-1.387, 1.029], mean_best_reward: --\n",
      " 67542/100000: episode: 1721, duration: 0.009s, episode steps: 29, steps per second: 3207, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.054 [-0.605, 0.976], mean_best_reward: --\n",
      " 67590/100000: episode: 1722, duration: 0.015s, episode steps: 48, steps per second: 3266, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.130 [-1.909, 1.685], mean_best_reward: --\n",
      " 67618/100000: episode: 1723, duration: 0.009s, episode steps: 28, steps per second: 3202, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.740, 1.203], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67647/100000: episode: 1724, duration: 0.009s, episode steps: 29, steps per second: 3096, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.045 [-0.607, 1.070], mean_best_reward: --\n",
      " 67661/100000: episode: 1725, duration: 0.005s, episode steps: 14, steps per second: 2694, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.093 [-0.976, 1.705], mean_best_reward: --\n",
      " 67687/100000: episode: 1726, duration: 0.011s, episode steps: 26, steps per second: 2319, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.105 [-1.938, 0.828], mean_best_reward: --\n",
      " 67718/100000: episode: 1727, duration: 0.013s, episode steps: 31, steps per second: 2380, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.053 [-0.815, 1.522], mean_best_reward: --\n",
      " 67760/100000: episode: 1728, duration: 0.018s, episode steps: 42, steps per second: 2307, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.045 [-0.573, 0.936], mean_best_reward: --\n",
      " 67824/100000: episode: 1729, duration: 0.020s, episode steps: 64, steps per second: 3179, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.120 [-1.054, 1.435], mean_best_reward: --\n",
      " 67863/100000: episode: 1730, duration: 0.013s, episode steps: 39, steps per second: 3099, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.082 [-0.606, 0.896], mean_best_reward: --\n",
      " 67920/100000: episode: 1731, duration: 0.016s, episode steps: 57, steps per second: 3458, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.160 [-0.613, 0.962], mean_best_reward: --\n",
      " 68000/100000: episode: 1732, duration: 0.023s, episode steps: 80, steps per second: 3491, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.136 [-0.867, 1.267], mean_best_reward: --\n",
      " 68033/100000: episode: 1733, duration: 0.010s, episode steps: 33, steps per second: 3305, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.121 [-0.574, 1.241], mean_best_reward: --\n",
      " 68063/100000: episode: 1734, duration: 0.010s, episode steps: 30, steps per second: 3120, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.104 [-0.426, 1.340], mean_best_reward: --\n",
      " 68114/100000: episode: 1735, duration: 0.015s, episode steps: 51, steps per second: 3417, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.151 [-0.562, 1.149], mean_best_reward: --\n",
      " 68168/100000: episode: 1736, duration: 0.015s, episode steps: 54, steps per second: 3667, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.186 [-0.560, 1.255], mean_best_reward: --\n",
      " 68202/100000: episode: 1737, duration: 0.011s, episode steps: 34, steps per second: 3230, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.131 [-1.231, 0.775], mean_best_reward: --\n",
      " 68217/100000: episode: 1738, duration: 0.005s, episode steps: 15, steps per second: 2957, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.102 [-0.601, 1.387], mean_best_reward: --\n",
      " 68240/100000: episode: 1739, duration: 0.007s, episode steps: 23, steps per second: 3106, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.080 [-0.638, 0.933], mean_best_reward: --\n",
      " 68274/100000: episode: 1740, duration: 0.010s, episode steps: 34, steps per second: 3266, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.090 [-0.881, 0.617], mean_best_reward: --\n",
      " 68316/100000: episode: 1741, duration: 0.016s, episode steps: 42, steps per second: 2644, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.147 [-0.408, 1.189], mean_best_reward: --\n",
      " 68335/100000: episode: 1742, duration: 0.010s, episode steps: 19, steps per second: 1893, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.042 [-0.994, 1.507], mean_best_reward: --\n",
      " 68354/100000: episode: 1743, duration: 0.009s, episode steps: 19, steps per second: 2046, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.077 [-1.361, 2.303], mean_best_reward: --\n",
      " 68390/100000: episode: 1744, duration: 0.012s, episode steps: 36, steps per second: 2921, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.044 [-1.145, 1.878], mean_best_reward: --\n",
      " 68403/100000: episode: 1745, duration: 0.005s, episode steps: 13, steps per second: 2580, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-0.819, 1.372], mean_best_reward: --\n",
      " 68464/100000: episode: 1746, duration: 0.017s, episode steps: 61, steps per second: 3527, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.051 [-1.481, 1.399], mean_best_reward: --\n",
      " 68529/100000: episode: 1747, duration: 0.019s, episode steps: 65, steps per second: 3437, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.007 [-1.272, 0.810], mean_best_reward: --\n",
      " 68549/100000: episode: 1748, duration: 0.006s, episode steps: 20, steps per second: 3120, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.099 [-0.433, 1.236], mean_best_reward: --\n",
      " 68583/100000: episode: 1749, duration: 0.010s, episode steps: 34, steps per second: 3364, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.735, 1.133], mean_best_reward: --\n",
      " 68606/100000: episode: 1750, duration: 0.007s, episode steps: 23, steps per second: 3211, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.072 [-0.978, 1.528], mean_best_reward: --\n",
      " 68633/100000: episode: 1751, duration: 0.009s, episode steps: 27, steps per second: 3155, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.108 [-1.748, 0.820], mean_best_reward: 85.000000\n",
      " 68660/100000: episode: 1752, duration: 0.009s, episode steps: 27, steps per second: 3158, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.091 [-0.580, 1.142], mean_best_reward: --\n",
      " 68704/100000: episode: 1753, duration: 0.013s, episode steps: 44, steps per second: 3277, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.088 [-0.829, 1.153], mean_best_reward: --\n",
      " 68715/100000: episode: 1754, duration: 0.004s, episode steps: 11, steps per second: 2548, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.104 [-1.349, 2.090], mean_best_reward: --\n",
      " 68767/100000: episode: 1755, duration: 0.016s, episode steps: 52, steps per second: 3156, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.024 [-1.138, 1.558], mean_best_reward: --\n",
      " 68782/100000: episode: 1756, duration: 0.005s, episode steps: 15, steps per second: 3031, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.112 [-0.802, 1.572], mean_best_reward: --\n",
      " 68809/100000: episode: 1757, duration: 0.008s, episode steps: 27, steps per second: 3263, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.112 [-0.599, 1.742], mean_best_reward: --\n",
      " 68823/100000: episode: 1758, duration: 0.005s, episode steps: 14, steps per second: 2823, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.618, 1.068], mean_best_reward: --\n",
      " 68856/100000: episode: 1759, duration: 0.010s, episode steps: 33, steps per second: 3371, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.085 [-0.983, 1.697], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68872/100000: episode: 1760, duration: 0.006s, episode steps: 16, steps per second: 2642, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.360, 0.740], mean_best_reward: --\n",
      " 68904/100000: episode: 1761, duration: 0.015s, episode steps: 32, steps per second: 2188, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.087 [-0.396, 1.388], mean_best_reward: --\n",
      " 68918/100000: episode: 1762, duration: 0.005s, episode steps: 14, steps per second: 2546, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.079 [-1.014, 1.599], mean_best_reward: --\n",
      " 68929/100000: episode: 1763, duration: 0.004s, episode steps: 11, steps per second: 2582, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.114 [-0.823, 1.342], mean_best_reward: --\n",
      " 68991/100000: episode: 1764, duration: 0.031s, episode steps: 62, steps per second: 1981, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.102, 1.003], mean_best_reward: --\n",
      " 69010/100000: episode: 1765, duration: 0.008s, episode steps: 19, steps per second: 2477, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.091 [-0.816, 1.409], mean_best_reward: --\n",
      " 69037/100000: episode: 1766, duration: 0.013s, episode steps: 27, steps per second: 2006, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.092 [-0.570, 1.234], mean_best_reward: --\n",
      " 69078/100000: episode: 1767, duration: 0.015s, episode steps: 41, steps per second: 2768, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.129 [-0.882, 0.441], mean_best_reward: --\n",
      " 69143/100000: episode: 1768, duration: 0.019s, episode steps: 65, steps per second: 3392, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.057 [-1.317, 1.226], mean_best_reward: --\n",
      " 69179/100000: episode: 1769, duration: 0.011s, episode steps: 36, steps per second: 3261, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.032 [-0.919, 1.265], mean_best_reward: --\n",
      " 69202/100000: episode: 1770, duration: 0.007s, episode steps: 23, steps per second: 3146, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.012 [-1.007, 1.249], mean_best_reward: --\n",
      " 69215/100000: episode: 1771, duration: 0.005s, episode steps: 13, steps per second: 2721, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.103 [-1.612, 0.941], mean_best_reward: --\n",
      " 69287/100000: episode: 1772, duration: 0.023s, episode steps: 72, steps per second: 3116, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.171 [-0.941, 1.397], mean_best_reward: --\n",
      " 69324/100000: episode: 1773, duration: 0.011s, episode steps: 37, steps per second: 3462, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.012 [-1.016, 1.569], mean_best_reward: --\n",
      " 69355/100000: episode: 1774, duration: 0.010s, episode steps: 31, steps per second: 3204, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.108 [-0.358, 0.721], mean_best_reward: --\n",
      " 69389/100000: episode: 1775, duration: 0.011s, episode steps: 34, steps per second: 3127, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.843, 1.209], mean_best_reward: --\n",
      " 69453/100000: episode: 1776, duration: 0.023s, episode steps: 64, steps per second: 2825, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: -0.140 [-1.722, 1.540], mean_best_reward: --\n",
      " 69509/100000: episode: 1777, duration: 0.021s, episode steps: 56, steps per second: 2672, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.054 [-1.170, 1.156], mean_best_reward: --\n",
      " 69560/100000: episode: 1778, duration: 0.016s, episode steps: 51, steps per second: 3189, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.028 [-0.786, 1.135], mean_best_reward: --\n",
      " 69600/100000: episode: 1779, duration: 0.013s, episode steps: 40, steps per second: 3159, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.116 [-1.310, 0.803], mean_best_reward: --\n",
      " 69613/100000: episode: 1780, duration: 0.004s, episode steps: 13, steps per second: 2920, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.146 [-0.559, 1.386], mean_best_reward: --\n",
      " 69655/100000: episode: 1781, duration: 0.013s, episode steps: 42, steps per second: 3283, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.119 [-0.729, 1.050], mean_best_reward: --\n",
      " 69696/100000: episode: 1782, duration: 0.012s, episode steps: 41, steps per second: 3355, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.064 [-0.617, 1.256], mean_best_reward: --\n",
      " 69731/100000: episode: 1783, duration: 0.011s, episode steps: 35, steps per second: 3222, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.173 [-0.346, 0.972], mean_best_reward: --\n",
      " 69784/100000: episode: 1784, duration: 0.016s, episode steps: 53, steps per second: 3337, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.011 [-1.154, 1.303], mean_best_reward: --\n",
      " 69799/100000: episode: 1785, duration: 0.005s, episode steps: 15, steps per second: 2972, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.089 [-1.006, 0.609], mean_best_reward: --\n",
      " 69848/100000: episode: 1786, duration: 0.015s, episode steps: 49, steps per second: 3354, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.160 [-0.634, 1.392], mean_best_reward: --\n",
      " 69887/100000: episode: 1787, duration: 0.012s, episode steps: 39, steps per second: 3282, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.081 [-1.069, 0.725], mean_best_reward: --\n",
      " 69907/100000: episode: 1788, duration: 0.006s, episode steps: 20, steps per second: 3143, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.075 [-1.264, 0.594], mean_best_reward: --\n",
      " 69940/100000: episode: 1789, duration: 0.010s, episode steps: 33, steps per second: 3263, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.034 [-0.730, 1.167], mean_best_reward: --\n",
      " 69958/100000: episode: 1790, duration: 0.007s, episode steps: 18, steps per second: 2747, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.736, 1.259], mean_best_reward: --\n",
      " 70018/100000: episode: 1791, duration: 0.018s, episode steps: 60, steps per second: 3382, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.110 [-1.233, 0.592], mean_best_reward: --\n",
      " 70048/100000: episode: 1792, duration: 0.009s, episode steps: 30, steps per second: 3243, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.079 [-1.744, 1.028], mean_best_reward: --\n",
      " 70087/100000: episode: 1793, duration: 0.013s, episode steps: 39, steps per second: 2952, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.041 [-1.357, 1.308], mean_best_reward: --\n",
      " 70151/100000: episode: 1794, duration: 0.024s, episode steps: 64, steps per second: 2714, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.946, 1.105], mean_best_reward: --\n",
      " 70236/100000: episode: 1795, duration: 0.026s, episode steps: 85, steps per second: 3281, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.145 [-2.491, 2.321], mean_best_reward: --\n",
      " 70288/100000: episode: 1796, duration: 0.016s, episode steps: 52, steps per second: 3243, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-1.531, 0.624], mean_best_reward: --\n",
      " 70327/100000: episode: 1797, duration: 0.012s, episode steps: 39, steps per second: 3228, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.095 [-0.359, 0.878], mean_best_reward: --\n",
      " 70369/100000: episode: 1798, duration: 0.013s, episode steps: 42, steps per second: 3225, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.089 [-0.963, 0.503], mean_best_reward: --\n",
      " 70386/100000: episode: 1799, duration: 0.006s, episode steps: 17, steps per second: 2888, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.064 [-1.716, 1.177], mean_best_reward: --\n",
      " 70422/100000: episode: 1800, duration: 0.011s, episode steps: 36, steps per second: 3260, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.001 [-0.800, 0.994], mean_best_reward: --\n",
      " 70477/100000: episode: 1801, duration: 0.016s, episode steps: 55, steps per second: 3364, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.044 [-0.970, 1.777], mean_best_reward: 192.000000\n",
      " 70563/100000: episode: 1802, duration: 0.025s, episode steps: 86, steps per second: 3422, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.153 [-1.458, 1.047], mean_best_reward: --\n",
      " 70650/100000: episode: 1803, duration: 0.026s, episode steps: 87, steps per second: 3339, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.181 [-0.996, 1.145], mean_best_reward: --\n",
      " 70678/100000: episode: 1804, duration: 0.009s, episode steps: 28, steps per second: 3173, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.131, 0.567], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70743/100000: episode: 1805, duration: 0.021s, episode steps: 65, steps per second: 3084, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.178 [-0.717, 1.238], mean_best_reward: --\n",
      " 70768/100000: episode: 1806, duration: 0.008s, episode steps: 25, steps per second: 3172, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.052 [-0.832, 1.400], mean_best_reward: --\n",
      " 70800/100000: episode: 1807, duration: 0.011s, episode steps: 32, steps per second: 2952, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.052 [-1.069, 0.639], mean_best_reward: --\n",
      " 70821/100000: episode: 1808, duration: 0.011s, episode steps: 21, steps per second: 1829, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.054 [-1.178, 1.867], mean_best_reward: --\n",
      " 70880/100000: episode: 1809, duration: 0.018s, episode steps: 59, steps per second: 3261, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.050 [-0.813, 1.388], mean_best_reward: --\n",
      " 70897/100000: episode: 1810, duration: 0.006s, episode steps: 17, steps per second: 2962, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.107 [-1.396, 0.608], mean_best_reward: --\n",
      " 70973/100000: episode: 1811, duration: 0.023s, episode steps: 76, steps per second: 3298, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.027 [-1.427, 0.790], mean_best_reward: --\n",
      " 71099/100000: episode: 1812, duration: 0.038s, episode steps: 126, steps per second: 3351, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.362 [-1.818, 0.828], mean_best_reward: --\n",
      " 71116/100000: episode: 1813, duration: 0.006s, episode steps: 17, steps per second: 3014, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.086 [-1.527, 0.772], mean_best_reward: --\n",
      " 71141/100000: episode: 1814, duration: 0.009s, episode steps: 25, steps per second: 2884, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.067 [-0.795, 1.513], mean_best_reward: --\n",
      " 71170/100000: episode: 1815, duration: 0.010s, episode steps: 29, steps per second: 3030, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.030 [-0.820, 1.117], mean_best_reward: --\n",
      " 71212/100000: episode: 1816, duration: 0.014s, episode steps: 42, steps per second: 3069, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.121 [-0.353, 0.852], mean_best_reward: --\n",
      " 71235/100000: episode: 1817, duration: 0.008s, episode steps: 23, steps per second: 2832, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.067 [-0.771, 1.114], mean_best_reward: --\n",
      " 71260/100000: episode: 1818, duration: 0.009s, episode steps: 25, steps per second: 2858, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.054 [-0.936, 1.368], mean_best_reward: --\n",
      " 71302/100000: episode: 1819, duration: 0.014s, episode steps: 42, steps per second: 3097, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.055 [-1.337, 1.352], mean_best_reward: --\n",
      " 71348/100000: episode: 1820, duration: 0.015s, episode steps: 46, steps per second: 3010, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-1.022, 1.535], mean_best_reward: --\n",
      " 71383/100000: episode: 1821, duration: 0.016s, episode steps: 35, steps per second: 2217, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.099 [-0.546, 1.075], mean_best_reward: --\n",
      " 71401/100000: episode: 1822, duration: 0.010s, episode steps: 18, steps per second: 1757, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.110 [-1.127, 0.554], mean_best_reward: --\n",
      " 71486/100000: episode: 1823, duration: 0.026s, episode steps: 85, steps per second: 3237, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.204 [-1.492, 2.046], mean_best_reward: --\n",
      " 71562/100000: episode: 1824, duration: 0.023s, episode steps: 76, steps per second: 3362, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.046 [-1.254, 0.826], mean_best_reward: --\n",
      " 71582/100000: episode: 1825, duration: 0.007s, episode steps: 20, steps per second: 2980, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.564, 1.318], mean_best_reward: --\n",
      " 71601/100000: episode: 1826, duration: 0.007s, episode steps: 19, steps per second: 2740, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.073 [-0.588, 1.126], mean_best_reward: --\n",
      " 71645/100000: episode: 1827, duration: 0.014s, episode steps: 44, steps per second: 3078, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.572, 1.106], mean_best_reward: --\n",
      " 71698/100000: episode: 1828, duration: 0.017s, episode steps: 53, steps per second: 3200, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.125 [-0.725, 1.335], mean_best_reward: --\n",
      " 71768/100000: episode: 1829, duration: 0.021s, episode steps: 70, steps per second: 3294, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.046 [-1.103, 0.925], mean_best_reward: --\n",
      " 71801/100000: episode: 1830, duration: 0.010s, episode steps: 33, steps per second: 3287, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.120 [-0.465, 0.982], mean_best_reward: --\n",
      " 71845/100000: episode: 1831, duration: 0.013s, episode steps: 44, steps per second: 3309, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.094 [-0.402, 1.184], mean_best_reward: --\n",
      " 71871/100000: episode: 1832, duration: 0.009s, episode steps: 26, steps per second: 2926, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.439, 0.780], mean_best_reward: --\n",
      " 71956/100000: episode: 1833, duration: 0.027s, episode steps: 85, steps per second: 3152, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.050 [-1.099, 1.594], mean_best_reward: --\n",
      " 71979/100000: episode: 1834, duration: 0.010s, episode steps: 23, steps per second: 2307, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.094 [-0.788, 1.311], mean_best_reward: --\n",
      " 71994/100000: episode: 1835, duration: 0.008s, episode steps: 15, steps per second: 1811, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.085 [-1.206, 1.740], mean_best_reward: --\n",
      " 72039/100000: episode: 1836, duration: 0.016s, episode steps: 45, steps per second: 2738, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.079 [-0.571, 1.174], mean_best_reward: --\n",
      " 72075/100000: episode: 1837, duration: 0.012s, episode steps: 36, steps per second: 2996, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.097 [-0.750, 1.142], mean_best_reward: --\n",
      " 72095/100000: episode: 1838, duration: 0.007s, episode steps: 20, steps per second: 2851, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.028, 0.580], mean_best_reward: --\n",
      " 72123/100000: episode: 1839, duration: 0.010s, episode steps: 28, steps per second: 2841, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.059 [-0.761, 1.142], mean_best_reward: --\n",
      " 72143/100000: episode: 1840, duration: 0.007s, episode steps: 20, steps per second: 2751, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.091 [-0.807, 1.357], mean_best_reward: --\n",
      " 72169/100000: episode: 1841, duration: 0.009s, episode steps: 26, steps per second: 2904, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.054 [-1.558, 0.992], mean_best_reward: --\n",
      " 72211/100000: episode: 1842, duration: 0.014s, episode steps: 42, steps per second: 2956, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.051 [-1.205, 0.579], mean_best_reward: --\n",
      " 72255/100000: episode: 1843, duration: 0.014s, episode steps: 44, steps per second: 3193, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.102 [-0.878, 1.179], mean_best_reward: --\n",
      " 72305/100000: episode: 1844, duration: 0.016s, episode steps: 50, steps per second: 3032, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.152 [-0.695, 0.931], mean_best_reward: --\n",
      " 72317/100000: episode: 1845, duration: 0.005s, episode steps: 12, steps per second: 2229, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-0.807, 1.490], mean_best_reward: --\n",
      " 72438/100000: episode: 1846, duration: 0.037s, episode steps: 121, steps per second: 3295, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.060 [-1.490, 1.365], mean_best_reward: --\n",
      " 72491/100000: episode: 1847, duration: 0.016s, episode steps: 53, steps per second: 3315, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.097 [-0.672, 0.979], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72553/100000: episode: 1848, duration: 0.019s, episode steps: 62, steps per second: 3351, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.066 [-0.976, 1.251], mean_best_reward: --\n",
      " 72565/100000: episode: 1849, duration: 0.004s, episode steps: 12, steps per second: 2860, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.175, 1.940], mean_best_reward: --\n",
      " 72603/100000: episode: 1850, duration: 0.014s, episode steps: 38, steps per second: 2784, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.092 [-0.437, 1.149], mean_best_reward: --\n",
      " 72629/100000: episode: 1851, duration: 0.012s, episode steps: 26, steps per second: 2118, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.609, 0.990], mean_best_reward: 106.500000\n",
      " 72654/100000: episode: 1852, duration: 0.009s, episode steps: 25, steps per second: 2811, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.066 [-0.644, 1.528], mean_best_reward: --\n",
      " 72702/100000: episode: 1853, duration: 0.016s, episode steps: 48, steps per second: 3042, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.726, 1.238], mean_best_reward: --\n",
      " 72765/100000: episode: 1854, duration: 0.019s, episode steps: 63, steps per second: 3319, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.155 [-1.557, 0.903], mean_best_reward: --\n",
      " 72889/100000: episode: 1855, duration: 0.036s, episode steps: 124, steps per second: 3469, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.022 [-1.366, 1.348], mean_best_reward: --\n",
      " 72930/100000: episode: 1856, duration: 0.012s, episode steps: 41, steps per second: 3333, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.077 [-0.561, 1.060], mean_best_reward: --\n",
      " 72949/100000: episode: 1857, duration: 0.006s, episode steps: 19, steps per second: 3044, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.074 [-0.801, 1.289], mean_best_reward: --\n",
      " 72983/100000: episode: 1858, duration: 0.011s, episode steps: 34, steps per second: 3236, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.040 [-0.838, 1.499], mean_best_reward: --\n",
      " 73003/100000: episode: 1859, duration: 0.007s, episode steps: 20, steps per second: 2967, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.036 [-1.140, 1.620], mean_best_reward: --\n",
      " 73068/100000: episode: 1860, duration: 0.019s, episode steps: 65, steps per second: 3381, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.017 [-0.939, 0.607], mean_best_reward: --\n",
      " 73151/100000: episode: 1861, duration: 0.025s, episode steps: 83, steps per second: 3375, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.181 [-1.832, 0.996], mean_best_reward: --\n",
      " 73176/100000: episode: 1862, duration: 0.008s, episode steps: 25, steps per second: 3058, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.072 [-0.608, 1.151], mean_best_reward: --\n",
      " 73242/100000: episode: 1863, duration: 0.024s, episode steps: 66, steps per second: 2767, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: -0.175 [-2.507, 2.494], mean_best_reward: --\n",
      " 73272/100000: episode: 1864, duration: 0.010s, episode steps: 30, steps per second: 2903, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.062 [-0.576, 1.189], mean_best_reward: --\n",
      " 73302/100000: episode: 1865, duration: 0.016s, episode steps: 30, steps per second: 1839, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.123 [-0.578, 1.479], mean_best_reward: --\n",
      " 73336/100000: episode: 1866, duration: 0.028s, episode steps: 34, steps per second: 1224, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.059 [-0.738, 1.197], mean_best_reward: --\n",
      " 73356/100000: episode: 1867, duration: 0.018s, episode steps: 20, steps per second: 1123, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.460, 1.000], mean_best_reward: --\n",
      " 73400/100000: episode: 1868, duration: 0.033s, episode steps: 44, steps per second: 1318, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: -0.046 [-1.210, 0.601], mean_best_reward: --\n",
      " 73425/100000: episode: 1869, duration: 0.019s, episode steps: 25, steps per second: 1291, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.081 [-1.974, 1.004], mean_best_reward: --\n",
      " 73467/100000: episode: 1870, duration: 0.032s, episode steps: 42, steps per second: 1314, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.157 [-1.864, 0.847], mean_best_reward: --\n",
      " 73508/100000: episode: 1871, duration: 0.028s, episode steps: 41, steps per second: 1467, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.127 [-0.507, 1.521], mean_best_reward: --\n",
      " 73589/100000: episode: 1872, duration: 0.035s, episode steps: 81, steps per second: 2302, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.192 [-1.646, 1.206], mean_best_reward: --\n",
      " 73634/100000: episode: 1873, duration: 0.014s, episode steps: 45, steps per second: 3264, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.115 [-1.422, 0.766], mean_best_reward: --\n",
      " 73649/100000: episode: 1874, duration: 0.005s, episode steps: 15, steps per second: 2900, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.106 [-0.618, 1.134], mean_best_reward: --\n",
      " 73685/100000: episode: 1875, duration: 0.011s, episode steps: 36, steps per second: 3261, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.762, 1.253], mean_best_reward: --\n",
      " 73755/100000: episode: 1876, duration: 0.020s, episode steps: 70, steps per second: 3427, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.100 [-1.059, 0.972], mean_best_reward: --\n",
      " 73798/100000: episode: 1877, duration: 0.013s, episode steps: 43, steps per second: 3340, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.141 [-0.642, 1.273], mean_best_reward: --\n",
      " 73830/100000: episode: 1878, duration: 0.010s, episode steps: 32, steps per second: 3263, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.077 [-0.817, 1.335], mean_best_reward: --\n",
      " 73893/100000: episode: 1879, duration: 0.018s, episode steps: 63, steps per second: 3410, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.027 [-1.065, 0.628], mean_best_reward: --\n",
      " 73929/100000: episode: 1880, duration: 0.011s, episode steps: 36, steps per second: 3249, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.007 [-0.840, 1.552], mean_best_reward: --\n",
      " 73984/100000: episode: 1881, duration: 0.017s, episode steps: 55, steps per second: 3266, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.174 [-1.247, 0.822], mean_best_reward: --\n",
      " 74035/100000: episode: 1882, duration: 0.016s, episode steps: 51, steps per second: 3289, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.012 [-0.950, 1.225], mean_best_reward: --\n",
      " 74061/100000: episode: 1883, duration: 0.008s, episode steps: 26, steps per second: 3179, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.066 [-1.344, 0.739], mean_best_reward: --\n",
      " 74102/100000: episode: 1884, duration: 0.013s, episode steps: 41, steps per second: 3094, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.078 [-0.750, 1.095], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74126/100000: episode: 1885, duration: 0.008s, episode steps: 24, steps per second: 2924, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.607, 1.251], mean_best_reward: --\n",
      " 74173/100000: episode: 1886, duration: 0.016s, episode steps: 47, steps per second: 2963, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.093 [-0.486, 1.348], mean_best_reward: --\n",
      " 74205/100000: episode: 1887, duration: 0.013s, episode steps: 32, steps per second: 2373, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.415, 1.245], mean_best_reward: --\n",
      " 74220/100000: episode: 1888, duration: 0.006s, episode steps: 15, steps per second: 2643, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.104 [-0.746, 1.303], mean_best_reward: --\n",
      " 74238/100000: episode: 1889, duration: 0.006s, episode steps: 18, steps per second: 2959, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.062 [-1.462, 1.018], mean_best_reward: --\n",
      " 74251/100000: episode: 1890, duration: 0.005s, episode steps: 13, steps per second: 2654, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.131 [-1.356, 0.595], mean_best_reward: --\n",
      " 74298/100000: episode: 1891, duration: 0.015s, episode steps: 47, steps per second: 3217, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.043 [-0.760, 1.201], mean_best_reward: --\n",
      " 74326/100000: episode: 1892, duration: 0.009s, episode steps: 28, steps per second: 3230, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.070 [-1.433, 1.002], mean_best_reward: --\n",
      " 74341/100000: episode: 1893, duration: 0.005s, episode steps: 15, steps per second: 2912, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.083 [-1.293, 0.760], mean_best_reward: --\n",
      " 74370/100000: episode: 1894, duration: 0.009s, episode steps: 29, steps per second: 3194, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.105 [-0.758, 1.726], mean_best_reward: --\n",
      " 74433/100000: episode: 1895, duration: 0.019s, episode steps: 63, steps per second: 3401, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.037 [-1.311, 1.256], mean_best_reward: --\n",
      " 74520/100000: episode: 1896, duration: 0.025s, episode steps: 87, steps per second: 3478, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.073 [-0.776, 1.206], mean_best_reward: --\n",
      " 74573/100000: episode: 1897, duration: 0.016s, episode steps: 53, steps per second: 3364, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.185 [-1.090, 0.681], mean_best_reward: --\n",
      " 74609/100000: episode: 1898, duration: 0.011s, episode steps: 36, steps per second: 3291, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-0.584, 0.972], mean_best_reward: --\n",
      " 74628/100000: episode: 1899, duration: 0.006s, episode steps: 19, steps per second: 3007, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.050 [-1.014, 1.409], mean_best_reward: --\n",
      " 74655/100000: episode: 1900, duration: 0.009s, episode steps: 27, steps per second: 3092, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.107 [-1.358, 0.503], mean_best_reward: --\n",
      " 74688/100000: episode: 1901, duration: 0.011s, episode steps: 33, steps per second: 3117, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.096 [-0.731, 1.570], mean_best_reward: 108.500000\n",
      " 74720/100000: episode: 1902, duration: 0.010s, episode steps: 32, steps per second: 3218, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.085 [-0.710, 1.366], mean_best_reward: --\n",
      " 74749/100000: episode: 1903, duration: 0.010s, episode steps: 29, steps per second: 3047, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.102 [-0.633, 1.008], mean_best_reward: --\n",
      " 74776/100000: episode: 1904, duration: 0.010s, episode steps: 27, steps per second: 2601, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.054 [-2.047, 1.229], mean_best_reward: --\n",
      " 74797/100000: episode: 1905, duration: 0.009s, episode steps: 21, steps per second: 2226, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.082 [-0.789, 1.610], mean_best_reward: --\n",
      " 74824/100000: episode: 1906, duration: 0.009s, episode steps: 27, steps per second: 2974, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.052 [-0.605, 1.136], mean_best_reward: --\n",
      " 74854/100000: episode: 1907, duration: 0.009s, episode steps: 30, steps per second: 3177, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.026 [-1.115, 1.596], mean_best_reward: --\n",
      " 74875/100000: episode: 1908, duration: 0.007s, episode steps: 21, steps per second: 3043, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.117 [-0.579, 0.979], mean_best_reward: --\n",
      " 74929/100000: episode: 1909, duration: 0.016s, episode steps: 54, steps per second: 3311, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-1.000, 0.776], mean_best_reward: --\n",
      " 74963/100000: episode: 1910, duration: 0.011s, episode steps: 34, steps per second: 3207, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.109 [-0.771, 1.216], mean_best_reward: --\n",
      " 74989/100000: episode: 1911, duration: 0.008s, episode steps: 26, steps per second: 3181, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.092 [-1.120, 0.750], mean_best_reward: --\n",
      " 75007/100000: episode: 1912, duration: 0.006s, episode steps: 18, steps per second: 2998, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.063 [-0.964, 1.587], mean_best_reward: --\n",
      " 75020/100000: episode: 1913, duration: 0.004s, episode steps: 13, steps per second: 3047, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.087 [-1.345, 1.995], mean_best_reward: --\n",
      " 75105/100000: episode: 1914, duration: 0.024s, episode steps: 85, steps per second: 3472, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.062 [-1.170, 1.115], mean_best_reward: --\n",
      " 75132/100000: episode: 1915, duration: 0.009s, episode steps: 27, steps per second: 2973, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.117 [-0.994, 1.465], mean_best_reward: --\n",
      " 75167/100000: episode: 1916, duration: 0.011s, episode steps: 35, steps per second: 3223, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.146 [-0.922, 0.450], mean_best_reward: --\n",
      " 75240/100000: episode: 1917, duration: 0.023s, episode steps: 73, steps per second: 3241, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.016 [-1.027, 0.985], mean_best_reward: --\n",
      " 75374/100000: episode: 1918, duration: 0.039s, episode steps: 134, steps per second: 3430, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.043 [-0.985, 1.069], mean_best_reward: --\n",
      " 75410/100000: episode: 1919, duration: 0.014s, episode steps: 36, steps per second: 2604, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.001 [-0.909, 1.220], mean_best_reward: --\n",
      " 75437/100000: episode: 1920, duration: 0.009s, episode steps: 27, steps per second: 2943, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.073 [-0.925, 1.445], mean_best_reward: --\n",
      " 75468/100000: episode: 1921, duration: 0.014s, episode steps: 31, steps per second: 2215, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.098 [-0.620, 1.314], mean_best_reward: --\n",
      " 75480/100000: episode: 1922, duration: 0.005s, episode steps: 12, steps per second: 2478, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.108 [-1.193, 1.992], mean_best_reward: --\n",
      " 75503/100000: episode: 1923, duration: 0.008s, episode steps: 23, steps per second: 2841, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.118 [-0.427, 0.990], mean_best_reward: --\n",
      " 75561/100000: episode: 1924, duration: 0.017s, episode steps: 58, steps per second: 3376, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.084 [-1.665, 0.657], mean_best_reward: --\n",
      " 75730/100000: episode: 1925, duration: 0.048s, episode steps: 169, steps per second: 3528, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.096 [-1.022, 1.006], mean_best_reward: --\n",
      " 75777/100000: episode: 1926, duration: 0.014s, episode steps: 47, steps per second: 3337, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.137 [-1.089, 0.617], mean_best_reward: --\n",
      " 75825/100000: episode: 1927, duration: 0.015s, episode steps: 48, steps per second: 3305, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.055 [-0.833, 1.681], mean_best_reward: --\n",
      " 75854/100000: episode: 1928, duration: 0.010s, episode steps: 29, steps per second: 3029, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.076 [-1.172, 1.907], mean_best_reward: --\n",
      " 75875/100000: episode: 1929, duration: 0.007s, episode steps: 21, steps per second: 3034, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.053 [-0.756, 1.148], mean_best_reward: --\n",
      " 75907/100000: episode: 1930, duration: 0.010s, episode steps: 32, steps per second: 3168, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.132 [-0.911, 0.569], mean_best_reward: --\n",
      " 75973/100000: episode: 1931, duration: 0.020s, episode steps: 66, steps per second: 3381, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.141 [-1.316, 0.725], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76064/100000: episode: 1932, duration: 0.028s, episode steps: 91, steps per second: 3239, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.009 [-1.071, 1.073], mean_best_reward: --\n",
      " 76128/100000: episode: 1933, duration: 0.025s, episode steps: 64, steps per second: 2581, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.071 [-1.631, 0.752], mean_best_reward: --\n",
      " 76247/100000: episode: 1934, duration: 0.038s, episode steps: 119, steps per second: 3162, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.250 [-2.040, 0.813], mean_best_reward: --\n",
      " 76383/100000: episode: 1935, duration: 0.039s, episode steps: 136, steps per second: 3523, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.196 [-1.683, 1.216], mean_best_reward: --\n",
      " 76398/100000: episode: 1936, duration: 0.006s, episode steps: 15, steps per second: 2675, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.081 [-1.449, 0.976], mean_best_reward: --\n",
      " 76435/100000: episode: 1937, duration: 0.012s, episode steps: 37, steps per second: 3200, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.092 [-0.778, 1.052], mean_best_reward: --\n",
      " 76468/100000: episode: 1938, duration: 0.010s, episode steps: 33, steps per second: 3267, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.099 [-0.400, 1.129], mean_best_reward: --\n",
      " 76541/100000: episode: 1939, duration: 0.022s, episode steps: 73, steps per second: 3294, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.148 [-0.903, 1.007], mean_best_reward: --\n",
      " 76616/100000: episode: 1940, duration: 0.022s, episode steps: 75, steps per second: 3362, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.262 [-1.457, 0.603], mean_best_reward: --\n",
      " 76662/100000: episode: 1941, duration: 0.014s, episode steps: 46, steps per second: 3299, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.109 [-0.623, 1.237], mean_best_reward: --\n",
      " 76702/100000: episode: 1942, duration: 0.013s, episode steps: 40, steps per second: 3185, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.034 [-0.595, 1.113], mean_best_reward: --\n",
      " 76729/100000: episode: 1943, duration: 0.011s, episode steps: 27, steps per second: 2501, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.121 [-0.379, 0.866], mean_best_reward: --\n",
      " 76758/100000: episode: 1944, duration: 0.012s, episode steps: 29, steps per second: 2400, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: 0.016 [-1.862, 1.510], mean_best_reward: --\n",
      " 76768/100000: episode: 1945, duration: 0.004s, episode steps: 10, steps per second: 2764, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.140 [-0.938, 1.571], mean_best_reward: --\n",
      " 76831/100000: episode: 1946, duration: 0.019s, episode steps: 63, steps per second: 3340, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.103 [-1.032, 1.153], mean_best_reward: --\n",
      " 76943/100000: episode: 1947, duration: 0.032s, episode steps: 112, steps per second: 3479, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.082 [-1.510, 1.323], mean_best_reward: --\n",
      " 76970/100000: episode: 1948, duration: 0.007s, episode steps: 27, steps per second: 3633, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.045 [-1.008, 1.746], mean_best_reward: --\n",
      " 77066/100000: episode: 1949, duration: 0.028s, episode steps: 96, steps per second: 3488, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.143 [-1.031, 1.345], mean_best_reward: --\n",
      " 77103/100000: episode: 1950, duration: 0.011s, episode steps: 37, steps per second: 3311, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.082 [-1.390, 0.571], mean_best_reward: --\n",
      " 77163/100000: episode: 1951, duration: 0.017s, episode steps: 60, steps per second: 3498, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.109 [-0.439, 1.314], mean_best_reward: 121.500000\n",
      " 77256/100000: episode: 1952, duration: 0.029s, episode steps: 93, steps per second: 3206, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.163 [-0.923, 0.820], mean_best_reward: --\n",
      " 77275/100000: episode: 1953, duration: 0.007s, episode steps: 19, steps per second: 2833, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.103 [-0.807, 1.229], mean_best_reward: --\n",
      " 77335/100000: episode: 1954, duration: 0.022s, episode steps: 60, steps per second: 2734, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.088 [-1.531, 1.447], mean_best_reward: --\n",
      " 77355/100000: episode: 1955, duration: 0.014s, episode steps: 20, steps per second: 1415, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.058 [-0.638, 1.278], mean_best_reward: --\n",
      " 77380/100000: episode: 1956, duration: 0.011s, episode steps: 25, steps per second: 2232, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.103 [-1.154, 0.608], mean_best_reward: --\n",
      " 77397/100000: episode: 1957, duration: 0.007s, episode steps: 17, steps per second: 2304, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.094 [-0.819, 1.484], mean_best_reward: --\n",
      " 77418/100000: episode: 1958, duration: 0.008s, episode steps: 21, steps per second: 2614, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.088 [-0.563, 0.914], mean_best_reward: --\n",
      " 77450/100000: episode: 1959, duration: 0.010s, episode steps: 32, steps per second: 3058, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-1.180, 0.825], mean_best_reward: --\n",
      " 77580/100000: episode: 1960, duration: 0.038s, episode steps: 130, steps per second: 3400, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.108 [-1.178, 1.971], mean_best_reward: --\n",
      " 77591/100000: episode: 1961, duration: 0.004s, episode steps: 11, steps per second: 2520, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.141 [-0.759, 1.501], mean_best_reward: --\n",
      " 77614/100000: episode: 1962, duration: 0.008s, episode steps: 23, steps per second: 2911, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.062 [-1.191, 1.978], mean_best_reward: --\n",
      " 77695/100000: episode: 1963, duration: 0.024s, episode steps: 81, steps per second: 3439, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.062 [-0.884, 1.508], mean_best_reward: --\n",
      " 77805/100000: episode: 1964, duration: 0.032s, episode steps: 110, steps per second: 3471, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: -0.198 [-1.735, 1.260], mean_best_reward: --\n",
      " 77824/100000: episode: 1965, duration: 0.006s, episode steps: 19, steps per second: 2948, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.042 [-1.426, 0.998], mean_best_reward: --\n",
      " 77846/100000: episode: 1966, duration: 0.007s, episode steps: 22, steps per second: 3049, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.057 [-0.607, 1.123], mean_best_reward: --\n",
      " 77879/100000: episode: 1967, duration: 0.011s, episode steps: 33, steps per second: 3024, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.057 [-0.848, 1.642], mean_best_reward: --\n",
      " 77891/100000: episode: 1968, duration: 0.004s, episode steps: 12, steps per second: 2760, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.125 [-0.755, 1.272], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77927/100000: episode: 1969, duration: 0.011s, episode steps: 36, steps per second: 3241, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.046 [-0.848, 1.774], mean_best_reward: --\n",
      " 77948/100000: episode: 1970, duration: 0.008s, episode steps: 21, steps per second: 2710, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.082 [-0.779, 1.249], mean_best_reward: --\n",
      " 78003/100000: episode: 1971, duration: 0.022s, episode steps: 55, steps per second: 2470, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.073 [-0.531, 1.443], mean_best_reward: --\n",
      " 78029/100000: episode: 1972, duration: 0.009s, episode steps: 26, steps per second: 2843, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.066 [-0.599, 1.253], mean_best_reward: --\n",
      " 78053/100000: episode: 1973, duration: 0.011s, episode steps: 24, steps per second: 2207, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.039 [-1.116, 0.797], mean_best_reward: --\n",
      " 78087/100000: episode: 1974, duration: 0.011s, episode steps: 34, steps per second: 3031, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.102 [-0.518, 1.045], mean_best_reward: --\n",
      " 78103/100000: episode: 1975, duration: 0.005s, episode steps: 16, steps per second: 3024, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.051 [-1.213, 1.704], mean_best_reward: --\n",
      " 78124/100000: episode: 1976, duration: 0.007s, episode steps: 21, steps per second: 3017, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.598, 0.984], mean_best_reward: --\n",
      " 78149/100000: episode: 1977, duration: 0.008s, episode steps: 25, steps per second: 3059, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.075 [-0.823, 1.430], mean_best_reward: --\n",
      " 78165/100000: episode: 1978, duration: 0.005s, episode steps: 16, steps per second: 2957, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.827, 1.278], mean_best_reward: --\n",
      " 78208/100000: episode: 1979, duration: 0.013s, episode steps: 43, steps per second: 3272, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: 0.027 [-0.986, 1.555], mean_best_reward: --\n",
      " 78238/100000: episode: 1980, duration: 0.009s, episode steps: 30, steps per second: 3243, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.055 [-0.926, 1.318], mean_best_reward: --\n",
      " 78257/100000: episode: 1981, duration: 0.006s, episode steps: 19, steps per second: 3066, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.111 [-0.773, 1.248], mean_best_reward: --\n",
      " 78301/100000: episode: 1982, duration: 0.013s, episode steps: 44, steps per second: 3279, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.183 [-0.630, 1.242], mean_best_reward: --\n",
      " 78340/100000: episode: 1983, duration: 0.012s, episode steps: 39, steps per second: 3275, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.108 [-1.110, 0.550], mean_best_reward: --\n",
      " 78407/100000: episode: 1984, duration: 0.020s, episode steps: 67, steps per second: 3408, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.239 [-0.770, 1.495], mean_best_reward: --\n",
      " 78427/100000: episode: 1985, duration: 0.007s, episode steps: 20, steps per second: 3002, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.090 [-0.756, 1.446], mean_best_reward: --\n",
      " 78452/100000: episode: 1986, duration: 0.008s, episode steps: 25, steps per second: 3139, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.074 [-2.279, 1.341], mean_best_reward: --\n",
      " 78565/100000: episode: 1987, duration: 0.035s, episode steps: 113, steps per second: 3233, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.113 [-1.307, 1.335], mean_best_reward: --\n",
      " 78593/100000: episode: 1988, duration: 0.013s, episode steps: 28, steps per second: 2212, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.104 [-0.786, 1.763], mean_best_reward: --\n",
      " 78629/100000: episode: 1989, duration: 0.012s, episode steps: 36, steps per second: 3035, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.059 [-1.894, 0.837], mean_best_reward: --\n",
      " 78672/100000: episode: 1990, duration: 0.013s, episode steps: 43, steps per second: 3212, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.141 [-0.742, 1.591], mean_best_reward: --\n",
      " 78706/100000: episode: 1991, duration: 0.010s, episode steps: 34, steps per second: 3270, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.046 [-0.620, 1.402], mean_best_reward: --\n",
      " 78731/100000: episode: 1992, duration: 0.008s, episode steps: 25, steps per second: 3078, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.065 [-0.649, 1.347], mean_best_reward: --\n",
      " 78764/100000: episode: 1993, duration: 0.011s, episode steps: 33, steps per second: 3100, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.090 [-0.958, 1.943], mean_best_reward: --\n",
      " 78785/100000: episode: 1994, duration: 0.007s, episode steps: 21, steps per second: 3001, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.048 [-0.836, 1.370], mean_best_reward: --\n",
      " 78808/100000: episode: 1995, duration: 0.007s, episode steps: 23, steps per second: 3125, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.088 [-0.390, 0.910], mean_best_reward: --\n",
      " 78829/100000: episode: 1996, duration: 0.007s, episode steps: 21, steps per second: 2989, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.060 [-1.942, 1.211], mean_best_reward: --\n",
      " 78846/100000: episode: 1997, duration: 0.006s, episode steps: 17, steps per second: 2992, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.108 [-0.778, 1.347], mean_best_reward: --\n",
      " 78867/100000: episode: 1998, duration: 0.007s, episode steps: 21, steps per second: 2994, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.104 [-0.794, 1.209], mean_best_reward: --\n",
      " 78944/100000: episode: 1999, duration: 0.023s, episode steps: 77, steps per second: 3371, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.075 [-1.076, 0.619], mean_best_reward: --\n",
      " 78968/100000: episode: 2000, duration: 0.008s, episode steps: 24, steps per second: 3095, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.085 [-0.577, 1.179], mean_best_reward: --\n",
      " 79002/100000: episode: 2001, duration: 0.011s, episode steps: 34, steps per second: 3068, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.550, 1.251], mean_best_reward: 114.500000\n",
      " 79062/100000: episode: 2002, duration: 0.018s, episode steps: 60, steps per second: 3375, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.066 [-0.903, 0.764], mean_best_reward: --\n",
      " 79108/100000: episode: 2003, duration: 0.014s, episode steps: 46, steps per second: 3301, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.131, 0.467], mean_best_reward: --\n",
      " 79171/100000: episode: 2004, duration: 0.019s, episode steps: 63, steps per second: 3388, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.138 [-0.587, 1.361], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 79227/100000: episode: 2005, duration: 0.023s, episode steps: 56, steps per second: 2420, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.112 [-0.617, 1.189], mean_best_reward: --\n",
      " 79247/100000: episode: 2006, duration: 0.010s, episode steps: 20, steps per second: 2009, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.058 [-1.125, 1.754], mean_best_reward: --\n",
      " 79274/100000: episode: 2007, duration: 0.009s, episode steps: 27, steps per second: 2979, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.060 [-0.790, 1.161], mean_best_reward: --\n",
      " 79302/100000: episode: 2008, duration: 0.009s, episode steps: 28, steps per second: 3051, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.070 [-1.535, 0.802], mean_best_reward: --\n",
      " 79323/100000: episode: 2009, duration: 0.007s, episode steps: 21, steps per second: 2996, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.089 [-0.626, 1.492], mean_best_reward: --\n",
      " 79346/100000: episode: 2010, duration: 0.008s, episode steps: 23, steps per second: 2988, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.077 [-0.766, 1.270], mean_best_reward: --\n",
      " 79388/100000: episode: 2011, duration: 0.013s, episode steps: 42, steps per second: 3176, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.587, 1.400], mean_best_reward: --\n",
      " 79409/100000: episode: 2012, duration: 0.007s, episode steps: 21, steps per second: 3012, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.123 [-0.558, 1.013], mean_best_reward: --\n",
      " 79499/100000: episode: 2013, duration: 0.025s, episode steps: 90, steps per second: 3537, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.169 [-0.961, 1.497], mean_best_reward: --\n",
      " 79509/100000: episode: 2014, duration: 0.004s, episode steps: 10, steps per second: 2669, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.133 [-0.956, 1.589], mean_best_reward: --\n",
      " 79550/100000: episode: 2015, duration: 0.013s, episode steps: 41, steps per second: 3263, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.058 [-0.432, 1.059], mean_best_reward: --\n",
      " 79651/100000: episode: 2016, duration: 0.029s, episode steps: 101, steps per second: 3501, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.208 [-1.663, 1.262], mean_best_reward: --\n",
      " 79740/100000: episode: 2017, duration: 0.027s, episode steps: 89, steps per second: 3279, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.201 [-1.510, 1.051], mean_best_reward: --\n",
      " 79759/100000: episode: 2018, duration: 0.009s, episode steps: 19, steps per second: 2088, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.069 [-0.827, 1.222], mean_best_reward: --\n",
      " 79778/100000: episode: 2019, duration: 0.007s, episode steps: 19, steps per second: 2714, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.094 [-1.163, 0.752], mean_best_reward: --\n",
      " 79823/100000: episode: 2020, duration: 0.026s, episode steps: 45, steps per second: 1713, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.081 [-1.236, 0.779], mean_best_reward: --\n",
      " 79842/100000: episode: 2021, duration: 0.010s, episode steps: 19, steps per second: 1811, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.079 [-0.791, 1.432], mean_best_reward: --\n",
      " 79871/100000: episode: 2022, duration: 0.012s, episode steps: 29, steps per second: 2385, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.109 [-0.572, 1.065], mean_best_reward: --\n",
      " 79886/100000: episode: 2023, duration: 0.010s, episode steps: 15, steps per second: 1570, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.052 [-1.030, 1.569], mean_best_reward: --\n",
      " 79913/100000: episode: 2024, duration: 0.013s, episode steps: 27, steps per second: 2111, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.100 [-0.991, 0.558], mean_best_reward: --\n",
      " 80024/100000: episode: 2025, duration: 0.040s, episode steps: 111, steps per second: 2742, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.175 [-0.969, 1.956], mean_best_reward: --\n",
      " 80040/100000: episode: 2026, duration: 0.007s, episode steps: 16, steps per second: 2448, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.101 [-0.956, 1.464], mean_best_reward: --\n",
      " 80056/100000: episode: 2027, duration: 0.006s, episode steps: 16, steps per second: 2571, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.617, 1.000], mean_best_reward: --\n",
      " 80095/100000: episode: 2028, duration: 0.013s, episode steps: 39, steps per second: 3105, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.037 [-0.748, 1.070], mean_best_reward: --\n",
      " 80142/100000: episode: 2029, duration: 0.014s, episode steps: 47, steps per second: 3464, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.117 [-1.058, 0.587], mean_best_reward: --\n",
      " 80172/100000: episode: 2030, duration: 0.008s, episode steps: 30, steps per second: 3648, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.567 [0.000, 1.000], mean observation: -0.081 [-1.813, 0.941], mean_best_reward: --\n",
      " 80188/100000: episode: 2031, duration: 0.005s, episode steps: 16, steps per second: 3025, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.117 [-1.165, 2.171], mean_best_reward: --\n",
      " 80298/100000: episode: 2032, duration: 0.032s, episode steps: 110, steps per second: 3449, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.200 [-1.651, 1.333], mean_best_reward: --\n",
      " 80343/100000: episode: 2033, duration: 0.014s, episode steps: 45, steps per second: 3183, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.195 [-0.488, 1.022], mean_best_reward: --\n",
      " 80369/100000: episode: 2034, duration: 0.008s, episode steps: 26, steps per second: 3185, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.149 [-0.733, 1.268], mean_best_reward: --\n",
      " 80431/100000: episode: 2035, duration: 0.020s, episode steps: 62, steps per second: 3050, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.130 [-0.628, 1.146], mean_best_reward: --\n",
      " 80443/100000: episode: 2036, duration: 0.005s, episode steps: 12, steps per second: 2658, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.098 [-0.978, 1.503], mean_best_reward: --\n",
      " 80454/100000: episode: 2037, duration: 0.007s, episode steps: 11, steps per second: 1483, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.120 [-0.979, 1.584], mean_best_reward: --\n",
      " 80495/100000: episode: 2038, duration: 0.013s, episode steps: 41, steps per second: 3133, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.063 [-0.997, 1.595], mean_best_reward: --\n",
      " 80571/100000: episode: 2039, duration: 0.023s, episode steps: 76, steps per second: 3242, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.021 [-1.100, 1.827], mean_best_reward: --\n",
      " 80592/100000: episode: 2040, duration: 0.007s, episode steps: 21, steps per second: 3098, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.103 [-0.775, 1.613], mean_best_reward: --\n",
      " 80628/100000: episode: 2041, duration: 0.011s, episode steps: 36, steps per second: 3231, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.064 [-0.599, 1.182], mean_best_reward: --\n",
      " 80653/100000: episode: 2042, duration: 0.008s, episode steps: 25, steps per second: 3079, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.080 [-1.196, 2.161], mean_best_reward: --\n",
      " 80742/100000: episode: 2043, duration: 0.025s, episode steps: 89, steps per second: 3497, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.151 [-1.310, 1.034], mean_best_reward: --\n",
      " 80792/100000: episode: 2044, duration: 0.016s, episode steps: 50, steps per second: 3200, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.012 [-0.646, 1.411], mean_best_reward: --\n",
      " 80809/100000: episode: 2045, duration: 0.006s, episode steps: 17, steps per second: 2803, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.077 [-1.193, 0.624], mean_best_reward: --\n",
      " 80861/100000: episode: 2046, duration: 0.015s, episode steps: 52, steps per second: 3424, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.170 [-0.485, 1.103], mean_best_reward: --\n",
      " 80894/100000: episode: 2047, duration: 0.010s, episode steps: 33, steps per second: 3343, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.049 [-1.030, 1.790], mean_best_reward: --\n",
      " 80938/100000: episode: 2048, duration: 0.012s, episode steps: 44, steps per second: 3541, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.164 [-1.087, 0.561], mean_best_reward: --\n",
      " 80993/100000: episode: 2049, duration: 0.016s, episode steps: 55, steps per second: 3406, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.240 [-1.668, 0.744], mean_best_reward: --\n",
      " 81012/100000: episode: 2050, duration: 0.006s, episode steps: 19, steps per second: 3295, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.077 [-0.989, 1.763], mean_best_reward: --\n",
      " 81032/100000: episode: 2051, duration: 0.006s, episode steps: 20, steps per second: 3118, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.109 [-0.379, 0.893], mean_best_reward: 137.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81120/100000: episode: 2052, duration: 0.028s, episode steps: 88, steps per second: 3185, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.135 [-1.690, 1.406], mean_best_reward: --\n",
      " 81165/100000: episode: 2053, duration: 0.016s, episode steps: 45, steps per second: 2800, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.008 [-1.411, 0.952], mean_best_reward: --\n",
      " 81178/100000: episode: 2054, duration: 0.005s, episode steps: 13, steps per second: 2627, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.116 [-0.951, 1.675], mean_best_reward: --\n",
      " 81218/100000: episode: 2055, duration: 0.012s, episode steps: 40, steps per second: 3293, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.059 [-1.097, 0.897], mean_best_reward: --\n",
      " 81260/100000: episode: 2056, duration: 0.013s, episode steps: 42, steps per second: 3281, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.105 [-0.622, 0.940], mean_best_reward: --\n",
      " 81306/100000: episode: 2057, duration: 0.014s, episode steps: 46, steps per second: 3298, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.057 [-1.321, 0.775], mean_best_reward: --\n",
      " 81328/100000: episode: 2058, duration: 0.007s, episode steps: 22, steps per second: 3097, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.082 [-0.770, 1.539], mean_best_reward: --\n",
      " 81357/100000: episode: 2059, duration: 0.009s, episode steps: 29, steps per second: 3256, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.041 [-0.920, 0.450], mean_best_reward: --\n",
      " 81389/100000: episode: 2060, duration: 0.010s, episode steps: 32, steps per second: 3237, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.147 [-0.413, 0.891], mean_best_reward: --\n",
      " 81408/100000: episode: 2061, duration: 0.006s, episode steps: 19, steps per second: 3060, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.093 [-1.347, 0.655], mean_best_reward: --\n",
      " 81438/100000: episode: 2062, duration: 0.009s, episode steps: 30, steps per second: 3217, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.086 [-1.036, 0.452], mean_best_reward: --\n",
      " 81456/100000: episode: 2063, duration: 0.006s, episode steps: 18, steps per second: 2983, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.095 [-0.775, 1.453], mean_best_reward: --\n",
      " 81473/100000: episode: 2064, duration: 0.006s, episode steps: 17, steps per second: 2875, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.103 [-0.955, 1.717], mean_best_reward: --\n",
      " 81492/100000: episode: 2065, duration: 0.006s, episode steps: 19, steps per second: 3008, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.102 [-1.256, 0.826], mean_best_reward: --\n",
      " 81529/100000: episode: 2066, duration: 0.011s, episode steps: 37, steps per second: 3226, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.085 [-0.915, 0.372], mean_best_reward: --\n",
      " 81552/100000: episode: 2067, duration: 0.007s, episode steps: 23, steps per second: 3083, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.096 [-0.804, 1.232], mean_best_reward: --\n",
      " 81644/100000: episode: 2068, duration: 0.027s, episode steps: 92, steps per second: 3391, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-1.188, 1.384], mean_best_reward: --\n",
      " 81673/100000: episode: 2069, duration: 0.009s, episode steps: 29, steps per second: 3106, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.004 [-0.968, 1.258], mean_best_reward: --\n",
      " 81702/100000: episode: 2070, duration: 0.009s, episode steps: 29, steps per second: 3191, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.108 [-1.303, 0.401], mean_best_reward: --\n",
      " 81748/100000: episode: 2071, duration: 0.015s, episode steps: 46, steps per second: 3137, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.037 [-0.747, 1.485], mean_best_reward: --\n",
      " 81765/100000: episode: 2072, duration: 0.006s, episode steps: 17, steps per second: 2727, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.075 [-0.750, 1.253], mean_best_reward: --\n",
      " 81860/100000: episode: 2073, duration: 0.032s, episode steps: 95, steps per second: 3011, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: 0.386 [-0.797, 1.810], mean_best_reward: --\n",
      " 81891/100000: episode: 2074, duration: 0.010s, episode steps: 31, steps per second: 3192, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.086 [-1.081, 0.613], mean_best_reward: --\n",
      " 81923/100000: episode: 2075, duration: 0.010s, episode steps: 32, steps per second: 3213, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.126 [-0.382, 1.210], mean_best_reward: --\n",
      " 81960/100000: episode: 2076, duration: 0.011s, episode steps: 37, steps per second: 3260, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.116 [-1.137, 0.733], mean_best_reward: --\n",
      " 81996/100000: episode: 2077, duration: 0.011s, episode steps: 36, steps per second: 3228, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.943, 1.283], mean_best_reward: --\n",
      " 82044/100000: episode: 2078, duration: 0.014s, episode steps: 48, steps per second: 3371, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.129 [-0.633, 0.896], mean_best_reward: --\n",
      " 82081/100000: episode: 2079, duration: 0.011s, episode steps: 37, steps per second: 3331, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.016 [-1.317, 1.637], mean_best_reward: --\n",
      " 82100/100000: episode: 2080, duration: 0.007s, episode steps: 19, steps per second: 2921, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.080 [-1.550, 0.787], mean_best_reward: --\n",
      " 82145/100000: episode: 2081, duration: 0.013s, episode steps: 45, steps per second: 3433, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.094 [-0.552, 1.072], mean_best_reward: --\n",
      " 82198/100000: episode: 2082, duration: 0.016s, episode steps: 53, steps per second: 3410, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.166 [-0.806, 1.319], mean_best_reward: --\n",
      " 82258/100000: episode: 2083, duration: 0.018s, episode steps: 60, steps per second: 3373, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.075 [-1.274, 0.651], mean_best_reward: --\n",
      " 82269/100000: episode: 2084, duration: 0.004s, episode steps: 11, steps per second: 2546, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-1.776, 0.982], mean_best_reward: --\n",
      " 82330/100000: episode: 2085, duration: 0.018s, episode steps: 61, steps per second: 3387, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.101 [-0.622, 1.273], mean_best_reward: --\n",
      " 82345/100000: episode: 2086, duration: 0.005s, episode steps: 15, steps per second: 2883, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.092 [-1.581, 0.955], mean_best_reward: --\n",
      " 82370/100000: episode: 2087, duration: 0.008s, episode steps: 25, steps per second: 3139, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.027 [-2.062, 1.331], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82406/100000: episode: 2088, duration: 0.013s, episode steps: 36, steps per second: 2854, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.114 [-1.365, 0.642], mean_best_reward: --\n",
      " 82485/100000: episode: 2089, duration: 0.029s, episode steps: 79, steps per second: 2764, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.063 [-1.189, 1.361], mean_best_reward: --\n",
      " 82554/100000: episode: 2090, duration: 0.022s, episode steps: 69, steps per second: 3196, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: -0.021 [-1.336, 0.755], mean_best_reward: --\n",
      " 82567/100000: episode: 2091, duration: 0.005s, episode steps: 13, steps per second: 2854, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.099 [-1.162, 1.914], mean_best_reward: --\n",
      " 82595/100000: episode: 2092, duration: 0.009s, episode steps: 28, steps per second: 3060, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.021 [-1.594, 0.988], mean_best_reward: --\n",
      " 82630/100000: episode: 2093, duration: 0.011s, episode steps: 35, steps per second: 3141, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.075 [-1.907, 1.770], mean_best_reward: --\n",
      " 82646/100000: episode: 2094, duration: 0.005s, episode steps: 16, steps per second: 3022, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.086 [-1.025, 1.721], mean_best_reward: --\n",
      " 82671/100000: episode: 2095, duration: 0.008s, episode steps: 25, steps per second: 3053, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.098 [-1.003, 0.591], mean_best_reward: --\n",
      " 82706/100000: episode: 2096, duration: 0.011s, episode steps: 35, steps per second: 3287, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.128 [-1.242, 0.485], mean_best_reward: --\n",
      " 82742/100000: episode: 2097, duration: 0.012s, episode steps: 36, steps per second: 3099, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.091 [-0.960, 0.506], mean_best_reward: --\n",
      " 82761/100000: episode: 2098, duration: 0.007s, episode steps: 19, steps per second: 2908, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.079 [-1.389, 0.969], mean_best_reward: --\n",
      " 82790/100000: episode: 2099, duration: 0.009s, episode steps: 29, steps per second: 3192, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.089 [-1.703, 0.646], mean_best_reward: --\n",
      " 82825/100000: episode: 2100, duration: 0.011s, episode steps: 35, steps per second: 3264, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.012 [-1.039, 0.784], mean_best_reward: --\n",
      " 82874/100000: episode: 2101, duration: 0.015s, episode steps: 49, steps per second: 3246, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.225 [-1.270, 0.713], mean_best_reward: 108.500000\n",
      " 82914/100000: episode: 2102, duration: 0.012s, episode steps: 40, steps per second: 3292, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.130 [-0.549, 0.956], mean_best_reward: --\n",
      " 82925/100000: episode: 2103, duration: 0.004s, episode steps: 11, steps per second: 2756, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.128 [-1.712, 2.804], mean_best_reward: --\n",
      " 82939/100000: episode: 2104, duration: 0.005s, episode steps: 14, steps per second: 2893, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.126 [-0.590, 1.282], mean_best_reward: --\n",
      " 83137/100000: episode: 2105, duration: 0.062s, episode steps: 198, steps per second: 3188, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.111 [-1.375, 1.500], mean_best_reward: --\n",
      " 83168/100000: episode: 2106, duration: 0.010s, episode steps: 31, steps per second: 3186, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.068 [-1.364, 2.446], mean_best_reward: --\n",
      " 83214/100000: episode: 2107, duration: 0.014s, episode steps: 46, steps per second: 3317, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.063 [-0.628, 1.021], mean_best_reward: --\n",
      " 83230/100000: episode: 2108, duration: 0.005s, episode steps: 16, steps per second: 2953, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.102 [-0.946, 1.636], mean_best_reward: --\n",
      " 83252/100000: episode: 2109, duration: 0.007s, episode steps: 22, steps per second: 3162, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.065 [-1.247, 0.776], mean_best_reward: --\n",
      " 83289/100000: episode: 2110, duration: 0.012s, episode steps: 37, steps per second: 3149, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.091 [-0.578, 1.356], mean_best_reward: --\n",
      " 83340/100000: episode: 2111, duration: 0.015s, episode steps: 51, steps per second: 3347, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.021 [-0.968, 1.465], mean_best_reward: --\n",
      " 83351/100000: episode: 2112, duration: 0.004s, episode steps: 11, steps per second: 2786, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.127 [-1.340, 2.142], mean_best_reward: --\n",
      " 83389/100000: episode: 2113, duration: 0.012s, episode steps: 38, steps per second: 3246, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.144 [-0.598, 1.159], mean_best_reward: --\n",
      " 83423/100000: episode: 2114, duration: 0.011s, episode steps: 34, steps per second: 3221, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.138 [-1.212, 0.740], mean_best_reward: --\n",
      " 83436/100000: episode: 2115, duration: 0.005s, episode steps: 13, steps per second: 2532, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.078 [-1.555, 1.017], mean_best_reward: --\n",
      " 83447/100000: episode: 2116, duration: 0.004s, episode steps: 11, steps per second: 2761, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.143 [-0.962, 1.716], mean_best_reward: --\n",
      " 83520/100000: episode: 2117, duration: 0.021s, episode steps: 73, steps per second: 3484, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.159 [-0.889, 1.305], mean_best_reward: --\n",
      " 83552/100000: episode: 2118, duration: 0.010s, episode steps: 32, steps per second: 3216, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.093 [-1.551, 0.600], mean_best_reward: --\n",
      " 83622/100000: episode: 2119, duration: 0.020s, episode steps: 70, steps per second: 3427, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.116 [-1.319, 1.231], mean_best_reward: --\n",
      " 83666/100000: episode: 2120, duration: 0.013s, episode steps: 44, steps per second: 3302, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.003 [-1.172, 1.825], mean_best_reward: --\n",
      " 83697/100000: episode: 2121, duration: 0.010s, episode steps: 31, steps per second: 3250, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.117 [-0.760, 1.737], mean_best_reward: --\n",
      " 83718/100000: episode: 2122, duration: 0.007s, episode steps: 21, steps per second: 3027, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.094 [-0.790, 1.219], mean_best_reward: --\n",
      " 83732/100000: episode: 2123, duration: 0.005s, episode steps: 14, steps per second: 2860, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.108 [-1.237, 0.773], mean_best_reward: --\n",
      " 83749/100000: episode: 2124, duration: 0.006s, episode steps: 17, steps per second: 2950, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.075 [-0.815, 1.328], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 83764/100000: episode: 2125, duration: 0.005s, episode steps: 15, steps per second: 2811, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.104 [-0.764, 1.333], mean_best_reward: --\n",
      " 83816/100000: episode: 2126, duration: 0.018s, episode steps: 52, steps per second: 2891, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.018 [-0.913, 1.133], mean_best_reward: --\n",
      " 83888/100000: episode: 2127, duration: 0.025s, episode steps: 72, steps per second: 2906, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.208 [-0.945, 1.249], mean_best_reward: --\n",
      " 83907/100000: episode: 2128, duration: 0.006s, episode steps: 19, steps per second: 3016, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.088 [-0.966, 1.558], mean_best_reward: --\n",
      " 83925/100000: episode: 2129, duration: 0.006s, episode steps: 18, steps per second: 3030, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.119 [-1.476, 0.775], mean_best_reward: --\n",
      " 83969/100000: episode: 2130, duration: 0.013s, episode steps: 44, steps per second: 3299, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.432 [0.000, 1.000], mean observation: -0.049 [-1.372, 1.801], mean_best_reward: --\n",
      " 83986/100000: episode: 2131, duration: 0.006s, episode steps: 17, steps per second: 2839, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.089 [-1.016, 0.586], mean_best_reward: --\n",
      " 84033/100000: episode: 2132, duration: 0.015s, episode steps: 47, steps per second: 3219, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.185 [-0.745, 1.344], mean_best_reward: --\n",
      " 84059/100000: episode: 2133, duration: 0.008s, episode steps: 26, steps per second: 3131, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.140 [-0.748, 1.253], mean_best_reward: --\n",
      " 84102/100000: episode: 2134, duration: 0.013s, episode steps: 43, steps per second: 3338, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.134 [-1.026, 1.470], mean_best_reward: --\n",
      " 84148/100000: episode: 2135, duration: 0.014s, episode steps: 46, steps per second: 3347, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.020 [-1.445, 0.992], mean_best_reward: --\n",
      " 84172/100000: episode: 2136, duration: 0.008s, episode steps: 24, steps per second: 3084, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.431, 0.847], mean_best_reward: --\n",
      " 84200/100000: episode: 2137, duration: 0.009s, episode steps: 28, steps per second: 3089, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.044 [-1.612, 0.826], mean_best_reward: --\n",
      " 84236/100000: episode: 2138, duration: 0.011s, episode steps: 36, steps per second: 3217, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.090 [-1.353, 1.129], mean_best_reward: --\n",
      " 84267/100000: episode: 2139, duration: 0.010s, episode steps: 31, steps per second: 3196, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.075 [-1.261, 0.796], mean_best_reward: --\n",
      " 84294/100000: episode: 2140, duration: 0.009s, episode steps: 27, steps per second: 3087, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.102 [-0.734, 1.124], mean_best_reward: --\n",
      " 84307/100000: episode: 2141, duration: 0.005s, episode steps: 13, steps per second: 2813, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.109 [-0.984, 0.580], mean_best_reward: --\n",
      " 84320/100000: episode: 2142, duration: 0.005s, episode steps: 13, steps per second: 2830, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.110 [-0.834, 1.328], mean_best_reward: --\n",
      " 84337/100000: episode: 2143, duration: 0.006s, episode steps: 17, steps per second: 2955, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.074 [-0.630, 1.184], mean_best_reward: --\n",
      " 84347/100000: episode: 2144, duration: 0.004s, episode steps: 10, steps per second: 2675, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.125 [-1.646, 0.968], mean_best_reward: --\n",
      " 84381/100000: episode: 2145, duration: 0.012s, episode steps: 34, steps per second: 2910, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.126 [-2.410, 1.149], mean_best_reward: --\n",
      " 84418/100000: episode: 2146, duration: 0.014s, episode steps: 37, steps per second: 2683, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.010 [-0.778, 1.063], mean_best_reward: --\n",
      " 84457/100000: episode: 2147, duration: 0.015s, episode steps: 39, steps per second: 2590, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.080 [-0.740, 1.397], mean_best_reward: --\n",
      " 84520/100000: episode: 2148, duration: 0.019s, episode steps: 63, steps per second: 3322, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.045 [-0.973, 1.453], mean_best_reward: --\n",
      " 84538/100000: episode: 2149, duration: 0.006s, episode steps: 18, steps per second: 2954, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.055 [-1.011, 1.587], mean_best_reward: --\n",
      " 84612/100000: episode: 2150, duration: 0.022s, episode steps: 74, steps per second: 3430, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.134 [-1.163, 1.085], mean_best_reward: --\n",
      " 84638/100000: episode: 2151, duration: 0.008s, episode steps: 26, steps per second: 3072, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.085 [-0.589, 1.438], mean_best_reward: 105.500000\n",
      " 84669/100000: episode: 2152, duration: 0.010s, episode steps: 31, steps per second: 3198, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: 0.003 [-1.746, 1.386], mean_best_reward: --\n",
      " 84697/100000: episode: 2153, duration: 0.009s, episode steps: 28, steps per second: 3232, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.078 [-0.753, 1.534], mean_best_reward: --\n",
      " 84751/100000: episode: 2154, duration: 0.016s, episode steps: 54, steps per second: 3481, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.116 [-1.378, 1.254], mean_best_reward: --\n",
      " 84799/100000: episode: 2155, duration: 0.014s, episode steps: 48, steps per second: 3449, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.084 [-0.884, 1.160], mean_best_reward: --\n",
      " 84821/100000: episode: 2156, duration: 0.007s, episode steps: 22, steps per second: 3136, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.096 [-0.537, 0.917], mean_best_reward: --\n",
      " 84893/100000: episode: 2157, duration: 0.020s, episode steps: 72, steps per second: 3603, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.020 [-1.517, 2.147], mean_best_reward: --\n",
      " 84953/100000: episode: 2158, duration: 0.018s, episode steps: 60, steps per second: 3416, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.043 [-1.506, 1.333], mean_best_reward: --\n",
      " 84970/100000: episode: 2159, duration: 0.006s, episode steps: 17, steps per second: 3011, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.076 [-1.474, 0.810], mean_best_reward: --\n",
      " 84986/100000: episode: 2160, duration: 0.005s, episode steps: 16, steps per second: 3086, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.119 [-0.747, 1.284], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85015/100000: episode: 2161, duration: 0.009s, episode steps: 29, steps per second: 3266, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.097 [-0.540, 0.985], mean_best_reward: --\n",
      " 85051/100000: episode: 2162, duration: 0.013s, episode steps: 36, steps per second: 2859, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.633, 1.192], mean_best_reward: --\n",
      " 85152/100000: episode: 2163, duration: 0.033s, episode steps: 101, steps per second: 3092, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.383 [-1.886, 1.121], mean_best_reward: --\n",
      " 85174/100000: episode: 2164, duration: 0.008s, episode steps: 22, steps per second: 2815, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.082 [-1.006, 1.875], mean_best_reward: --\n",
      " 85245/100000: episode: 2165, duration: 0.021s, episode steps: 71, steps per second: 3312, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.178 [-1.691, 1.214], mean_best_reward: --\n",
      " 85294/100000: episode: 2166, duration: 0.015s, episode steps: 49, steps per second: 3348, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.109 [-0.970, 0.555], mean_best_reward: --\n",
      " 85305/100000: episode: 2167, duration: 0.004s, episode steps: 11, steps per second: 2668, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.119 [-0.948, 1.594], mean_best_reward: --\n",
      " 85343/100000: episode: 2168, duration: 0.012s, episode steps: 38, steps per second: 3284, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.077 [-0.555, 1.107], mean_best_reward: --\n",
      " 85377/100000: episode: 2169, duration: 0.011s, episode steps: 34, steps per second: 3230, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.087 [-0.569, 0.977], mean_best_reward: --\n",
      " 85388/100000: episode: 2170, duration: 0.004s, episode steps: 11, steps per second: 2786, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.115 [-1.333, 0.778], mean_best_reward: --\n",
      " 85409/100000: episode: 2171, duration: 0.007s, episode steps: 21, steps per second: 2872, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.085 [-1.239, 0.804], mean_best_reward: --\n",
      " 85480/100000: episode: 2172, duration: 0.021s, episode steps: 71, steps per second: 3434, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.082 [-1.315, 1.228], mean_best_reward: --\n",
      " 85505/100000: episode: 2173, duration: 0.010s, episode steps: 25, steps per second: 2532, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.045 [-1.217, 0.875], mean_best_reward: --\n",
      " 85568/100000: episode: 2174, duration: 0.020s, episode steps: 63, steps per second: 3202, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.175 [-0.895, 1.195], mean_best_reward: --\n",
      " 85697/100000: episode: 2175, duration: 0.040s, episode steps: 129, steps per second: 3216, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.040 [-0.762, 1.363], mean_best_reward: --\n",
      " 85710/100000: episode: 2176, duration: 0.005s, episode steps: 13, steps per second: 2785, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.121 [-1.204, 0.598], mean_best_reward: --\n",
      " 85742/100000: episode: 2177, duration: 0.015s, episode steps: 32, steps per second: 2071, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.061 [-0.550, 1.050], mean_best_reward: --\n",
      " 85817/100000: episode: 2178, duration: 0.024s, episode steps: 75, steps per second: 3137, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.069 [-1.280, 1.291], mean_best_reward: --\n",
      " 85835/100000: episode: 2179, duration: 0.006s, episode steps: 18, steps per second: 2957, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.065 [-1.192, 1.936], mean_best_reward: --\n",
      " 85848/100000: episode: 2180, duration: 0.005s, episode steps: 13, steps per second: 2678, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.122 [-1.724, 0.949], mean_best_reward: --\n",
      " 85883/100000: episode: 2181, duration: 0.011s, episode steps: 35, steps per second: 3202, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.055 [-0.741, 1.327], mean_best_reward: --\n",
      " 85925/100000: episode: 2182, duration: 0.013s, episode steps: 42, steps per second: 3271, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.052 [-1.239, 0.575], mean_best_reward: --\n",
      " 85951/100000: episode: 2183, duration: 0.008s, episode steps: 26, steps per second: 3166, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.040 [-0.827, 1.394], mean_best_reward: --\n",
      " 85971/100000: episode: 2184, duration: 0.007s, episode steps: 20, steps per second: 3070, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.091 [-0.639, 1.312], mean_best_reward: --\n",
      " 85998/100000: episode: 2185, duration: 0.008s, episode steps: 27, steps per second: 3194, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.039 [-0.880, 1.357], mean_best_reward: --\n",
      " 86008/100000: episode: 2186, duration: 0.004s, episode steps: 10, steps per second: 2566, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.116 [-1.555, 0.934], mean_best_reward: --\n",
      " 86026/100000: episode: 2187, duration: 0.006s, episode steps: 18, steps per second: 3014, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.069 [-1.414, 0.835], mean_best_reward: --\n",
      " 86070/100000: episode: 2188, duration: 0.013s, episode steps: 44, steps per second: 3284, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.062 [-0.663, 1.087], mean_best_reward: --\n",
      " 86127/100000: episode: 2189, duration: 0.017s, episode steps: 57, steps per second: 3367, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.064 [-1.131, 1.227], mean_best_reward: --\n",
      " 86184/100000: episode: 2190, duration: 0.017s, episode steps: 57, steps per second: 3421, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.102 [-0.886, 1.208], mean_best_reward: --\n",
      " 86217/100000: episode: 2191, duration: 0.010s, episode steps: 33, steps per second: 3154, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.069 [-0.675, 1.185], mean_best_reward: --\n",
      " 86301/100000: episode: 2192, duration: 0.024s, episode steps: 84, steps per second: 3434, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.067 [-2.103, 1.357], mean_best_reward: --\n",
      " 86348/100000: episode: 2193, duration: 0.020s, episode steps: 47, steps per second: 2403, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.134 [-0.737, 1.163], mean_best_reward: --\n",
      " 86371/100000: episode: 2194, duration: 0.011s, episode steps: 23, steps per second: 2103, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.121 [-0.597, 1.055], mean_best_reward: --\n",
      " 86394/100000: episode: 2195, duration: 0.008s, episode steps: 23, steps per second: 2944, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.040 [-0.737, 1.217], mean_best_reward: --\n",
      " 86412/100000: episode: 2196, duration: 0.007s, episode steps: 18, steps per second: 2689, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.068 [-1.098, 0.635], mean_best_reward: --\n",
      " 86467/100000: episode: 2197, duration: 0.017s, episode steps: 55, steps per second: 3217, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.050 [-2.012, 1.151], mean_best_reward: --\n",
      " 86569/100000: episode: 2198, duration: 0.030s, episode steps: 102, steps per second: 3441, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.091 [-1.315, 0.928], mean_best_reward: --\n",
      " 86716/100000: episode: 2199, duration: 0.043s, episode steps: 147, steps per second: 3426, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.261 [-1.382, 1.279], mean_best_reward: --\n",
      " 86775/100000: episode: 2200, duration: 0.023s, episode steps: 59, steps per second: 2610, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.023 [-1.144, 1.285], mean_best_reward: --\n",
      " 86837/100000: episode: 2201, duration: 0.027s, episode steps: 62, steps per second: 2304, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.209 [-1.295, 0.848], mean_best_reward: 122.500000\n",
      " 86867/100000: episode: 2202, duration: 0.010s, episode steps: 30, steps per second: 2948, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.633 [0.000, 1.000], mean observation: -0.015 [-2.334, 1.569], mean_best_reward: --\n",
      " 86877/100000: episode: 2203, duration: 0.004s, episode steps: 10, steps per second: 2462, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.114 [-1.188, 1.791], mean_best_reward: --\n",
      " 86895/100000: episode: 2204, duration: 0.006s, episode steps: 18, steps per second: 2779, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.083 [-0.964, 1.696], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86940/100000: episode: 2205, duration: 0.015s, episode steps: 45, steps per second: 3049, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.076 [-0.609, 1.258], mean_best_reward: --\n",
      " 86960/100000: episode: 2206, duration: 0.006s, episode steps: 20, steps per second: 3101, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.083 [-1.008, 1.712], mean_best_reward: --\n",
      " 86974/100000: episode: 2207, duration: 0.007s, episode steps: 14, steps per second: 2074, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.102 [-1.647, 0.980], mean_best_reward: --\n",
      " 86996/100000: episode: 2208, duration: 0.011s, episode steps: 22, steps per second: 2084, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-1.000, 1.484], mean_best_reward: --\n",
      " 87008/100000: episode: 2209, duration: 0.005s, episode steps: 12, steps per second: 2477, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-1.840, 1.165], mean_best_reward: --\n",
      " 87058/100000: episode: 2210, duration: 0.016s, episode steps: 50, steps per second: 3160, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.581, 1.093], mean_best_reward: --\n",
      " 87097/100000: episode: 2211, duration: 0.012s, episode steps: 39, steps per second: 3164, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.054 [-1.117, 0.742], mean_best_reward: --\n",
      " 87125/100000: episode: 2212, duration: 0.009s, episode steps: 28, steps per second: 3107, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.080 [-1.640, 0.765], mean_best_reward: --\n",
      " 87159/100000: episode: 2213, duration: 0.011s, episode steps: 34, steps per second: 3197, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.931, 1.318], mean_best_reward: --\n",
      " 87276/100000: episode: 2214, duration: 0.034s, episode steps: 117, steps per second: 3442, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.217 [-1.132, 1.281], mean_best_reward: --\n",
      " 87350/100000: episode: 2215, duration: 0.022s, episode steps: 74, steps per second: 3419, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.554 [0.000, 1.000], mean observation: 0.281 [-0.951, 1.848], mean_best_reward: --\n",
      " 87368/100000: episode: 2216, duration: 0.006s, episode steps: 18, steps per second: 2946, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.094 [-1.408, 0.794], mean_best_reward: --\n",
      " 87396/100000: episode: 2217, duration: 0.009s, episode steps: 28, steps per second: 3156, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.053 [-2.150, 1.225], mean_best_reward: --\n",
      " 87444/100000: episode: 2218, duration: 0.014s, episode steps: 48, steps per second: 3318, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.122 [-0.615, 1.410], mean_best_reward: --\n",
      " 87467/100000: episode: 2219, duration: 0.008s, episode steps: 23, steps per second: 3023, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.079 [-1.156, 1.862], mean_best_reward: --\n",
      " 87489/100000: episode: 2220, duration: 0.009s, episode steps: 22, steps per second: 2525, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.087 [-1.122, 0.597], mean_best_reward: --\n",
      " 87524/100000: episode: 2221, duration: 0.011s, episode steps: 35, steps per second: 3191, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.107 [-0.594, 1.137], mean_best_reward: --\n",
      " 87664/100000: episode: 2222, duration: 0.045s, episode steps: 140, steps per second: 3140, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.084 [-0.965, 1.305], mean_best_reward: --\n",
      " 87688/100000: episode: 2223, duration: 0.008s, episode steps: 24, steps per second: 3001, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.050 [-1.420, 0.804], mean_best_reward: --\n",
      " 87702/100000: episode: 2224, duration: 0.005s, episode steps: 14, steps per second: 2809, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.104 [-0.938, 1.638], mean_best_reward: --\n",
      " 87737/100000: episode: 2225, duration: 0.011s, episode steps: 35, steps per second: 3224, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.087 [-0.417, 0.986], mean_best_reward: --\n",
      " 87776/100000: episode: 2226, duration: 0.012s, episode steps: 39, steps per second: 3273, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.071 [-0.834, 0.598], mean_best_reward: --\n",
      " 87794/100000: episode: 2227, duration: 0.006s, episode steps: 18, steps per second: 2874, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.080 [-1.635, 0.951], mean_best_reward: --\n",
      " 87864/100000: episode: 2228, duration: 0.021s, episode steps: 70, steps per second: 3398, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.038 [-1.194, 1.417], mean_best_reward: --\n",
      " 87892/100000: episode: 2229, duration: 0.009s, episode steps: 28, steps per second: 3210, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.071 [-0.583, 1.249], mean_best_reward: --\n",
      " 87996/100000: episode: 2230, duration: 0.030s, episode steps: 104, steps per second: 3431, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.110 [-1.132, 1.148], mean_best_reward: --\n",
      " 88025/100000: episode: 2231, duration: 0.009s, episode steps: 29, steps per second: 3156, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.013 [-1.007, 1.580], mean_best_reward: --\n",
      " 88044/100000: episode: 2232, duration: 0.006s, episode steps: 19, steps per second: 2960, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.087 [-1.210, 0.755], mean_best_reward: --\n",
      " 88065/100000: episode: 2233, duration: 0.007s, episode steps: 21, steps per second: 2926, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.096 [-1.180, 0.612], mean_best_reward: --\n",
      " 88143/100000: episode: 2234, duration: 0.023s, episode steps: 78, steps per second: 3390, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.138 [-1.116, 0.783], mean_best_reward: --\n",
      " 88157/100000: episode: 2235, duration: 0.006s, episode steps: 14, steps per second: 2510, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.096 [-0.993, 1.467], mean_best_reward: --\n",
      " 88194/100000: episode: 2236, duration: 0.011s, episode steps: 37, steps per second: 3239, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.189 [-0.397, 0.930], mean_best_reward: --\n",
      " 88229/100000: episode: 2237, duration: 0.012s, episode steps: 35, steps per second: 2971, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: -0.039 [-1.571, 2.076], mean_best_reward: --\n",
      " 88243/100000: episode: 2238, duration: 0.005s, episode steps: 14, steps per second: 2832, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.103 [-0.816, 1.293], mean_best_reward: --\n",
      " 88260/100000: episode: 2239, duration: 0.006s, episode steps: 17, steps per second: 2979, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.085 [-1.012, 0.579], mean_best_reward: --\n",
      " 88273/100000: episode: 2240, duration: 0.005s, episode steps: 13, steps per second: 2826, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.107 [-1.337, 0.833], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88337/100000: episode: 2241, duration: 0.021s, episode steps: 64, steps per second: 3037, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.062 [-1.174, 1.197], mean_best_reward: --\n",
      " 88432/100000: episode: 2242, duration: 0.030s, episode steps: 95, steps per second: 3155, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.077 [-1.109, 1.328], mean_best_reward: --\n",
      " 88531/100000: episode: 2243, duration: 0.029s, episode steps: 99, steps per second: 3380, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.092 [-1.170, 1.707], mean_best_reward: --\n",
      " 88588/100000: episode: 2244, duration: 0.017s, episode steps: 57, steps per second: 3324, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.050 [-1.123, 1.006], mean_best_reward: --\n",
      " 88622/100000: episode: 2245, duration: 0.011s, episode steps: 34, steps per second: 3192, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.102 [-0.386, 0.747], mean_best_reward: --\n",
      " 88699/100000: episode: 2246, duration: 0.023s, episode steps: 77, steps per second: 3322, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.231 [-0.499, 1.318], mean_best_reward: --\n",
      " 88829/100000: episode: 2247, duration: 0.038s, episode steps: 130, steps per second: 3429, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.283 [-0.938, 1.372], mean_best_reward: --\n",
      " 88847/100000: episode: 2248, duration: 0.006s, episode steps: 18, steps per second: 2981, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.969, 1.399], mean_best_reward: --\n",
      " 88933/100000: episode: 2249, duration: 0.025s, episode steps: 86, steps per second: 3418, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.274 [-1.148, 2.074], mean_best_reward: --\n",
      " 89003/100000: episode: 2250, duration: 0.021s, episode steps: 70, steps per second: 3336, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.121 [-1.003, 1.255], mean_best_reward: --\n",
      " 89065/100000: episode: 2251, duration: 0.023s, episode steps: 62, steps per second: 2683, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.043 [-1.087, 1.014], mean_best_reward: 108.000000\n",
      " 89107/100000: episode: 2252, duration: 0.014s, episode steps: 42, steps per second: 3047, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.063 [-1.410, 0.790], mean_best_reward: --\n",
      " 89191/100000: episode: 2253, duration: 0.024s, episode steps: 84, steps per second: 3485, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.196 [-0.809, 2.036], mean_best_reward: --\n",
      " 89238/100000: episode: 2254, duration: 0.014s, episode steps: 47, steps per second: 3298, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.145 [-1.161, 0.491], mean_best_reward: --\n",
      " 89304/100000: episode: 2255, duration: 0.019s, episode steps: 66, steps per second: 3399, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.253 [-0.590, 1.476], mean_best_reward: --\n",
      " 89346/100000: episode: 2256, duration: 0.013s, episode steps: 42, steps per second: 3299, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.013 [-1.558, 0.942], mean_best_reward: --\n",
      " 89410/100000: episode: 2257, duration: 0.020s, episode steps: 64, steps per second: 3236, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.031 [-1.105, 1.125], mean_best_reward: --\n",
      " 89426/100000: episode: 2258, duration: 0.007s, episode steps: 16, steps per second: 2376, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.101 [-1.241, 0.589], mean_best_reward: --\n",
      " 89453/100000: episode: 2259, duration: 0.009s, episode steps: 27, steps per second: 3001, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.046 [-0.781, 1.266], mean_best_reward: --\n",
      " 89473/100000: episode: 2260, duration: 0.007s, episode steps: 20, steps per second: 2902, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.043 [-1.886, 1.187], mean_best_reward: --\n",
      " 89485/100000: episode: 2261, duration: 0.004s, episode steps: 12, steps per second: 2811, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-0.991, 1.695], mean_best_reward: --\n",
      " 89499/100000: episode: 2262, duration: 0.005s, episode steps: 14, steps per second: 2921, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.451, 0.746], mean_best_reward: --\n",
      " 89521/100000: episode: 2263, duration: 0.007s, episode steps: 22, steps per second: 3014, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.120 [-0.549, 1.004], mean_best_reward: --\n",
      " 89630/100000: episode: 2264, duration: 0.032s, episode steps: 109, steps per second: 3380, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.431 [0.000, 1.000], mean observation: -0.475 [-3.328, 1.293], mean_best_reward: --\n",
      " 89643/100000: episode: 2265, duration: 0.005s, episode steps: 13, steps per second: 2448, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.131 [-0.587, 1.327], mean_best_reward: --\n",
      " 89705/100000: episode: 2266, duration: 0.022s, episode steps: 62, steps per second: 2836, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.202 [-0.767, 1.602], mean_best_reward: --\n",
      " 89723/100000: episode: 2267, duration: 0.006s, episode steps: 18, steps per second: 2784, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.120 [-0.555, 1.059], mean_best_reward: --\n",
      " 89801/100000: episode: 2268, duration: 0.023s, episode steps: 78, steps per second: 3332, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.154 [-1.672, 1.614], mean_best_reward: --\n",
      " 89815/100000: episode: 2269, duration: 0.005s, episode steps: 14, steps per second: 2836, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.083 [-1.479, 0.965], mean_best_reward: --\n",
      " 89914/100000: episode: 2270, duration: 0.029s, episode steps: 99, steps per second: 3419, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.140 [-1.905, 1.763], mean_best_reward: --\n",
      " 89932/100000: episode: 2271, duration: 0.006s, episode steps: 18, steps per second: 2984, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.055 [-1.180, 1.875], mean_best_reward: --\n",
      " 89958/100000: episode: 2272, duration: 0.008s, episode steps: 26, steps per second: 3168, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.069 [-1.408, 0.789], mean_best_reward: --\n",
      " 89997/100000: episode: 2273, duration: 0.013s, episode steps: 39, steps per second: 3110, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: -0.019 [-1.357, 1.770], mean_best_reward: --\n",
      " 90044/100000: episode: 2274, duration: 0.014s, episode steps: 47, steps per second: 3291, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.051 [-1.132, 0.799], mean_best_reward: --\n",
      " 90106/100000: episode: 2275, duration: 0.022s, episode steps: 62, steps per second: 2822, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-0.977, 0.736], mean_best_reward: --\n",
      " 90137/100000: episode: 2276, duration: 0.012s, episode steps: 31, steps per second: 2657, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.124 [-1.266, 0.744], mean_best_reward: --\n",
      " 90152/100000: episode: 2277, duration: 0.005s, episode steps: 15, steps per second: 2897, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.093 [-0.819, 1.227], mean_best_reward: --\n",
      " 90189/100000: episode: 2278, duration: 0.011s, episode steps: 37, steps per second: 3248, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.017 [-0.955, 1.383], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90315/100000: episode: 2279, duration: 0.040s, episode steps: 126, steps per second: 3188, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.463 [-2.129, 1.299], mean_best_reward: --\n",
      " 90326/100000: episode: 2280, duration: 0.007s, episode steps: 11, steps per second: 1495, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.123 [-1.329, 2.163], mean_best_reward: --\n",
      " 90399/100000: episode: 2281, duration: 0.022s, episode steps: 73, steps per second: 3273, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.155 [-1.336, 1.148], mean_best_reward: --\n",
      " 90428/100000: episode: 2282, duration: 0.009s, episode steps: 29, steps per second: 3118, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.063 [-0.946, 1.430], mean_best_reward: --\n",
      " 90460/100000: episode: 2283, duration: 0.010s, episode steps: 32, steps per second: 3263, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.589, 0.918], mean_best_reward: --\n",
      " 90523/100000: episode: 2284, duration: 0.018s, episode steps: 63, steps per second: 3452, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.051 [-1.117, 0.574], mean_best_reward: --\n",
      " 90548/100000: episode: 2285, duration: 0.008s, episode steps: 25, steps per second: 3112, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.069 [-0.839, 1.281], mean_best_reward: --\n",
      " 90567/100000: episode: 2286, duration: 0.006s, episode steps: 19, steps per second: 3002, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.102 [-1.350, 0.569], mean_best_reward: --\n",
      " 90594/100000: episode: 2287, duration: 0.008s, episode steps: 27, steps per second: 3181, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.107 [-0.809, 1.195], mean_best_reward: --\n",
      " 90612/100000: episode: 2288, duration: 0.007s, episode steps: 18, steps per second: 2718, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.051 [-1.363, 0.827], mean_best_reward: --\n",
      " 90635/100000: episode: 2289, duration: 0.007s, episode steps: 23, steps per second: 3120, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.055 [-1.155, 1.743], mean_best_reward: --\n",
      " 90661/100000: episode: 2290, duration: 0.008s, episode steps: 26, steps per second: 3174, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-1.207, 0.691], mean_best_reward: --\n",
      " 90693/100000: episode: 2291, duration: 0.010s, episode steps: 32, steps per second: 3221, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.598, 1.273], mean_best_reward: --\n",
      " 90716/100000: episode: 2292, duration: 0.007s, episode steps: 23, steps per second: 3071, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.112 [-0.417, 0.924], mean_best_reward: --\n",
      " 90745/100000: episode: 2293, duration: 0.009s, episode steps: 29, steps per second: 3192, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.049 [-0.750, 1.170], mean_best_reward: --\n",
      " 90820/100000: episode: 2294, duration: 0.022s, episode steps: 75, steps per second: 3388, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.166 [-1.323, 1.547], mean_best_reward: --\n",
      " 90856/100000: episode: 2295, duration: 0.011s, episode steps: 36, steps per second: 3257, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-1.491, 0.995], mean_best_reward: --\n",
      " 90885/100000: episode: 2296, duration: 0.009s, episode steps: 29, steps per second: 3102, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.088 [-0.553, 0.969], mean_best_reward: --\n",
      " 90914/100000: episode: 2297, duration: 0.010s, episode steps: 29, steps per second: 2978, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.056 [-1.114, 0.820], mean_best_reward: --\n",
      " 90926/100000: episode: 2298, duration: 0.005s, episode steps: 12, steps per second: 2578, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.082 [-1.661, 1.035], mean_best_reward: --\n",
      " 90950/100000: episode: 2299, duration: 0.011s, episode steps: 24, steps per second: 2276, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.072 [-1.614, 0.847], mean_best_reward: --\n",
      " 90982/100000: episode: 2300, duration: 0.013s, episode steps: 32, steps per second: 2402, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-1.185, 0.635], mean_best_reward: --\n",
      " 91015/100000: episode: 2301, duration: 0.011s, episode steps: 33, steps per second: 3108, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.052 [-0.582, 0.985], mean_best_reward: 85.500000\n",
      " 91055/100000: episode: 2302, duration: 0.012s, episode steps: 40, steps per second: 3265, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.112 [-0.588, 0.952], mean_best_reward: --\n",
      " 91158/100000: episode: 2303, duration: 0.030s, episode steps: 103, steps per second: 3377, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.184 [-1.686, 0.935], mean_best_reward: --\n",
      " 91249/100000: episode: 2304, duration: 0.026s, episode steps: 91, steps per second: 3485, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.066 [-0.827, 1.391], mean_best_reward: --\n",
      " 91287/100000: episode: 2305, duration: 0.012s, episode steps: 38, steps per second: 3301, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.049 [-1.125, 0.601], mean_best_reward: --\n",
      " 91316/100000: episode: 2306, duration: 0.009s, episode steps: 29, steps per second: 3065, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.047 [-0.767, 1.268], mean_best_reward: --\n",
      " 91365/100000: episode: 2307, duration: 0.014s, episode steps: 49, steps per second: 3382, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.145 [-0.655, 1.139], mean_best_reward: --\n",
      " 91388/100000: episode: 2308, duration: 0.008s, episode steps: 23, steps per second: 3008, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.056 [-0.985, 1.423], mean_best_reward: --\n",
      " 91457/100000: episode: 2309, duration: 0.020s, episode steps: 69, steps per second: 3382, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.009 [-1.006, 0.978], mean_best_reward: --\n",
      " 91529/100000: episode: 2310, duration: 0.021s, episode steps: 72, steps per second: 3433, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.060 [-1.303, 1.130], mean_best_reward: --\n",
      " 91541/100000: episode: 2311, duration: 0.004s, episode steps: 12, steps per second: 2857, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.131 [-1.307, 0.775], mean_best_reward: --\n",
      " 91573/100000: episode: 2312, duration: 0.011s, episode steps: 32, steps per second: 3027, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.085 [-0.985, 1.513], mean_best_reward: --\n",
      " 91634/100000: episode: 2313, duration: 0.024s, episode steps: 61, steps per second: 2579, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.108 [-0.576, 1.025], mean_best_reward: --\n",
      " 91654/100000: episode: 2314, duration: 0.007s, episode steps: 20, steps per second: 3039, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.091 [-0.561, 1.327], mean_best_reward: --\n",
      " 91750/100000: episode: 2315, duration: 0.028s, episode steps: 96, steps per second: 3388, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.225 [-1.075, 0.790], mean_best_reward: --\n",
      " 91791/100000: episode: 2316, duration: 0.012s, episode steps: 41, steps per second: 3282, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.118 [-0.438, 0.973], mean_best_reward: --\n",
      " 91810/100000: episode: 2317, duration: 0.006s, episode steps: 19, steps per second: 3017, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.084 [-1.317, 0.645], mean_best_reward: --\n",
      " 91828/100000: episode: 2318, duration: 0.006s, episode steps: 18, steps per second: 2916, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.102 [-1.248, 0.558], mean_best_reward: --\n",
      " 91877/100000: episode: 2319, duration: 0.015s, episode steps: 49, steps per second: 3340, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.100 [-1.050, 1.105], mean_best_reward: --\n",
      " 91896/100000: episode: 2320, duration: 0.006s, episode steps: 19, steps per second: 3071, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.048 [-1.783, 1.216], mean_best_reward: --\n",
      " 92015/100000: episode: 2321, duration: 0.042s, episode steps: 119, steps per second: 2866, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.017 [-1.504, 2.146], mean_best_reward: --\n",
      " 92055/100000: episode: 2322, duration: 0.013s, episode steps: 40, steps per second: 3139, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.608, 1.263], mean_best_reward: --\n",
      " 92163/100000: episode: 2323, duration: 0.034s, episode steps: 108, steps per second: 3155, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.143 [-0.859, 1.279], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92209/100000: episode: 2324, duration: 0.016s, episode steps: 46, steps per second: 2839, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.100 [-0.587, 1.398], mean_best_reward: --\n",
      " 92229/100000: episode: 2325, duration: 0.012s, episode steps: 20, steps per second: 1670, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.060 [-1.352, 0.830], mean_best_reward: --\n",
      " 92282/100000: episode: 2326, duration: 0.023s, episode steps: 53, steps per second: 2330, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.174 [-0.976, 1.638], mean_best_reward: --\n",
      " 92409/100000: episode: 2327, duration: 0.050s, episode steps: 127, steps per second: 2556, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.338 [-0.968, 1.851], mean_best_reward: --\n",
      " 92429/100000: episode: 2328, duration: 0.008s, episode steps: 20, steps per second: 2626, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.081 [-0.977, 0.603], mean_best_reward: --\n",
      " 92468/100000: episode: 2329, duration: 0.013s, episode steps: 39, steps per second: 3012, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.108 [-0.757, 1.133], mean_best_reward: --\n",
      " 92519/100000: episode: 2330, duration: 0.016s, episode steps: 51, steps per second: 3220, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.070 [-0.491, 1.123], mean_best_reward: --\n",
      " 92585/100000: episode: 2331, duration: 0.020s, episode steps: 66, steps per second: 3323, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.023 [-0.802, 1.232], mean_best_reward: --\n",
      " 92666/100000: episode: 2332, duration: 0.024s, episode steps: 81, steps per second: 3350, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.209 [-1.745, 1.586], mean_best_reward: --\n",
      " 92699/100000: episode: 2333, duration: 0.011s, episode steps: 33, steps per second: 2957, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.077 [-0.751, 1.148], mean_best_reward: --\n",
      " 92737/100000: episode: 2334, duration: 0.012s, episode steps: 38, steps per second: 3250, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.043 [-1.592, 1.133], mean_best_reward: --\n",
      " 92790/100000: episode: 2335, duration: 0.017s, episode steps: 53, steps per second: 3110, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.053 [-1.180, 1.356], mean_best_reward: --\n",
      " 92857/100000: episode: 2336, duration: 0.023s, episode steps: 67, steps per second: 2920, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.208 [-1.313, 0.542], mean_best_reward: --\n",
      " 92974/100000: episode: 2337, duration: 0.035s, episode steps: 117, steps per second: 3391, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.103 [-1.454, 0.807], mean_best_reward: --\n",
      " 92995/100000: episode: 2338, duration: 0.007s, episode steps: 21, steps per second: 3052, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.050 [-1.180, 1.910], mean_best_reward: --\n",
      " 93046/100000: episode: 2339, duration: 0.017s, episode steps: 51, steps per second: 3022, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.154 [-1.369, 0.745], mean_best_reward: --\n",
      " 93091/100000: episode: 2340, duration: 0.013s, episode steps: 45, steps per second: 3345, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.067 [-0.647, 1.442], mean_best_reward: --\n",
      " 93113/100000: episode: 2341, duration: 0.007s, episode steps: 22, steps per second: 2979, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.082 [-0.772, 1.587], mean_best_reward: --\n",
      " 93152/100000: episode: 2342, duration: 0.013s, episode steps: 39, steps per second: 3006, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: 0.125 [-0.815, 1.184], mean_best_reward: --\n",
      " 93182/100000: episode: 2343, duration: 0.010s, episode steps: 30, steps per second: 3124, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.062 [-1.242, 0.606], mean_best_reward: --\n",
      " 93307/100000: episode: 2344, duration: 0.050s, episode steps: 125, steps per second: 2520, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.062 [-1.334, 1.983], mean_best_reward: --\n",
      " 93320/100000: episode: 2345, duration: 0.007s, episode steps: 13, steps per second: 1886, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.100 [-0.807, 1.416], mean_best_reward: --\n",
      " 93360/100000: episode: 2346, duration: 0.013s, episode steps: 40, steps per second: 3069, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.854, 1.354], mean_best_reward: --\n",
      " 93400/100000: episode: 2347, duration: 0.013s, episode steps: 40, steps per second: 3116, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.141 [-0.962, 0.648], mean_best_reward: --\n",
      " 93495/100000: episode: 2348, duration: 0.031s, episode steps: 95, steps per second: 3044, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.053 [-1.002, 1.033], mean_best_reward: --\n",
      " 93526/100000: episode: 2349, duration: 0.010s, episode steps: 31, steps per second: 3140, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.131 [-0.470, 1.213], mean_best_reward: --\n",
      " 93547/100000: episode: 2350, duration: 0.007s, episode steps: 21, steps per second: 3002, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.093 [-0.739, 1.133], mean_best_reward: --\n",
      " 93560/100000: episode: 2351, duration: 0.005s, episode steps: 13, steps per second: 2582, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.066 [-1.028, 1.519], mean_best_reward: 121.500000\n",
      " 93617/100000: episode: 2352, duration: 0.027s, episode steps: 57, steps per second: 2092, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.129 [-1.358, 0.753], mean_best_reward: --\n",
      " 93633/100000: episode: 2353, duration: 0.006s, episode steps: 16, steps per second: 2865, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.087 [-0.603, 1.305], mean_best_reward: --\n",
      " 93651/100000: episode: 2354, duration: 0.006s, episode steps: 18, steps per second: 2872, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.066 [-0.834, 1.336], mean_best_reward: --\n",
      " 93672/100000: episode: 2355, duration: 0.007s, episode steps: 21, steps per second: 2964, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.084 [-0.625, 1.412], mean_best_reward: --\n",
      " 93705/100000: episode: 2356, duration: 0.010s, episode steps: 33, steps per second: 3226, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.080 [-0.853, 0.631], mean_best_reward: --\n",
      " 93741/100000: episode: 2357, duration: 0.011s, episode steps: 36, steps per second: 3187, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.076 [-0.788, 1.170], mean_best_reward: --\n",
      " 93845/100000: episode: 2358, duration: 0.030s, episode steps: 104, steps per second: 3454, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.091 [-1.144, 0.873], mean_best_reward: --\n",
      " 93861/100000: episode: 2359, duration: 0.007s, episode steps: 16, steps per second: 2450, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.561, 2.523], mean_best_reward: --\n",
      " 93881/100000: episode: 2360, duration: 0.007s, episode steps: 20, steps per second: 2694, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.098 [-0.736, 1.269], mean_best_reward: --\n",
      " 93954/100000: episode: 2361, duration: 0.021s, episode steps: 73, steps per second: 3486, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.085 [-0.888, 1.266], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94000/100000: episode: 2362, duration: 0.014s, episode steps: 46, steps per second: 3195, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: 0.061 [-1.000, 1.732], mean_best_reward: --\n",
      " 94075/100000: episode: 2363, duration: 0.029s, episode steps: 75, steps per second: 2583, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.167 [-1.280, 1.067], mean_best_reward: --\n",
      " 94099/100000: episode: 2364, duration: 0.008s, episode steps: 24, steps per second: 3056, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.083 [-0.936, 1.674], mean_best_reward: --\n",
      " 94137/100000: episode: 2365, duration: 0.012s, episode steps: 38, steps per second: 3276, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.101 [-0.766, 1.814], mean_best_reward: --\n",
      " 94151/100000: episode: 2366, duration: 0.011s, episode steps: 14, steps per second: 1220, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.077 [-1.388, 0.833], mean_best_reward: --\n",
      " 94166/100000: episode: 2367, duration: 0.006s, episode steps: 15, steps per second: 2568, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.096 [-0.949, 1.711], mean_best_reward: --\n",
      " 94192/100000: episode: 2368, duration: 0.009s, episode steps: 26, steps per second: 2975, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.027 [-1.418, 2.173], mean_best_reward: --\n",
      " 94222/100000: episode: 2369, duration: 0.009s, episode steps: 30, steps per second: 3226, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.064 [-0.562, 1.199], mean_best_reward: --\n",
      " 94237/100000: episode: 2370, duration: 0.005s, episode steps: 15, steps per second: 2740, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.099 [-0.631, 1.348], mean_best_reward: --\n",
      " 94333/100000: episode: 2371, duration: 0.030s, episode steps: 96, steps per second: 3229, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.018 [-1.337, 1.150], mean_best_reward: --\n",
      " 94375/100000: episode: 2372, duration: 0.014s, episode steps: 42, steps per second: 3073, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.076 [-0.462, 1.434], mean_best_reward: --\n",
      " 94415/100000: episode: 2373, duration: 0.019s, episode steps: 40, steps per second: 2081, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: 0.035 [-1.320, 1.915], mean_best_reward: --\n",
      " 94459/100000: episode: 2374, duration: 0.015s, episode steps: 44, steps per second: 3002, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: 0.033 [-0.748, 1.191], mean_best_reward: --\n",
      " 94499/100000: episode: 2375, duration: 0.017s, episode steps: 40, steps per second: 2373, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: -0.072 [-1.348, 0.819], mean_best_reward: --\n",
      " 94539/100000: episode: 2376, duration: 0.013s, episode steps: 40, steps per second: 3139, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.118 [-1.293, 0.735], mean_best_reward: --\n",
      " 94593/100000: episode: 2377, duration: 0.021s, episode steps: 54, steps per second: 2588, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.118 [-1.650, 0.983], mean_best_reward: --\n",
      " 94613/100000: episode: 2378, duration: 0.016s, episode steps: 20, steps per second: 1228, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.105 [-1.196, 0.583], mean_best_reward: --\n",
      " 94629/100000: episode: 2379, duration: 0.010s, episode steps: 16, steps per second: 1646, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.120 [-0.565, 1.066], mean_best_reward: --\n",
      " 94689/100000: episode: 2380, duration: 0.050s, episode steps: 60, steps per second: 1210, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.108 [-1.156, 1.247], mean_best_reward: --\n",
      " 94705/100000: episode: 2381, duration: 0.018s, episode steps: 16, steps per second: 875, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.059 [-0.998, 1.462], mean_best_reward: --\n",
      " 94739/100000: episode: 2382, duration: 0.030s, episode steps: 34, steps per second: 1117, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.035 [-0.829, 1.640], mean_best_reward: --\n",
      " 94807/100000: episode: 2383, duration: 0.030s, episode steps: 68, steps per second: 2298, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.301 [-1.845, 0.761], mean_best_reward: --\n",
      " 94828/100000: episode: 2384, duration: 0.011s, episode steps: 21, steps per second: 1930, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.088 [-0.825, 1.340], mean_best_reward: --\n",
      " 94847/100000: episode: 2385, duration: 0.022s, episode steps: 19, steps per second: 879, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.069 [-0.993, 1.630], mean_best_reward: --\n",
      " 94877/100000: episode: 2386, duration: 0.027s, episode steps: 30, steps per second: 1131, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.101 [-0.366, 0.894], mean_best_reward: --\n",
      " 94912/100000: episode: 2387, duration: 0.023s, episode steps: 35, steps per second: 1533, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.118 [-0.406, 0.998], mean_best_reward: --\n",
      " 94988/100000: episode: 2388, duration: 0.060s, episode steps: 76, steps per second: 1259, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.180 [-1.449, 0.832], mean_best_reward: --\n",
      " 95031/100000: episode: 2389, duration: 0.029s, episode steps: 43, steps per second: 1488, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.104 [-0.549, 1.154], mean_best_reward: --\n",
      " 95066/100000: episode: 2390, duration: 0.018s, episode steps: 35, steps per second: 1979, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.103 [-1.623, 0.829], mean_best_reward: --\n",
      " 95106/100000: episode: 2391, duration: 0.016s, episode steps: 40, steps per second: 2489, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.041 [-1.188, 1.219], mean_best_reward: --\n",
      " 95150/100000: episode: 2392, duration: 0.016s, episode steps: 44, steps per second: 2671, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.084 [-1.070, 0.770], mean_best_reward: --\n",
      " 95190/100000: episode: 2393, duration: 0.016s, episode steps: 40, steps per second: 2499, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.100 [-0.621, 1.803], mean_best_reward: --\n",
      " 95206/100000: episode: 2394, duration: 0.010s, episode steps: 16, steps per second: 1665, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.102 [-0.588, 1.159], mean_best_reward: --\n",
      " 95222/100000: episode: 2395, duration: 0.007s, episode steps: 16, steps per second: 2281, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.062 [-1.000, 1.485], mean_best_reward: --\n",
      " 95265/100000: episode: 2396, duration: 0.020s, episode steps: 43, steps per second: 2191, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.442 [0.000, 1.000], mean observation: -0.071 [-1.752, 1.759], mean_best_reward: --\n",
      " 95299/100000: episode: 2397, duration: 0.014s, episode steps: 34, steps per second: 2451, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.102 [-0.675, 1.158], mean_best_reward: --\n",
      " 95332/100000: episode: 2398, duration: 0.016s, episode steps: 33, steps per second: 2100, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.075 [-0.569, 1.377], mean_best_reward: --\n",
      " 95365/100000: episode: 2399, duration: 0.012s, episode steps: 33, steps per second: 2844, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.122 [-0.504, 1.096], mean_best_reward: --\n",
      " 95407/100000: episode: 2400, duration: 0.013s, episode steps: 42, steps per second: 3241, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.145 [-1.249, 0.665], mean_best_reward: --\n",
      " 95433/100000: episode: 2401, duration: 0.009s, episode steps: 26, steps per second: 3057, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.069 [-0.586, 1.265], mean_best_reward: 125.500000\n",
      " 95472/100000: episode: 2402, duration: 0.013s, episode steps: 39, steps per second: 2983, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.077 [-0.615, 1.038], mean_best_reward: --\n",
      " 95514/100000: episode: 2403, duration: 0.013s, episode steps: 42, steps per second: 3179, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.005 [-1.059, 0.769], mean_best_reward: --\n",
      " 95563/100000: episode: 2404, duration: 0.015s, episode steps: 49, steps per second: 3294, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.167 [-1.127, 0.738], mean_best_reward: --\n",
      " 95610/100000: episode: 2405, duration: 0.014s, episode steps: 47, steps per second: 3377, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.023 [-1.567, 1.015], mean_best_reward: --\n",
      " 95642/100000: episode: 2406, duration: 0.011s, episode steps: 32, steps per second: 3033, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.074 [-0.814, 1.547], mean_best_reward: --\n",
      " 95661/100000: episode: 2407, duration: 0.007s, episode steps: 19, steps per second: 2762, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.099 [-0.611, 0.958], mean_best_reward: --\n",
      " 95674/100000: episode: 2408, duration: 0.005s, episode steps: 13, steps per second: 2753, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.102 [-1.028, 1.592], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95744/100000: episode: 2409, duration: 0.021s, episode steps: 70, steps per second: 3337, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.136 [-1.291, 0.943], mean_best_reward: --\n",
      " 95846/100000: episode: 2410, duration: 0.040s, episode steps: 102, steps per second: 2521, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.250 [-1.535, 1.139], mean_best_reward: --\n",
      " 95904/100000: episode: 2411, duration: 0.018s, episode steps: 58, steps per second: 3250, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.064 [-1.372, 1.337], mean_best_reward: --\n",
      " 95948/100000: episode: 2412, duration: 0.013s, episode steps: 44, steps per second: 3267, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.003 [-1.025, 1.580], mean_best_reward: --\n",
      " 95971/100000: episode: 2413, duration: 0.007s, episode steps: 23, steps per second: 3138, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.082 [-1.976, 1.006], mean_best_reward: --\n",
      " 95996/100000: episode: 2414, duration: 0.008s, episode steps: 25, steps per second: 3192, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.069 [-0.614, 1.063], mean_best_reward: --\n",
      " 96033/100000: episode: 2415, duration: 0.011s, episode steps: 37, steps per second: 3227, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.122 [-0.597, 0.932], mean_best_reward: --\n",
      " 96091/100000: episode: 2416, duration: 0.017s, episode steps: 58, steps per second: 3338, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.099 [-0.588, 1.545], mean_best_reward: --\n",
      " 96153/100000: episode: 2417, duration: 0.018s, episode steps: 62, steps per second: 3355, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.052 [-1.177, 0.565], mean_best_reward: --\n",
      " 96210/100000: episode: 2418, duration: 0.017s, episode steps: 57, steps per second: 3376, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.124 [-0.930, 0.823], mean_best_reward: --\n",
      " 96332/100000: episode: 2419, duration: 0.036s, episode steps: 122, steps per second: 3392, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.025 [-1.260, 0.876], mean_best_reward: --\n",
      " 96404/100000: episode: 2420, duration: 0.025s, episode steps: 72, steps per second: 2931, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.144 [-1.171, 0.714], mean_best_reward: --\n",
      " 96428/100000: episode: 2421, duration: 0.008s, episode steps: 24, steps per second: 2842, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.088 [-0.564, 1.047], mean_best_reward: --\n",
      " 96464/100000: episode: 2422, duration: 0.013s, episode steps: 36, steps per second: 2821, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.029 [-0.644, 1.218], mean_best_reward: --\n",
      " 96507/100000: episode: 2423, duration: 0.013s, episode steps: 43, steps per second: 3301, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.098 [-1.013, 1.656], mean_best_reward: --\n",
      " 96611/100000: episode: 2424, duration: 0.030s, episode steps: 104, steps per second: 3411, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.379 [-2.021, 0.995], mean_best_reward: --\n",
      " 96666/100000: episode: 2425, duration: 0.017s, episode steps: 55, steps per second: 3322, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: -0.055 [-1.703, 0.855], mean_best_reward: --\n",
      " 96699/100000: episode: 2426, duration: 0.010s, episode steps: 33, steps per second: 3205, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.060 [-0.790, 1.379], mean_best_reward: --\n",
      " 96756/100000: episode: 2427, duration: 0.017s, episode steps: 57, steps per second: 3289, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.139 [-1.538, 1.498], mean_best_reward: --\n",
      " 96782/100000: episode: 2428, duration: 0.008s, episode steps: 26, steps per second: 3123, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.066 [-0.604, 1.133], mean_best_reward: --\n",
      " 96804/100000: episode: 2429, duration: 0.007s, episode steps: 22, steps per second: 3129, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.070 [-0.847, 1.734], mean_best_reward: --\n",
      " 96836/100000: episode: 2430, duration: 0.010s, episode steps: 32, steps per second: 3217, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-1.578, 0.846], mean_best_reward: --\n",
      " 96875/100000: episode: 2431, duration: 0.012s, episode steps: 39, steps per second: 3269, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.113 [-0.618, 1.092], mean_best_reward: --\n",
      " 96911/100000: episode: 2432, duration: 0.011s, episode steps: 36, steps per second: 3264, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.004 [-1.384, 1.805], mean_best_reward: --\n",
      " 97015/100000: episode: 2433, duration: 0.030s, episode steps: 104, steps per second: 3425, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.121 [-0.887, 1.027], mean_best_reward: --\n",
      " 97066/100000: episode: 2434, duration: 0.016s, episode steps: 51, steps per second: 3178, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.009 [-1.357, 0.741], mean_best_reward: --\n",
      " 97090/100000: episode: 2435, duration: 0.012s, episode steps: 24, steps per second: 2085, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.602, 1.011], mean_best_reward: --\n",
      " 97132/100000: episode: 2436, duration: 0.013s, episode steps: 42, steps per second: 3130, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-1.209, 0.585], mean_best_reward: --\n",
      " 97153/100000: episode: 2437, duration: 0.007s, episode steps: 21, steps per second: 2939, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.094 [-1.577, 0.760], mean_best_reward: --\n",
      " 97210/100000: episode: 2438, duration: 0.018s, episode steps: 57, steps per second: 3221, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.121 [-0.959, 0.545], mean_best_reward: --\n",
      " 97231/100000: episode: 2439, duration: 0.007s, episode steps: 21, steps per second: 3126, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.097 [-0.547, 0.984], mean_best_reward: --\n",
      " 97257/100000: episode: 2440, duration: 0.008s, episode steps: 26, steps per second: 3126, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.015 [-2.246, 1.559], mean_best_reward: --\n",
      " 97297/100000: episode: 2441, duration: 0.013s, episode steps: 40, steps per second: 3088, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.575, 1.056], mean_best_reward: --\n",
      " 97328/100000: episode: 2442, duration: 0.010s, episode steps: 31, steps per second: 3217, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.109 [-1.641, 0.650], mean_best_reward: --\n",
      " 97361/100000: episode: 2443, duration: 0.010s, episode steps: 33, steps per second: 3228, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.042 [-1.186, 1.773], mean_best_reward: --\n",
      " 97386/100000: episode: 2444, duration: 0.008s, episode steps: 25, steps per second: 3167, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.129 [-0.559, 1.085], mean_best_reward: --\n",
      " 97431/100000: episode: 2445, duration: 0.014s, episode steps: 45, steps per second: 3298, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.005 [-1.336, 1.836], mean_best_reward: --\n",
      " 97455/100000: episode: 2446, duration: 0.008s, episode steps: 24, steps per second: 3189, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.087 [-1.438, 0.631], mean_best_reward: --\n",
      " 97520/100000: episode: 2447, duration: 0.019s, episode steps: 65, steps per second: 3414, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.287 [-0.714, 1.808], mean_best_reward: --\n",
      " 97549/100000: episode: 2448, duration: 0.009s, episode steps: 29, steps per second: 3151, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.060 [-1.178, 0.769], mean_best_reward: --\n",
      " 97595/100000: episode: 2449, duration: 0.014s, episode steps: 46, steps per second: 3208, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.081 [-0.889, 1.301], mean_best_reward: --\n",
      " 97617/100000: episode: 2450, duration: 0.008s, episode steps: 22, steps per second: 2892, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.045 [-1.449, 1.022], mean_best_reward: --\n",
      " 97670/100000: episode: 2451, duration: 0.016s, episode steps: 53, steps per second: 3248, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.034 [-0.742, 1.031], mean_best_reward: 147.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97695/100000: episode: 2452, duration: 0.010s, episode steps: 25, steps per second: 2492, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.078 [-2.069, 1.144], mean_best_reward: --\n",
      " 97706/100000: episode: 2453, duration: 0.005s, episode steps: 11, steps per second: 2226, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.094 [-1.588, 1.012], mean_best_reward: --\n",
      " 97721/100000: episode: 2454, duration: 0.007s, episode steps: 15, steps per second: 2262, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.048 [-1.696, 1.218], mean_best_reward: --\n",
      " 97746/100000: episode: 2455, duration: 0.009s, episode steps: 25, steps per second: 2696, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.062 [-0.987, 0.606], mean_best_reward: --\n",
      " 97774/100000: episode: 2456, duration: 0.009s, episode steps: 28, steps per second: 3120, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.746, 1.291], mean_best_reward: --\n",
      " 97789/100000: episode: 2457, duration: 0.006s, episode steps: 15, steps per second: 2697, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.171, 1.924], mean_best_reward: --\n",
      " 97814/100000: episode: 2458, duration: 0.009s, episode steps: 25, steps per second: 2938, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.049 [-0.959, 0.636], mean_best_reward: --\n",
      " 97928/100000: episode: 2459, duration: 0.033s, episode steps: 114, steps per second: 3465, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-1.397, 1.662], mean_best_reward: --\n",
      " 97942/100000: episode: 2460, duration: 0.005s, episode steps: 14, steps per second: 2821, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.091 [-1.529, 2.451], mean_best_reward: --\n",
      " 98029/100000: episode: 2461, duration: 0.025s, episode steps: 87, steps per second: 3450, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.044 [-1.413, 0.780], mean_best_reward: --\n",
      " 98070/100000: episode: 2462, duration: 0.012s, episode steps: 41, steps per second: 3280, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.047 [-1.211, 0.611], mean_best_reward: --\n",
      " 98088/100000: episode: 2463, duration: 0.006s, episode steps: 18, steps per second: 2943, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.097 [-1.456, 0.932], mean_best_reward: --\n",
      " 98130/100000: episode: 2464, duration: 0.013s, episode steps: 42, steps per second: 3203, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.051 [-0.797, 1.600], mean_best_reward: --\n",
      " 98165/100000: episode: 2465, duration: 0.013s, episode steps: 35, steps per second: 2689, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.133 [-0.817, 1.242], mean_best_reward: --\n",
      " 98203/100000: episode: 2466, duration: 0.012s, episode steps: 38, steps per second: 3231, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.139 [-0.714, 1.366], mean_best_reward: --\n",
      " 98212/100000: episode: 2467, duration: 0.004s, episode steps: 9, steps per second: 2471, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.129 [-1.017, 1.638], mean_best_reward: --\n",
      " 98273/100000: episode: 2468, duration: 0.020s, episode steps: 61, steps per second: 3031, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.271 [-1.830, 0.998], mean_best_reward: --\n",
      " 98304/100000: episode: 2469, duration: 0.011s, episode steps: 31, steps per second: 2826, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.114 [-0.909, 0.398], mean_best_reward: --\n",
      " 98323/100000: episode: 2470, duration: 0.012s, episode steps: 19, steps per second: 1546, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.069 [-1.209, 2.067], mean_best_reward: --\n",
      " 98370/100000: episode: 2471, duration: 0.021s, episode steps: 47, steps per second: 2229, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.033 [-1.341, 1.483], mean_best_reward: --\n",
      " 98387/100000: episode: 2472, duration: 0.008s, episode steps: 17, steps per second: 2205, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.111 [-1.494, 0.994], mean_best_reward: --\n",
      " 98450/100000: episode: 2473, duration: 0.029s, episode steps: 63, steps per second: 2137, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.189 [-1.212, 0.735], mean_best_reward: --\n",
      " 98489/100000: episode: 2474, duration: 0.016s, episode steps: 39, steps per second: 2374, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.564 [0.000, 1.000], mean observation: -0.071 [-1.933, 0.949], mean_best_reward: --\n",
      " 98548/100000: episode: 2475, duration: 0.021s, episode steps: 59, steps per second: 2829, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.101 [-0.799, 1.156], mean_best_reward: --\n",
      " 98593/100000: episode: 2476, duration: 0.015s, episode steps: 45, steps per second: 2991, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.002 [-0.816, 1.027], mean_best_reward: --\n",
      " 98631/100000: episode: 2477, duration: 0.013s, episode steps: 38, steps per second: 2867, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: 0.103 [-0.547, 0.870], mean_best_reward: --\n",
      " 98685/100000: episode: 2478, duration: 0.017s, episode steps: 54, steps per second: 3236, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.068 [-1.434, 0.668], mean_best_reward: --\n",
      " 98712/100000: episode: 2479, duration: 0.009s, episode steps: 27, steps per second: 3054, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.051 [-0.935, 1.680], mean_best_reward: --\n",
      " 98758/100000: episode: 2480, duration: 0.014s, episode steps: 46, steps per second: 3289, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: -0.095 [-1.775, 0.799], mean_best_reward: --\n",
      " 98833/100000: episode: 2481, duration: 0.023s, episode steps: 75, steps per second: 3237, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.017 [-1.100, 0.918], mean_best_reward: --\n",
      " 98865/100000: episode: 2482, duration: 0.013s, episode steps: 32, steps per second: 2482, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.344 [0.000, 1.000], mean observation: -0.008 [-1.942, 2.774], mean_best_reward: --\n",
      " 98903/100000: episode: 2483, duration: 0.013s, episode steps: 38, steps per second: 3016, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.053 [-1.316, 1.370], mean_best_reward: --\n",
      " 98922/100000: episode: 2484, duration: 0.008s, episode steps: 19, steps per second: 2469, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.100 [-0.792, 1.360], mean_best_reward: --\n",
      " 98958/100000: episode: 2485, duration: 0.012s, episode steps: 36, steps per second: 3042, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.029 [-0.803, 1.439], mean_best_reward: --\n",
      " 98989/100000: episode: 2486, duration: 0.010s, episode steps: 31, steps per second: 2981, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.108 [-0.666, 1.813], mean_best_reward: --\n",
      " 99030/100000: episode: 2487, duration: 0.016s, episode steps: 41, steps per second: 2586, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.008 [-0.924, 1.254], mean_best_reward: --\n",
      " 99072/100000: episode: 2488, duration: 0.013s, episode steps: 42, steps per second: 3280, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: -0.084 [-1.338, 1.522], mean_best_reward: --\n",
      " 99093/100000: episode: 2489, duration: 0.007s, episode steps: 21, steps per second: 3103, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.095 [-1.078, 0.546], mean_best_reward: --\n",
      " 99185/100000: episode: 2490, duration: 0.027s, episode steps: 92, steps per second: 3463, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.215 [-1.283, 2.018], mean_best_reward: --\n",
      " 99223/100000: episode: 2491, duration: 0.012s, episode steps: 38, steps per second: 3298, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.075 [-1.176, 0.749], mean_best_reward: --\n",
      " 99289/100000: episode: 2492, duration: 0.020s, episode steps: 66, steps per second: 3272, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.172 [-0.872, 1.174], mean_best_reward: --\n",
      " 99300/100000: episode: 2493, duration: 0.005s, episode steps: 11, steps per second: 2026, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.125 [-1.397, 2.314], mean_best_reward: --\n",
      " 99331/100000: episode: 2494, duration: 0.011s, episode steps: 31, steps per second: 2907, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.094 [-0.622, 0.957], mean_best_reward: --\n",
      " 99413/100000: episode: 2495, duration: 0.024s, episode steps: 82, steps per second: 3475, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.209 [-2.034, 1.746], mean_best_reward: --\n",
      " 99431/100000: episode: 2496, duration: 0.006s, episode steps: 18, steps per second: 2983, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.273, 0.819], mean_best_reward: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99483/100000: episode: 2497, duration: 0.019s, episode steps: 52, steps per second: 2711, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.106 [-1.339, 0.539], mean_best_reward: --\n",
      " 99501/100000: episode: 2498, duration: 0.008s, episode steps: 18, steps per second: 2241, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.082 [-1.141, 1.790], mean_best_reward: --\n",
      " 99585/100000: episode: 2499, duration: 0.026s, episode steps: 84, steps per second: 3206, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.076 [-1.255, 1.031], mean_best_reward: --\n",
      " 99606/100000: episode: 2500, duration: 0.008s, episode steps: 21, steps per second: 2649, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.083 [-2.023, 1.017], mean_best_reward: --\n",
      " 99703/100000: episode: 2501, duration: 0.028s, episode steps: 97, steps per second: 3502, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.042 [-0.988, 1.235], mean_best_reward: 149.500000\n",
      " 99772/100000: episode: 2502, duration: 0.022s, episode steps: 69, steps per second: 3140, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.175 [-1.999, 1.127], mean_best_reward: --\n",
      " 99804/100000: episode: 2503, duration: 0.010s, episode steps: 32, steps per second: 3078, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.761, 1.070], mean_best_reward: --\n",
      " 99820/100000: episode: 2504, duration: 0.006s, episode steps: 16, steps per second: 2883, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.086 [-2.013, 1.137], mean_best_reward: --\n",
      " 99839/100000: episode: 2505, duration: 0.006s, episode steps: 19, steps per second: 2974, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.095 [-1.482, 0.792], mean_best_reward: --\n",
      " 99867/100000: episode: 2506, duration: 0.010s, episode steps: 28, steps per second: 2708, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.051 [-0.968, 1.388], mean_best_reward: --\n",
      " 99960/100000: episode: 2507, duration: 0.028s, episode steps: 93, steps per second: 3353, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.029 [-0.895, 0.781], mean_best_reward: --\n",
      "done, took 35.794 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 76.000, steps: 76\n",
      "Episode 2: reward: 87.000, steps: 87\n",
      "Episode 3: reward: 89.000, steps: 89\n",
      "Episode 4: reward: 81.000, steps: 81\n",
      "Episode 5: reward: 104.000, steps: 104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ea4029da0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.cem import CEMAgent\n",
    "from rl.memory import EpisodeParameterMemory\n",
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "nb_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Option 1 : Simple model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Option 2: deep network\n",
    "# model = Sequential()\n",
    "# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(16))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dense(nb_actions))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = EpisodeParameterMemory(limit=1000, window_length=1)\n",
    "\n",
    "cem = CEMAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
    "               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\n",
    "cem.compile()\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "cem.fit(env, nb_steps=100000, visualize=False, verbose=2)\n",
    "\n",
    "# After training is done, we save the best weights.\n",
    "\n",
    "#cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "cem.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x7f3f904fd900>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.np_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 22 timesteps , Reward: 1.0\n",
      "Episode finished after 14 timesteps , Reward: 1.0\n",
      "Episode finished after 23 timesteps , Reward: 1.0\n",
      "Episode finished after 12 timesteps , Reward: 1.0\n",
      "Episode finished after 51 timesteps , Reward: 1.0\n",
      "Episode finished after 14 timesteps , Reward: 1.0\n",
      "Episode finished after 13 timesteps , Reward: 1.0\n",
      "Episode finished after 10 timesteps , Reward: 1.0\n",
      "Episode finished after 49 timesteps , Reward: 1.0\n",
      "Episode finished after 49 timesteps , Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "mn = []\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1),', Reward:',reward)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
